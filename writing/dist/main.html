<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">

<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for Mac OS X (vers 31 October 2006 - Apple Inc. build 15.6), see www.w3.org">

  <title></title>
</head>

<body>
  <h1 id="abstract"><a href="#abstract"><span class=
  "header-section-number">1</span> Abstract</a></h1>

  <p>A Javascript REST client library targeting both Node.js and
  web browsers that incorporates http streaming, pattern matching,
  and progressive JSON parsing, with the aim of improving
  performance, fault tolerance, and encouraging a greater degree of
  loose coupling between programs. Loose coupling is particularly
  considered in light of the application of Agile methodologies to
  SOA, providing a framework in which it is acceptable to partially
  restructure the JSON format in which a resource is expressed
  whilst maintaining compatibility with dependent systems.</p>

  <p>A critique is made of current practice under which resources
  are entirely retrieved before items of interest are extracted
  programmatically. An alternative model is presented allowing the
  specification of items of interest using a declarative syntax
  similar to JSONPath. The identified items are then provided
  incrementally while the resource is still downloading.</p>

  <p>In addition to a consideration of performance in absolute
  terms, the usability implications of an incremental model are
  also evaluated with regards to differences in user perception of
  performance.</p>

  <h1 id="introduction"><a href="#introduction"><span class=
  "header-section-number">2</span> Introduction</a></h1>

  <p>This dissertation does not focus on implementing software for
  any particular problem domain. Rather, its purpose is to
  encourage the REST paradigm to be viewed through a novel lens. In
  application this may be used to deliver tangible benefits to many
  common REST use cases. Although I express my thesis through
  programming, the contribution I hope to deliver is felt more
  strongly as a shift in how we <em>think</em> about http than it
  is a change in the underlying technology.</p>

  <p>In the interest of developer ergonomics, REST clients have
  tended to style the calling of remote resources similar to the
  call style of the host programming language. Depending on the
  language, one of two schemas are followed: a synchronous style in
  which the http call is an expression which evaluates to the
  resource that was fetched; or asynchronous or monadic in which
  some logic is specified which may be applied to the response once
  it is complete. This tendency to cast REST calls using terms from
  the language feels quite natural; we may call a remote service
  without having to make any adjustment for the fact that it is
  remote. However, we should remember that this construct is not
  the only possible mapping. Importing some moderate Whorfianism
  <span class="citation">(Whorf 1956)</span><span class=
  "citation">(Sapir 1958)</span> from linguistics, we might venture
  to say that the programming languages we use encourage us to
  think in the terms that they easily support. Also UML! For any
  multi-packet message sent via a network some parts will arrive
  before others, at least approximately in-order, but whilst coding
  a C-inspired language whose return statements yield single,
  discrete values it comfortable to conceptualise the REST response
  as a discrete event. Perhaps better suited to representing a
  progressively returned value would have been the relatively
  unsupported Generator routine <span class="citation">(Ralston
  2000)</span>.</p>

  <p>In most practical cases where software is being used to
  perform a task there is no reasonable distinction between being
  earlier and being quicker. Therefore, if our interest is to
  create fast software we should be using data at the first
  possible opportunity. Examining data <em>while</em> it streams
  rather than hold unexamined until the message ends.</p>

  <p>The coining of the term REST represented a shift in how we
  think about http, away from the transfer of hypertext documents
  to that of arbitrary data <span class="citation">(Fielding 2000,
  407&acirc;&euro;&ldquo;416)</span>. It introduced no
  fundamentally new methods. Likewise, no genuinely new computer
  science techniques need be invented to realise my thesis. As a
  minimum, the implementation requires an http client which exposes
  the response whilst it is in progress and a parser which can
  start making sense of a response before it sees all of it. I also
  could not claim this thesis to be an entirely novel composition
  of such parts. Few ideas are genuinely new and it is often wiser
  to mine for solved problems then to solve again afresh. The
  intense competition of Web browsers to be as fast as possible has
  already found this solution. Load any graphics rich with images
  -- essentially an aggregation of hypertext and images -- the HTML
  is parsed incrementally while it is downloading and the images
  are requested as soon as individual &lt;img&gt; tags are
  encountered. The browser's implementation involves a highly
  optimised parser created for a single task, that of displaying
  web pages. The new contribution of this dissertation is to
  provide a generic analog applicable to any problem domain.</p>

  <p>Also progressive SVGs.<sup><a href="#fn1" class="footnoteRef"
  id="fnref1" name="fnref1">1</a></sup></p>

  <h2 id="rest-aggregation-could-be-faster"><a href=
  "#rest-aggregation-could-be-faster"><span class=
  "header-section-number">2.1</span> REST aggregation could be
  faster</a></h2>

  <div class="figure">
    <img src="images/rest_timeline_1.png" alt=
    "Aggregation of lower-level resources exposed via REST. The client fetches a listing of an author's publications and then the first three articles. The sequence represents the most commonly used technique in which the client does not react to the response until it is complete. In this example the second wave of requests cannot be made until the original response is complete, at which time they are issued in quick succession. ">

    <p class="caption"><strong>Aggregation of lower-level resources
    exposed via REST.</strong> The client fetches a listing of an
    author's publications and then the first three articles. The
    sequence represents the most commonly used technique in which
    the client does not react to the response until it is complete.
    In this example the second wave of requests cannot be made
    until the original response is complete, at which time they are
    issued in quick succession.</p>
  </div>

  <div class="figure">
    <img src="images/rest_timeline_2.png" alt=
    "Revised sequence of aggregation performed by a client capable of progressively interpreting the fetched resource. Because UML sequence diagrams arrows draw the concept of a returned value as a one-off event rather than a continuous process, I have introduced the notation of lighter arrows illustrating fragments of an ongoing response. Each individual publication request is made at the earliest possible time, as soon as the its URL can be extracted from the publications list. Once the required data has been read from the original resource it is aborted rather than continue to download unnecessary data. This results in a moderate reduction in wait time to see all three articles but a dramatic reduction in waiting before the first content is presented. Note also how the cadence of requests is more even with four connections opened at roughly equal intervals rather than a single request followed by a rapid burst of three. Clients frequently limit the number of simultaneous connections per domain so avoiding bursts of requests is further to our advantage. ">

    <p class="caption"><strong>Revised sequence of aggregation
    performed by a client capable of progressively interpreting the
    fetched resource.</strong> Because UML sequence diagrams arrows
    draw the concept of a returned value as a one-off event rather
    than a continuous process, I have introduced the notation of
    lighter arrows illustrating fragments of an ongoing response.
    Each individual publication request is made at the earliest
    possible time, as soon as the its URL can be extracted from the
    publications list. Once the required data has been read from
    the original resource it is aborted rather than continue to
    download unnecessary data. This results in a moderate reduction
    in wait time to see all three articles but a dramatic reduction
    in waiting before the first content is presented. Note also how
    the cadence of requests is more even with four connections
    opened at roughly equal intervals rather than a single request
    followed by a rapid burst of three. Clients frequently limit
    the number of simultaneous connections per domain so avoiding
    bursts of requests is further to our advantage.</p>
  </div><!--- 
connections per peer limited:
http://stackoverflow.com/questions/5751515/official-references-for-default-values-of-concurrent-http-1-1-connections-per-se 
=-->

  <p>Figures and illustrate how a progressive REST client may
  without adjustments to the server be used to aggregate REST
  resources faster. The greatest improvement is in how early the
  first piece of data is able to be used. This is advantageous:
  firstly, progressive display in itself raises the human
  perception of performance <span class="citation">(Geelhoed et al.
  1995)</span>; secondly, a user wanting to scan from top to bottom
  may start reading the first article while waiting for the later
  ones to arrive; thirdly, on seeing the first content the user may
  notice that they have requested the wrong aggregation, allowing
  them to backtrack earlier.</p>

  <p>Although the label "client software" in the figures above
  hints at software running directly on a user's own device this is
  not necessarily the case, for example the client may in fact be
  an server-side aggregation layer. Nodes in an n-tier architecture
  commonly defy categorisation as 'client' or 'server' in a way
  which is appropriate from all frames of reference. Rather, nodes
  may be thought of as a client from the layer below and as a
  server from the layer above. A further example would be a
  server-side webpage generator maintaining a perceptual
  performance improvement by progressively writing out html using
  http chunked encoding. <span class="citation">(Stefanov
  2009)</span>. The demonstrated advantages hold regardless of
  where in the stack the 'client' is located.</p>

  <h2 id="stepping-outside-the-big-small-tradeoff"><a href=
  "#stepping-outside-the-big-small-tradeoff"><span class=
  "header-section-number">2.2</span> Stepping outside the big-small
  tradeoff</a></h2>

  <p>Where a domain model contains a series of data, of which
  ranges are made available via REST, I have often seen a trade-off
  with regards to how much of the series each call should request.
  Answering this question is usually a compromise between competing
  concerns in which it is not simultaneously possible to addresses
  all concerns satisfactorily. A good example might be a Twitter's
  pages listing a series of tweets where the interface designers
  adopted a currently trending pattern <span class=
  "citation">(Ahuvia 2013)</span>, Infinite Scrolling. Starting
  from an initial page showing some finite number of tweets, upon
  scrolling to the bottom the next batch is automatically
  requested. The new batch is fetched in a json format and, once
  loaded, presented as html and added to the bottom of the page.
  Applied repeatedly this allows the user to scroll indefinitely,
  albeit punctuated by slightly jolting pauses while new content is
  loaded. To frame the big-small tradeoff we might consider the
  extreme choices. Firstly, requesting just one tweet per http
  request. By requesting the smallest possible content individual
  calls would complete very quickly and the pauses would be short.
  Taking the extreme small end the page stutters, pausing
  momentarily but frequently. Taking the opposite extreme, by
  requesting some huge number of tweets we see long periods of
  smooth scrolling partitioned by long waits.</p>

  <p>I propose that my thesis may be applied used to stand down
  from this compromise by delivering pauses which are both
  infrequent and short. In the Twitter example, once we have
  thinking about http progressively this may be achieved quite
  simply by issuing large requests but instead of deferring all
  rendering until the request completes, render individual tweets
  incrementally as they are progressively parsed out of the ongoing
  response.</p>

  <p>Integrate: twitter: page could update at bottom and top with
  same transport perhaps.</p>

  <h2 id="staying-fast-on-a-fallible-network"><a href=
  "#staying-fast-on-a-fallible-network"><span class=
  "header-section-number">2.3</span> Staying fast on a fallible
  network</a></h2>

  <p>The reliability of networks that REST operates over varies
  widely. Considering the worst case we see mobile networks in
  marginal signal over which it is common for ongoing downloads to
  be abruptly disconnected. Existing http clients handle this kind
  of unexpected termination poorly. Consider the everyday situation
  of somebody using a smartphone browser to check their email. The
  use of Webmail necessitates that the communication in made via
  REST rather than a mail specific protocol such as IMAP. Mobile
  data coverage is less than network operators claim <span class=
  "citation">(Anon. 2011)</span> so while travelling the signal can
  be expected to be lost and reestablished many times. Whilst not
  strictly forbidding their inspection, the web developer's
  standard AJAX toolkit are structured in such a way as to
  encourage the developer to consider partially successful messages
  as wholly unsuccessful. For example, the popular AJAX library
  jQuery automatically parses complete JSON or XML responses before
  handing back to the application. But on failure there is no
  attempt to parse or deliver the partial response. To programmers
  who know where to look the partial responses are retrievable as
  raw text but handling them is a special case,
  bringing-your-own-parser affair. Because of this difficulty I can
  only find examples of partial messages being dropped without
  inspection. In practice this means that for the user checking her
  email, even if 90% of her inbox had been retrieved she will be
  shown nothing. When the network is available again the
  application will have to download from scratch, including the 90%
  which it already fetched. I see much potential for improvement
  here.</p>

  <p>Not every message, incomplete, is useful. Whilst of course a
  generic REST client cannot understand the semantics of specific
  messages fully enough to decide if a partially downloaded message
  is useful, I propose it would be an improvement if the content
  from incomplete responses could be handled using much the same
  programming as for complete responses. This follows naturally
  from a conceptualisation of the http response as a progressive
  stream of many small parts; as each part arrives it should be
  possible to use it without knowing if the next will be delivered
  successfully. This style of programming encourages thinking in
  terms of optimistic locking. Upon each partial delivery there is
  an implicit assumption that it may be acted on straight away and
  the next will also be successful. In cases where this assumption
  fails the application should be notified so that some rollback
  may be performed.</p>

  <h2 id=
  "agile-methodologies-frequent-deployments-and-compatibility-today-with-versions-tomorrow">
  <a href=
  "#agile-methodologies-frequent-deployments-and-compatibility-today-with-versions-tomorrow">
  <span class="header-section-number">2.4</span> Agile
  methodologies, frequent deployments, and compatibility today with
  versions tomorrow</a></h2>

  <p>In most respects SOA architecture fits well with the fast
  release cycle that Agile methodologies encourage. Because in SOA
  we may consider that all data is local rather than global and
  that the components are loosely coupled and autonomous, frequent
  releases of any particular sub-system shouldn't pose a problem to
  the correct operation of the whole. Following emergent design it
  should be possible for the format of resources to be realised
  slowly and iteratively as a greater understanding of the problem
  is achieved. Unfortunately in practice the ability to change is
  hampered by tools which encourage programming against rigidly
  specified formats. Working in enterprise I have often seen the
  release of dozens of components cancelled because of a single
  unit that failed to meet acceptance criteria. By allowing a tight
  coupling that depends on exact versions of formats, the perfect
  environment is created for contagion whereby the updating of any
  single unit may only be done as part of the updating of the
  whole.</p>

  <p>An effective response to this problem would be to integrate
  into a REST client library the ability to use a response whilst
  being only loosely coupled to the <em>shape</em> of the overall
  message.</p>

  <h2 id="deliverables"><a href="#deliverables"><span class=
  "header-section-number">2.5</span> Deliverables</a></h2>

  <p>To avoid feature creep I am paring down the software
  deliverables to the smallest work which can we said to realise my
  thesis. Amongst commentators on start-up companies this is known
  as a <em>zoom-in pivot</em> and the work it produces should be
  the <em>Minimum Viable Product</em> or MVP <span class=
  "citation">(Reis 2011 p. ??)</span>, the guiding principle being
  that it is preferable to produce a little well than more badly.
  By focusing tightly I cannot not deliver a full stack so I am
  forced to implement only solutions which interoperate with
  existing deployments. This is advantageous; to somebody looking
  to improve their system small additions are easier to action than
  wholesale change.</p>

  <p>To reify the vision above, a streaming client is the MVP.
  Because all network transmissions may be viewed though a
  streaming lens an explicitly streaming server is not required.
  Additionally, whilst http servers capable of streaming are quite
  common even if they are not always programmed as such, I have
  been unable to find any example of a streaming-capable REST
  client.</p>

  <h2 id="criteria-for-success"><a href=
  "#criteria-for-success"><span class=
  "header-section-number">2.6</span> Criteria for success</a></h2>

  <p>In evaluating this project, we may say it has been a success
  if non-trivial improvements in speed can be made without a
  corresponding increase in the difficulty of programming the
  client. This improvement may be in terms of the absolute total
  time required to complete a representative task or in a user's
  perception of the speed in completing the task. Because
  applications in the target domain are much more io-bound than
  CPU-bound, optimisation in terms of the execution time of a
  algorithms will be de-emphasised unless especially egregious. The
  measuring of speed will include a consideration of performance
  degradation due to connections which are terminated early.</p>

  <p>Additionally, I shall be looking at common ways in which the
  semantics of a message are expanded as a system's design emerges
  and commenting on the value of loose coupling in avoiding
  disruption given unanticipated format changes.</p>

  <h1 id="background"><a href="#background"><span class=
  "header-section-number">3</span> Background</a></h1><!---
background should be 2-10 pages

That's 1,000 to 5,000 words. (500 per page)

Or, 666 to 3,333 (333 per page)

 * introduces the reader to the problem domain or application area
 * ...the context in which the project takes place
 * principles and techniques that will be applied or discussed
    * prior art
    * what out there is being popularly used?
    
 * Don't go into solutions! (that belongs in R&A)   
 * Can analyse a bit but any new reflections should be put in R&A
   
=-->

  <h2 id="the-web-as-an-application-platform"><a href=
  "#the-web-as-an-application-platform"><span class=
  "header-section-number">3.1</span> The web as an application
  platform</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "A webapp running with a front end generated partially on server and partially on client side. Ie, front-end client-side, front-end server-side, presentation layer a more meaningful distinction than">

    <p class="caption"><em>A webapp running with a front end
    generated partially on server and partially on client
    side.</em> Ie, front-end client-side, front-end server-side,
    presentation layer a more meaningful distinction than</p>
  </div>

  <p>Application design, particularly regarding the presentation
  layer, has charted an undulating path pulled by competing
  patterns of thick and thin clients. Having been taken up as the
  platform today for all but the most specialised applications, the
  web continues in this fashion by resisting easy categorisation as
  either mode. Although born on the network, at inception the web
  wasn't particularly graphical and didn't tread in the steps of
  networked graphical technologies such as X11 in which every
  presentation decision was made on a remote server<sup><a href=
  "#fn2" class="footnoteRef" id="fnref2" name="fnref2">2</a></sup>
  -- instead of sending fine-grained graphical instructions, a much
  more compact document mark-up format was used. At the same time,
  the markup-format was unlike like Gopher by being not totally
  semantic meaning that presentation layer concerns were kept
  partially resident on the server. At this time, whereas CGI was
  being used to serve documents with changeable content, it was not
  until 1996 with <em>ViaWeb</em> (later to become Yahoo Stores)
  that a user could be given pages comparable in function to the
  GUI interface of a desktop application. <span class=
  "citation">(Graham 2004 - get page number, in old dis)</span>.
  The interface of these early web applications comprised of pages
  dynamically generated on the server side, but handled statically
  on the client side so far as the browser was not able to be
  scripted to manipulate the page in any way.</p>

  <p>The modern, client-scripted web bears a striking resemblance
  to NeWS. Rather than send many individual drawings, the server
  could send parametrised instructions to show the client
  <em>how</em> some item of presentation is drawn. Having received
  the program, the only communications required are the parameters.
  This mixed-model provides no lesser degree of server-side control
  but by using client-side rendering a much faster experience was
  possible than would otherwise be possible over low-speed networks
  <span class="citation">(Hopkins 1994)</span>.</p>

  <p>Today it is agreed that program architecture should separate
  presentation from operational logic but there is no firm
  consensus on where each concern should be exercised. While it
  feels that Javascript is becoming requisite to even display a
  page, there are also actions in the opposite direction, for
  example in 2012 twitter moved much of their rendering back to the
  server-side reducing load times to one fifth of their previous
  design, commenting "The future is coming and it looks just like
  the past" <span class="citation">(Lea 2012)</span>. This model
  generated server-side short pages that load quick and are ready
  to be displayed but also sent the Javascript which would allow
  the display to be updated without another full server load. One
  weakness of this model is that the same presentational logic
  requires two expressions.</p>

  <p>Like most interactive programming, client-side scripts usually
  suffer greater delays waiting for io than because javascript
  execution times present a bottleneck. Because Javascript is used
  for user interfaces, frame-rates are important. Single threaded
  so js holds up rendering. Important to return control to the
  browser quickly. However, once execution of each js frame of
  execution is no more than the monitor refresh rate, further
  optimisation brings zero benefit. Hence, writing extremely
  optimised Javascript, especially focusing on micro-optimisations
  that hurt code readability is a bit silly.</p>

  <blockquote>
    <p>The user does something, then the app responds visually with
    immediacy at 30 frames per second or more, and completes a task
    in a few hundred milliseconds. As long as an app meets this
    user goal, it doesn&acirc;&euro;&trade;t matter how big an
    abstraction layer it has to go through to get to silicon.
    <span class="citation">(Mullany 2013)</span></p>
  </blockquote>

  <h2 id="node.js"><a href="#node.js"><span class=
  "header-section-number">3.2</span> Node.js</a></h2>

  <p>It is difficult to say to what degree Node's use of Javascript
  is a distraction from the system's principled design aims and to
  what degree it defines the technology. Paradoxically, both may be
  so. Javascript has proven itself very effective as the language
  to meet Node's design goals but this suitability is not based on
  Javascript's association with web browsers, although it is
  certainly beneficial: for the first time it is possible to
  program presentation logic once which is capable of running on
  either client or server. Being already familiar with Javascript,
  web programmers were the first to take up Node.js first but the
  project mission statement makes no reference to the web; Node's
  architecture is well suited to any application domain where
  low-latency responses to i/o is more of a concern than
  heavyweight computation. Web applications fit well into this
  niche but they are far from the only domain that does so.</p>

  <p>In most imperative languages attempts at concurrency have
  focused on threaded execution, whereas Node is by design
  single-threaded. Threads are an effective means to speed up
  parallel computation but not well suited to concurrently running
  tasks which are mostly i/o dependent. Used for io, threads
  consume considerable resources while spending most of their lives
  waiting, occasionally punctuated with short bursts of activity.
  Programming Java safely with threads which share access to
  mutable objects requires great care and experience, otherwise the
  programmer is liable to create race conditions. If we consider
  for example a Java thread-based http aggregator; each 'requester'
  thread waits for seconds and then processes for milliseconds. The
  ratio of waiting to processing is so high that any gains achieved
  through actual concurrent execution of the active phase is
  pyrrhic. Following Node's lead, even traditionally thread-based
  environments such as Java are starting to embrace asynchronous,
  single-threaded servers with projects such as Netty.</p>

  <p>Node manages concurrency by managing an event loop of queued
  tasks and expects each task never to block. Non-blocking calls
  are used for all io and are callback based. Unlike Erlang, Node
  does not swap tasks out preemptively, it always waits for tasks
  to complete. This means that each task must complete quickly;
  while this might at first seem like an onerous requirement to put
  on the programmer, in practice the asynchronous nature of the
  toolkit makes following this requirement more natural than not.
  Indeed, other than accidental non-terminating loops or heavy
  number-crunching, the lack of any blocking io whatsoever makes it
  rather difficult to write a node program whose tasks do not exit
  quickly. This programming model of callback-based, asynchronous,
  non-blocking io with an event loop is already the model followed
  inside web browsers, which although multi-threaded in some
  regards, present a single-threaded virtual machine in terms of
  Javascript execution.</p>

  <p>A programmer working with Node's single-thread is able to
  switch contexts quickly to achieve a very efficient kind of
  concurrency because of Javascript's support for closures. Because
  of closures, under Node the responsibility to explicitly store
  state between making an asynchronous call and receiving the
  callback is removed from the programmer. Closures require no new
  syntax, the implicit storage of this data feels so natural and
  inevitable that looking at the typical program it is often not
  obvious that the responsibility exists at all.</p>

  <h2 id="streams-in-node"><a href="#streams-in-node"><span class=
  "header-section-number">3.3</span> Streams in Node</a></h2>

  <blockquote>
    <p>Streams in node are one of the rare occasions when doing
    something the fast way is actually easier. SO USE THEM. not
    since bash has streaming been introduced into a high level
    language as nicely as it is in node." <a href=
    "https://gist.github.com/2401787">high level node style
    guide</a></p>
  </blockquote>

  <p>Bash streams a powerful abstraction easily programmed for
  linear streaming. Node more powerful, allows a powerful streaming
  abstraction which is no more complex to program than a javascript
  webapp front end. Essentially a lower-level (and therefore more
  powerful) interface to streaming such as unix sockets or tcp
  connections.</p>

  <blockquote>
    <p>Node Stream API, which is the core I/O abstraction in
    Node.js (which is a tool for I/O) is essentially an abstract
    in/out interface that can handle any protocol/stream that also
    happens to be written in JavaScript. [<a href=
    "http://maxogden.com/a-proposal-for-streaming-xhr.html">http://maxogden.com/a-proposal-for-streaming-xhr.html</a>]</p>
  </blockquote>

  <p>Streams in node are a variant of the observer pattern and fit
  into a wider Node event model. Streams emit 'readable' events
  when they have some data to be read and 'end' events when they
  are finished. Apart from error handling, so far as reading is
  concerned, that is the extent of the API.</p>

  <h2 id="web-browsers-hosting-rest-clients"><a href=
  "#web-browsers-hosting-rest-clients"><span class=
  "header-section-number">3.4</span> Web browsers hosting REST
  clients</a></h2>

  <p>Http is essentially a thinly-wrapped text response around some
  usually text-based (but sometimes binary) data. It may give the
  length of the content as a header, but is not obliged to. It
  supports an explicitly chunked mode, but even the non-chunked
  mode may be considered as a stream. For example, a program
  generating web pages on the server side might choose to use
  chunking so that the browser is better able to choose when to
  re-render during the progressive display of a page <span class=
  "citation">(Stefanov 2009)</span> but this is optional and
  without these hints progressive rendering will still take
  place.</p>

  <p>The requesting of http from Javascript, commonly termed AJAX,
  was so significant a technique in establishing the modern web
  application architecture that it is often taken as being a
  synonym for Javascript-heavy web pages. Although an acronym for
  Asynchronous Javascript and XML, for data services designed with
  delivery to client-side web applications in mind JSON is almost
  exclusively preferred to XML and the term is used without regard
  for the data format of the response (the unpronounceable
  <em>AJAJ</em> never took off). During the 'browser war' years
  adding non-standard features was a common form of competition
  between authors; following this pattern Internet Explorer
  originally made AJAX possible by exposing Microsoft's Active X
  <em>Xml Http Request</em>, or XHR, object to Javascript
  programmers. This was widely copied as functional equivalents
  were added to all major browsers and the technique was eventually
  formalised by the W3C<span class="citation">(van Kesteren and
  Jackson 2006)</span>. What followed was a period of stagnation
  for web browsers. HTML4 reached W3C Recommendation status in 2001
  but having subsequently found several evolutionary dead ends such
  as XHTML, the developer community would see no major updates
  until HTML5 started to gather pace some ten years later. In this
  context the web continued to rapidly mature as an application
  platform and AJAX programming inevitably overtook the original
  XHR specification, browser vendors again adding their own
  proprietary extensions to compensate.</p>

  <p>Given this backdrop of non-standard extensions and lagging
  standardisation, abstraction layers predictably rose in
  popularity. Despite a reputation Javascript being poorly
  standardised, as a language it is very consistently implemented.
  More accurately we should say that the libraries provided by the
  environment lack compatibility. Given an abstraction layer to
  gloss over considerable differences cross-browser webapp
  developers found little difficulty in targeting multiple
  platforms. The various abstraction competed on developer
  ergonomics with the popular jQuery and Prototype.js promoting
  themselves respectively as <em>"do more, write less"</em> and
  <em>"elegant APIs around the clumsy interfaces of Ajax"</em>.
  JSON being a subset of Javascript, web developers barely noticed
  their privileged position whereby the serialisation of their data
  format mapped exactly onto the basic types of their programming
  language. As such there was never any confusion as to which exact
  object structure to de-serialise to. If this seems like a small
  advantage, contrast with the plethora of confusing and
  incompatible representations of JSON output presented by the
  various Java JSON parsers; JSON's Object better resembles Java's
  Map than Object and the confusion between JSON null, Java null,
  and Jackson's NullNode<sup><a href="#fn3" class="footnoteRef" id=
  "fnref3" name="fnref3">3</a></sup> is a common cause of errors.
  Endowed with certainty regarding deserialisation, JSON parsers
  could be safely integrated directly into AJAX libraries. This
  provided a call style while working with remote resources so
  streamlined as to require hardly any additional effort.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"ot">jQuery</span>.<span class="fu">ajax</span>(<span class=
"st">'http://example.com/people.json'</span>, <span class=
"kw">function</span>( people ) {

   <span class=
"co">// The parsing of the people json into a javascript object</span>
   <span class=
"co">// feels so natural that it is easy to forget while looking </span>
   <span class="co">// at the code that it happens at all. </span>
   
   <span class="fu">alert</span>(<span class=
"st">'the first person is called '</span> + people[<span class=
"dv">0</span>].<span class="fu">name</span>);
});</code>
</pre>

  <p>Whilst simple, the above call style is built on the assumption
  that a response is a one-time event and no accommodation is made
  for a continuously delivered response. Meanwhile, the XHR2
  standardisation process had started and was busy observing and
  specifying proprietary extensions to the original XHR1. Given an
  interest in streaming, the most interesting of these is the
  progress event:</p>

  <blockquote>
    <p>While the download is progressing, queue a task to fire a
    progress event named progress about every 50ms or for every
    byte received, whichever is least frequent. <span class=
    "citation">(van Kesteren 2012)</span></p>
  </blockquote>

  <p>Prior to this addition there had been no mechanism, at least
  so far as the published specs to an XHR instance in a streaming
  fashion. However, while all major browsers currently support
  progress events in their most recently versions, the installed
  userbase of supporting browsers is unlikely to grow fast enough
  that this technique may be relied upon without a fallback for
  several years.</p>

  <p>In fact, this is exactly how web browsers are implemented.
  However, this progressive use of http is hardwired into the
  browser engines rather than exposing an API suitable for general
  use and as such is treated as something of a special case
  specific to web browsers and has not so far seen a more general
  application. I wish to argue that a general application of this
  technique is viable and offers a worthwhile improvement over
  current common methods.</p>

  <p>While until recently browsers have provided no mechanism to
  stream into AJAX, almost every other instance of downloading has
  taken advantage of streaming and progressive interpretation. This
  includes image formats, as the progressive PNG and JPEG; markup
  as progressive display of html and svg; video; and Javascript
  itself -- script interpretation starts before the script is
  wholly fetched. Each of these progressive considerations is
  implemented as a specific-purpose mechanism internal to the
  browser which is not exported to Javascript and as such is not
  possible to repurpose.</p>

  <h2 id="browser-streaming-frameworks"><a href=
  "#browser-streaming-frameworks"><span class=
  "header-section-number">3.5</span> Browser streaming
  frameworks</a></h2>

  <p>As the web's remit spread to include more applications which
  would previously have been native apps, to be truly 'live' many
  applications found the need to be able to receive real-time push
  events. Dozens of streaming transports have been developed
  sidestepping the browser's apparent limitations.</p>

  <p>The earliest and most basic attempt was to poll by making many
  requests, I won't consider this approach other than to say it
  came with all the usually associated downsides. Despite the
  inadequacy of this approach, from here the improved technique of
  <em>long polling</em> was invented. A client makes a request to
  the server side. Once the connection is open the server waits,
  writing nothing until a push is required. To push the server
  writes the message and closes the http connection; since the http
  response is now complete the content may be handled by the
  Javascript client which then immediately makes a new request,
  reiterating the cycle of wait and response. This approach works
  well where messages are infrequently pushed but where the
  frequency is high the limitation of one http transmission per
  connections requires imposes a high overhead.</p>

  <p>Observing that while browsers lack progressive ajax,
  progressive html rendering is available, <em>push tables</em>
  achieve progressive data transfer by serialising streaming data
  to a HTML format. Most commonly messages are written to a table,
  one row per message. On the client side this table is hidden in
  an off-screen frame and the Javascript streaming client watches
  the table and reacts whenever a new row is found. In many ways an
  improvement over long-polling, this approach nevertheless suffers
  from an unnatural data format. Whilst html is a textual format so
  provides a degree of human-readability, html was not designed
  with the goal of an elegent or compact transfer of asynchronous
  data. Contrasted with a SOA ideal of <em>'plumbing on the
  outside'</em>, peeking inside the system is difficult whilst
  bloated and confusing formats are tasked with conveying
  meaning.</p>

  <p>Both long polling and push tables are better throught of as a
  means to circumvent restrictions than indigene technology. A
  purose-built stack, <em>Websockets</em> is poised to take over,
  building a standardised duplex transport and API on top of http's
  chunked mode. While the newest browsers support websockets, most
  of the wild use base does not. Nor do older browsers provide a
  fine-grained enough interface into http in order to allow a
  Javascript implementation. In practice, real-world streaming
  libraries such as socket.io [CITE] are capable of several
  streaming techniques and can select the best for a given context.
  To the programmer debugging an application the assortment of
  transports only enhances the black-box mentality with regards to
  the underlying transports.</p><!---
*some or all of the below could move to A&R, it is wondering into
analysis* =-->

  <p>Whilst there is some overlap, each of the approaches above
  addresses a problem only tangentially related to this project's
  aims. Firstly, requiring a server that can write to an esoteric
  format feels quite anti-REST, especially given that the server is
  sending in a format which requires a specific, known, specialised
  client rather than a generic tool. In REST I have always valued
  how prominently the plumbing of a system is visible, so that to
  sample a resource all that is required is to type a URL and be
  presented with it in a human-comprehensible format.</p>

  <p>Secondly, as adaptations to the context in which they were
  created, these frameworks realise a view of network usage in
  which downloading and streaming are dichotomously split, whereas
  I aim to realise a schema without dichotomy in which
  <em>streaming is adapted as the most effective means of
  downloading</em>. In existing common practice a wholly distinct
  mechanism is provided vs for data which is ongoing vs data which
  is finite. For example, the display of real-time stock data might
  start by AJAXing in historical and then separately use a
  websocket to maintain up-to-the-second updates. This requires the
  server to support two distinct modes. However, I see no reason
  why a single transport could not be used for both. Such a server
  might start answering a request by write historic events from a
  database, then switch to writing out live data in the same format
  in response to messages from a MOM. By closing the dichotomy we
  would have the advantage that a single implementation is able to
  handle all cases.</p>

  <p>It shouldn't be a surprise that a dichotomous implementation
  of streaming, where a streaming transport is used only for live
  events is incompatible with http caching. If an event is streamed
  when it is new, but then when it is old made available for
  download, http caching between the two requests is impossible.
  However, where a single mode is used for both live and historic
  events the transport is wholly compatible with http caching.</p>

  <p>If we take streaming as a technique to achieve efficient
  downloading, not only for the transfer of forever-ongoing data,
  none of these approaches are particularly satisfactory.</p>

  <h2 id="json-and-xml"><a href="#json-and-xml"><span class=
  "header-section-number">3.6</span> Json and XML</a></h2>

  <p>Although AJAX started as a means to transfer XML, today JSON
  "The fat-free alternative to XML<span class="citation">(Douglas
  2009)</span>" is the more popular serialisation format. The goals
  of XML were to simplify SGML to the point that a graduate student
  would be able to implement a parser in a week [@javaxml p ???].
  For the student tackling JSON a few hours with a parser generator
  should surfice, being expressable in 15 CFGs. Indeed, because
  JSON is a strict subset of Javascript, in many cases the
  Javascript programmer requires no parser at all. Unimpeeded by
  SGML's roots as a document format, JSON provides a much more
  direct analogue to the metamodel of a canonical modern
  programming language with entities such as <em>string</em>,
  <em>number</em>, <em>object</em> and <em>array</em>. By closely
  mirroring a programmer's metamodel, visualising a mapping between
  a domain model and it's serialised objects becomes trivial.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{
   <span class="dt">people</span>: [
      {<span class="dt">name</span>: <span class=
"st">'John'</span>, <span class="dt">town</span>:<span class=
"st">'Oxford'</span>},
      {<span class="dt">name</span>: <span class=
"st">'Jack'</span>, <span class="dt">town</span>:<span class=
"st">'Bristol'</span>}
   ]
}</code>
</pre>

  <p>This close resemblance to the model of the programming in some
  cases causes fast-changing formats.</p>

  <p>Like XML attributes, as a serialised text format, JSON objects
  have an order but are almost always parsed to and from orderless
  maps meaning that the order of the keys/value pairings as seen in
  the stream usually follows no defined order. No rule in the
  format would forbid representing of an ordered map in an ordered
  way but most tools on receiving such a message would ignore the
  ordering.</p>

  <p>(MINE SOA assignment). Also the diagram.</p>

  <h2 id="parsing-sax-and-dom"><a href=
  "#parsing-sax-and-dom"><span class=
  "header-section-number">3.7</span> Parsing: SAX and Dom</a></h2>

  <p>In the XML world two standard parser models exist, SAX and
  DOM, with DOM far the more popular. DOM performs a parse as a
  single evaluation, on the request of the programmer, returning an
  object model representing the whole of the document. At this
  level of abstraction the details of the markup are only distant
  concern. Conversely, SAX parsers are probably better considered
  as tokenisers, providing a very low-level event driven interface
  in line with the Observer pattern to notify the programmer of
  syntax as it is seen. Each element's opening and closing tag is
  noted</p>

  <p>This presents poor developer ergonomics by requiring that the
  programmer implement the recording of state with regard to the
  nodes that they have seen. For programmers using SAX, a
  conversion to their domain objects is usually implemented
  imperatively. This programming tends to be difficult to read and
  programmed once per usage rather than assembled as the
  combination of reusable parts. For this reason SAX is usually
  reserved for fringe cases where messages are very large or memory
  unusually scarce.</p>

  <p>DOM isn't just a parser, it is also a cross-language defined
  interface for manipulating the XML in real time, for example to
  change the contents of a web page in order to provide some
  interactivity. In JSON world, DOM-style parser not referring to
  the DOM spec, or what browser makers would mean. Rather,
  borrowing from the XML world to mean a parser which requires the
  whole file to be loaded.</p>

  <p>Suppose we want to extract the name of the first person. Given
  a DOM parser this is very easy:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">function</span> <span class=
"fu">nameOfFirstPerson</span>( myJsonString ) {

   <span class=
"co">// Extracting an interesting part from JSON-serialised data is</span>
   <span class=
"co">// relatively easy given a DOM-style parser. Unfortunately this</span>
   <span class=
"co">// forbids any kind of progressive consideration of the data.</span>
   <span class=
"co">// All recent browsers provide a JSON parser as standard. </span>

   <span class="kw">var</span> document = <span class=
"ot">JSON</span>.<span class="fu">parse</span>( myJsonString );
   <span class="kw">return</span> <span class=
"ot">document</span>.<span class="fu">people</span>[<span class=
"dv">0</span>].<span class="fu">name</span>; <span class=
"co">// that was easy!</span>
}</code>
</pre>

  <p>Contrast with the programming below which uses the clarinet
  JSON SAX parser. To prove that I'm not exaggerating the case, see
  published usages at [Clarinet demos].</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">function</span> <span class=
"fu">nameOfFirstPerson</span>( myJsonString, callbackFunction ){

   <span class=
"co">// The equivalent logic, expressed in the most natural way</span>
   <span class=
"co">// fora s JSON SAX parser is longer and much more </span>
   <span class=
"co">// difficult to read. The developer pays a high price for </span>
   <span class="co">// progressive parsing. </span>

   <span class="kw">var</span> clarinet = <span class=
"ot">clarinet</span>.<span class="fu">parser</span>(),
   
       <span class=
"co">// with a SAX parser it is the developer's responsibility </span>
       <span class=
"co">// to track where in the document the cursor currently is,</span>
       <span class=
"co">// requiring several variables to maintain.        </span>
       inPeopleArray = <span class="kw">false</span>,   
       inPersonObject = <span class="kw">false</span>,
       inNameAttribute = <span class="kw">false</span>,
       found = <span class="kw">false</span>;
   
   <span class="ot">clarinet</span>.<span class=
"fu">onopenarray</span> = <span class="kw">function</span>(){
      <span class=
"co">// for brevity we'll cheat by assuming there is only one</span>
      <span class=
"co">// array in the document. In practice this would be overly</span>
      <span class="co">// brittle.</span>
      
      inPeopleArray = <span class="kw">true</span>; 
   };
   
   <span class="ot">clarinet</span>.<span class=
"fu">onclosearray</span> = <span class="kw">function</span>(){
      inPeopleArray = <span class="kw">false</span>;
   };   
   
   <span class="ot">clarinet</span>.<span class=
"fu">onopenobject</span> = <span class="kw">function</span>(){
      inPersonObject = inPeopleArray; 
   };
   
   <span class="ot">clarinet</span>.<span class=
"fu">oncloseobject</span> = <span class="kw">function</span>(){
      inPersonObject = <span class="kw">false</span>;
   };   
      
   <span class="ot">clarinet</span>.<span class=
"fu">onkey</span> = <span class="kw">function</span>(key){
      inNameAttribute = ( inPeopleObject &amp;&amp; key == <span class="st">'name'</span>);
   };

   <span class="ot">clarinet</span>.<span class=
"fu">onvalue</span> = <span class="kw">function</span>(value){
      <span class=
"kw">if</span>( !found &amp;&amp; inNameAttribute ) {
         <span class="co">// finally!</span>
         <span class="fu">callbackFunction</span>( value );
         found = <span class="kw">true</span>;
      }
   };      
   
   <span class="ot">clarinet</span>.<span class=
"fu">write</span>(myJsonString);   
}</code>
</pre>

  <p>As we can see above, SAX's low-level semantics require a
  lengthy expression and for the programmer to maintain state
  regarding the position in the document -- usually recording the
  ancestors seen on the descent from the root to the current node
  -- in order to identify the interesting parts. This order of the
  code is also quite unintuitive; generally event handlers will
  cover multiple unrelated concerns and each concern will span
  multiple event handlers. This lends to programming in which
  separate concerns are not separately expressed in the code.</p>

  <h2 id="common-patterns-when-connecting-to-rest-services">
  <a href="#common-patterns-when-connecting-to-rest-services"><span class="header-section-number">
  3.8</span> Common patterns when connecting to REST
  services</a></h2>

  <p>Marshaling provides two-way mapping between a domain model and
  a serialisation as JSON or XML, either completely automatically
  or based on a declarative specification. To handle a fetched rest
  response it is common to automatically demarshal it so that the
  application may make use of the response from inside its own
  model, no differently from objects assembled in any other way.
  From the perspective of the programmer it is as if the domain
  objects themselves had been fetched. Another common design
  pattern, intended to give a degree of isolation between concerns,
  is to demarshal automatically only so far as Data Transfer
  Objects (DTOs), instances of classes which implement no logic
  other than storage, and from there programmatically instantiate
  the domain model objects. Going one step further, for JSON
  resources sent to loosely-typed languages with a native
  representation of objects as generic key-value pairs such as
  Javascript or Clojure, the marshaling step is often skipped: the
  output from the parser so closely resembles the language's
  built-in types that it is simplest to use it directly. Depending
  on the programming style adopted we might say that the JSON
  parser's output <em>is</em> the DTO and create domain model
  objects based on it, or that no further instantiation is
  necessary.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Degrees of automatic marshaling. From marshaling directly to domain objects, DTOs, using parser output as a DTO, or using objects directly. Distinguish work done by library vs application programmer's domain">

    <p class="caption"><em>Degrees of automatic marshaling</em>.
    From marshaling directly to domain objects, DTOs, using parser
    output as a DTO, or using objects directly. Distinguish work
    done by library vs application programmer's domain</p>
  </div>

  <p>Ultimately the degree of marshaling that is used changes only
  the level of abstraction of the resource that the REST client
  library hands over to the application developer. Regardless of
  the exact form of the response model, the developer will usually
  programmatically extract one or more parts from it via calls in
  the programming language itself. For example, on receiving a
  resource de-marshaled to domain objects, a Java developer will
  inspect it by calling a series of getters in order to narrow down
  to the interesting parts. This is not to say that the whole of
  the message might not in some way be interesting, only that by
  using it certain parts will need to be identified as distinct
  areas of concern.</p>
  <pre class="sourceCode java">
<code class="sourceCode java"><span class=
"co">// An example programmatic approach to a domain model interrogation </span>
<span class=
"co">// under Java; upon receiving a list of people, each person's name</span>
<span class=
"co">// is added to a database. The methods used to drill down to the</span>
<span class=
"co">// pertinent components of the response are all getters: getPeople, </span>
<span class="co">// getGivenName, and getSurname. </span>
<span class="dt">void</span> <span class=
"fu">handleResponse</span>( RestResponse response ) {

   <span class="kw">for</span>( Person p : response.<span class=
"fu">getPeople</span>() ) {
      <span class="fu">addNameToDb</span>( p.<span class=
"fu">getGivenName</span>(), p.<span class=
"fu">getSurname</span>() );
   }   
}</code>
</pre>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"co">// Although in this Javascript example the objects passed to the handler </span>
<span class=
"co">// remain in the form given by the JSON parser, containing no domain-specific</span>
<span class=
"co">// getters, the programming represents a different expression of the same </span>
<span class="co">// basic process.</span>
<span class="kw">function</span> <span class=
"fu">handleResponse</span>( response ){

   <span class="ot">response</span>.<span class=
"ot">people</span>.<span class="fu">forEach</span>( <span class=
"kw">function</span>( person ){
      <span class="fu">addNameToDb</span>( <span class=
"ot">p</span>.<span class="fu">givenName</span>, <span class=
"ot">p</span>.<span class="fu">surname</span> );
   });
}</code>
</pre>

  <p>Because it is applied directly to the metamodel of the
  language[^ It could be argued that getters aren't a part of the
  metamodel of Java itself, but they form such a common pattern
  that it is a part ], this extraction has become such a natural
  component of a workflow that it maye be used while thinking of it
  as wholly unremarkable. In the examples above we are interacting
  with the model in the way that the language makes the most easy
  to conceptualise. However se should consider that, however subtly
  embedded, the technique is an invented construct and only one of
  the possible formulations which might have been drawn.</p>

  <p>One weakness of this inspection model is that, once much code
  is written to interrogate models in this way, the interface of
  the model becomes increasingly expensive to change as the code
  making the inspections becomes more tightly coupled with the
  thing that it is inspecting. Taking the above example, if the
  model were later refactored such that the concepts of firstName
  and surName were pulled from the Person class into an extracted
  Name class, because the inspection relies on a sequence of calls
  made directly into domain objects, the code making the query
  would also have to change. Whilst following the object oriented
  principle of encapsulation of data, such that the caller does not
  have to concern themselves with the data structures hidden behind
  the getter, there is no such abstraction for when the structure
  itself changes. Given an Agile environment where the shape of
  data is refactored regularly, this would be a problem when
  programming against any kind of resource; for example, if change
  of objects formats propagates knock-on changes where ever the
  object is used it is very difficult to commit small diffs to the
  VCS which make incremental changes to a tightly focused area of
  the system. A method of programming which truly embraced extreme
  programming would allow constant change without disparate, barely
  related parts having to be modified in parallel when structural
  refactoring occurs. The coupling is all the more acute where the
  format of the item being inspected is defined by an independently
  maintained service.</p>

  <p><em>contagion problem</em></p>

  <p>Extraneous changes dilute the changelog, making it less easily
  defined by code changes which are intrinsically linked to the
  actual change in the logic being expressed by the program, and
  therefore to the thinking behind the change and the reason for
  the change.</p>

  <h2 id="jsonpath-and-xpath"><a href=
  "#jsonpath-and-xpath"><span class=
  "header-section-number">3.9</span> JsonPath and XPath</a></h2>

  <p>Both the above difficulty in identifying the interesting parts
  of a message whilst using a streaming parser and the problem with
  tight coupling of programmatic drilling down to REST formats
  leads me to search for areas where this problem has already been
  solved.</p>

  <p>In the domain of markup languages there are associated query
  languages such as XPATH whose coupling is loose enough that their
  expressions may continue to function after the exact shape of a
  message is refactored. While observing this is nothing more
  radical than using the query languages in more-or-less they were
  intended, their employment is not the most natural coming from a
  programming context in which the application developer's
  responsibilities usually start where the demarshaler's end.
  Consider the following XML:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;givenName&gt;</span>...<span class=
"kw">&lt;/givenName&gt;</span>   
      <span class="kw">&lt;familyName&gt;</span>Bond<span class=
"kw">&lt;/familyName&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>The XPath //person[0]//surname//text() would continue to
  identify the correct part of the resource without being updated
  after the xml analogue of the above Java Name refactor:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class="kw">&lt;givenName&gt;</span>...<span class=
"kw">&lt;/givenName&gt;</span>
         <span class="kw">&lt;familyName&gt;</span>Bond<span class=
"kw">&lt;/familyName&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>Luckily in JSON there exists already an attempt at an
  equivalent named Jsonpath. JsonPath closely resembles the
  javascript code which would select the same nodes. Not a real
  spec.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
<span class=
"co">// an in-memory person with a multi-line address:</span>
<span class="kw">let</span> person = {
   <span class="dt">name</span>: {<span class=
"dt">givenName</span>:<span class="st">''</span>, <span class=
"dt">familyName</span>:<span class="st">''</span>},
   <span class="dt">address</span>: [
      <span class="st">"line1"</span>,
      <span class="st">"line2"</span>,
      <span class="st">"line3"</span>
   ]
}


<span class=
"co">// in javascript we can get line two of the address as such:</span>
<span class="kw">let</span> address = <span class=
"ot">person</span>.<span class="fu">address</span>[<span class=
"dv">2</span>]

<span class=
"co">// the equivalent jsonpath expression is identical:</span>
<span class="kw">let</span> jsonPath = <span class=
"st">"person.address[2]"</span>

<span class=
"co">// although jsonpath also allows ancestor relationships which are not</span>
<span class=
"co">// expressible quite so neatly as basic Javascript:</span>
<span class="kw">let</span> jsonPath2 = <span class=
"st">"person..given"</span></code>
</pre>

  <p>Xpath is able to express identifiers which often survive
  refactoring because XML represents a tree, hence we can consider
  relationships between entities to be that of contains/contained
  in (also siblings?). In application of XML, in the languages that
  we build on top of XML, it is very natural to consider all
  elements to belong to their ancestors. Examples are myriad, for
  example consider a word count in a book written in DOCBook format
  - it should be calculable without knowing if the book is split
  into chapters or not since this is a concept internal to the
  organisation of the book itself nd not something that a querier
  is likely to find interesting - if this must be considered the
  structure acts as barrier to information rather than enabling the
  information's delivery. Therefore, in many cases the exact
  location of a piece of information is not as important as a more
  general location of x being in some way under y.</p>

  <p>This may not always hold. A slightly contrived example might
  be if we were representing a model of partial knowledge:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class=
"kw">&lt;isNot&gt;&lt;surname&gt;</span>Bond<span class=
"kw">&lt;/surname&gt;&lt;/isNot&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>The typical use pattern of XPath or JSONPath is to search for
  nodes once the whole serialisation has been parsed into a
  DOM-style model. JSONPath implementation only allows for
  search-type usage: <a href=
  "https://code.google.com/p/jsonpath/To">https://code.google.com/p/jsonpath/To</a>
  examine a whole document for the list of nodes that match a
  jsonpath expression the whole of the tree is required. But to
  evaluate if a single node matches an expression, only the
  <em>path of the descent from the root to that node</em> is
  required -- the same state as a programmer usually maintains
  whilst employing a SAX parser. This is possible because JSONPath
  does not have a way to express the relationship with sibling
  nodes, only ancestors and decedents.</p>

  <p>One limitation of the JSONPath language is that it is not
  possible to construct an 'containing' expression. CSS4 allows
  this in a way that is likely to become familiar to web developers
  over the next five years or so.</p>

  <h2 id="testing"><a href="#testing"><span class=
  "header-section-number">3.10</span> Testing</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Relationship between the main players in the JS testing landscape. JSTD, Karma, Jasmine, NodeUnit, jasmine-node, Browsers">

    <p class="caption">Relationship between the main players in the
    JS testing landscape. JSTD, Karma, Jasmine, NodeUnit,
    jasmine-node, Browsers</p>
  </div>

  <p>By the commonjs spec, test directory should be called 'test'
  (<a href=
  "http://wiki.commonjs.org/wiki/Packages/1.0">http://wiki.commonjs.org/wiki/Packages/1.0</a>#Package_Directory_Layout)
  doesn't matter for my project since not using commonjs, but might
  as well stick to the convention.</p>

  <p>How TDD helps How can fit into methodology</p>

  <ul>
    <li>JSTD</li>

    <li>NodeUnit</li>

    <li>Karma</li>

    <li>Jasmine</li>
  </ul>

  <p>Initially started with jstestdriver but found it difficult.
  Karma started because engineers working on the Angular project in
  Google were "struggling a lot with jstd": <a href=
  "http://www.youtube.com/watch?v=MVw8N3hTfCI">http://www.youtube.com/watch?v=MVw8N3hTfCI</a>
  - jstd is a google project Even Jstd's authors seems to be
  disowning it slightly. Describe what was once its main mode of
  operation as now being for stress testing of jstd itself only.
  Problems: browsers become unresponsive. Generally unreliable, has
  to be restarted frequently.</p>

  <p>JSTD, as a Java program, is difficult to start via Grunt. Also
  an issue that Grunt post-dates Karma by enough that JSTD doesn't
  have the attention of the Grunt community.</p>

  <h1 id="application-and-reflection"><a href=
  "#application-and-reflection"><span class=
  "header-section-number">4</span> Application and
  Reflection</a></h1><!---
**40 to 60 pages**
=-->

  <p>What a Micro-library is. What motivates the trend? This
  library has a fairly small set of functionality, it isn't a
  general purpose do-everything library like jQuery so its size
  will be looked at more critically if it is too large. Micro
  library is the current gold standard for compactness. Still, have
  a lot to do in not very much code.</p>

  <p>Relationship between type of a node and its purpose in the
  document. Purpose is often obvious from a combination of URL and
  type so can disregard the place in the document. This structure
  may be carefully designed but ultimately a looser interpretation
  of the structure can be safer.</p>

  <p>Interestingly, the mixed paradigm design hasn't changed the
  top-level design very much from how it'd be as a pure OO project
  (IoC, decorators, event filters, pub/sub etc).</p>

  <p>Why SAX is dumb: As a principle, the programmer should only
  have to handle the cases which are interesting to them, not wade
  manually through a haystack in search of a needle, which means
  the library should provide an expressive way of associating the
  nodes of interest with their targetted callbacks.</p>

  <h2 id="js-code-style"><a href="#js-code-style"><span class=
  "header-section-number">4.1</span> JS code style</a></h2>

  <p>Javascript: not the greatest for 'final' elegant presentation
  of programming. Does allow 'messy' first drafts which can be
  refactored into beautiful code. Ie, can write stateful and
  refactor in small steps towards being stateless. An awareness of
  beautiful languages lets us know the right direction to go in. An
  ugly language lets us find something easy to write that works to
  get us started. Allows a very sketchy program to be written,
  little more than a programming scratchpad.</p>

  <p>Without strict typing, hard to know if program is correct
  without running it. In theory (decidability) and in practice
  (often find errors through running and finding errors thrown).
  Echo FPR: once compiling, good typing tends to give a reasonable
  sureness that the code is correct.</p>

  <p>Criticisms of Node. Esp from Erlang etc devs. Pyramid code and
  promises.</p>

  <p>Although the streams themselves are stateful, because they are
  based on callbacks it is entirely possible to use them from a
  component of a javascript program which is wholly stateless.</p>

  <h2 id="something-else"><a href="#something-else"><span class=
  "header-section-number">4.2</span> something else</a></h2>

  <p>A feature set which is minimal but contain no obvious
  omissions.</p>

  <p>Under the heading [Anatomy of a SOA client] I deconstructed
  the way in which programming logic is often used to identify the
  parts of a model which are currently interesting and started to
  look at some declarative ways in which these parts can be
  obtained.</p>

  <p>Turn this model inside out. Instead of the programmer finding
  the parts they want as a part of the general logic of the
  program, declaratively define the interesting parts and have
  these parts delivered to the language logic. Once we make the
  shift to thinking in this way, it is no longer necessary to have
  the whole resource locally before the interesting sub-parts are
  delivered.</p>

  <p>Focus on replacing ajax, rather than streaming. In older
  browsers, getting the whole message at once is no worse than it
  is now.</p>

  <div class="figure">
    <img src="images/timeline.png" alt=
    "Over several hops of aggregation, the benefits of finding the interesting parts early">

    <p class="caption">Over several hops of aggregation, the
    benefits of finding the interesting parts early</p>
  </div>

  <h2 id="resume-on-failure"><a href=
  "#resume-on-failure"><span class=
  "header-section-number">4.3</span> resume on failure</a></h2>

  <p>Http 1.1 provides a mechanism for Byte Serving via the
  Accepts-Ranges header [<a href=
  "http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html">http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html</a>#sec14.5]
  which can be used to request any contiguous part of a response
  rather than the whole. Common in download managers but not REST
  clients. This ability can be used to. Why not this one. Resume on
  a higher semantic level.</p>

  <h2 id="delivery-methodology"><a href=
  "#delivery-methodology"><span class=
  "header-section-number">4.4</span> delivery methodology</a></h2>

  <p>Because Kanban focusses on always having a potentially
  releasable product, it mitigates problems which could otherwise
  lead to non-delivery and allows the direction to be changed while
  the project is in progress. For each unit of work (under Kanban,
  a card), an entire vertical slice of planning, design,
  implementation and reflection must be complete before going onto
  the next card. Alongside each software feature, every written
  chapter will be expanded and refactored in much the same way as
  the code. Just as for well designed software, the order of
  implementation should not be apparent to a user, my plan is that
  the written work should not feel disjointed for having been
  written non-sequentially. I plan to manage the Kanban process
  using paper only, with cards on a physical board.</p>

  <h2 id=
  "overall-design-philosophy-and-breaking-out-of-bigsmall-tradeoff">
  <a href=
  "#overall-design-philosophy-and-breaking-out-of-bigsmall-tradeoff">
  <span class="header-section-number">4.5</span> overall design
  philosophy and breaking out of big/small tradeoff</a></h2>

  <p>In which a callback call is received not just when the whole
  resource is downloaded but for every interesting part which is
  seen while the transfer is ongoing. The definition of
  'interesting' will be generic and accommodating enough so as to
  apply to any data domain and allow any granularity of interest,
  from large object to individual datums. With just a few lines of
  programming</p>

  <p>Best of both modes</p>

  <p>Granularity: only need read as far as necessary. Services
  could be designed to write the big picture first. Alternatively,
  where resources link to one another, can stop reading at the
  link. Eg, looking for a person's publications, start with an
  index of people but no need to read whole list of people.</p>

  <p>Aborting http request may not stop processing on the server.
  Why this is perhaps desirable - transactions, leaving resources
  in a half-complete state.</p>

  <h2 id="choice-of-technologies"><a href=
  "#choice-of-technologies"><span class=
  "header-section-number">4.6</span> choice of
  technologies</a></h2>

  <p>can justify why js as:</p>

  <p>Most widely deployable.</p>

  <p>Node: asynchronous model built into language already, no
  'concurrent' library needed. Closures convenient for picking up
  again where left off.</p>

  <p>Node programs often so asynchronous and callback based they
  become unclear in structure. Promises approach to avoid
  pyramid-shaped code and callback spaghetti.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"co">// example of pyramid code</span></code>
</pre>

  <p>In comparison to typical Tomcat-style threading model.
  Threaded model is powerful for genuine parallel computation but
  Wasteful of resources where the tasks are more io-bound than
  cpu-bound. Resources consumed by threads while doing nothing but
  waiting.</p>

  <p>Compare to Erlang. Waiter model. Node resturaunt much more
  efficient use of expensive resources.</p>

  <p>funcitonal, pure functional possible [FPR] but not as nicely
  as in a pure functional language, ie function caches although can
  be implemented, not universal on all functions.</p>

  <p>easy to distribute softare (npm etc)</p>

  <h2 id="summary-of-json"><a href="#summary-of-json"><span class=
  "header-section-number">4.7</span> summary of json</a></h2>

  <p>Why json?</p>

  <p>A bridge between languages that isn't too different from the
  common types in the languages themselves a common bridge between
  languages</p>

  <p>Also very simple. Easy to parse.</p>

  <h2 id="creating-a-losely-coupled-reader"><a href=
  "#creating-a-losely-coupled-reader"><span class=
  "header-section-number">4.8</span> creating a losely coupled
  reader</a></h2>

  <p>Programming to identify a certain interesting part of a
  resource today should with a high probability still work when
  applied to future releases.</p>

  <p>Requires a small amount of discipline on behalf of the service
  provider: Upgrade by adding of semantics only most of the time
  rather than changing existing semantics.</p>

  <p>Adding of semantics should could include adding new fields to
  objects (which could themselves contain large sub-trees) or a
  "push-down" refactor in which what was a root node is pushed down
  a level by being suspended from a new parent. See</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "extended json rest service that still works - maybe do a table instead ">

    <p class="caption">extended json rest service that still works
    - maybe do a table instead</p>
  </div>

  <p>(CITE: re-read citations from SOA)</p>

  <h2 id="design-of-the-jsonpath-parser"><a href=
  "#design-of-the-jsonpath-parser"><span class=
  "header-section-number">4.9</span> Design of the jsonpath
  parser</a></h2>

  <p>Explain why Haskel/lisp style lists are used rather than
  arrays</p>

  <ul>
    <li>In parser clauses, lots of 'do this then go to the next
    function with the rest'.</li>

    <li>Normal arrays extremely inefficient to make a copy with one
    item popped off the start</li>

    <li>Link to FastList on github</li>

    <li>For sake of micro-library, implemented tiny list code with
    very bare needed</li>

    <li>Alternative (first impl) was to pass an index around</li>

    <li>But clause fns don't really care about indexes, they care
    about top of the list.</li>

    <li>Slight advantage to index: allows going past the start for
    the root path (which doesn't have any index) instead, have to
    use a special value to keep node and path list of the same
    length</li>

    <li>Special token for root, takes advantage of object identity
    to make certain that cannot clash with something from the json.
    Better than '<strong>root</strong>' or similar which could
    clash. String in js not considered distinct, any two strings
    with identical character sequences are indistinguishable.</li>
  </ul>

  <p>Anti-list: nothing is quite so small when making a
  mircro-library as using the types built into the language, coming
  as they are for zero bytes.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Diagram showing why list is more memory efficient - multiple handles into same structure with different starts, contrast with same as an array">

    <p class="caption">Diagram showing why list is more memory
    efficient - multiple handles into same structure with different
    starts, contrast with same as an array</p>
  </div>

  <ul>
    <li>For recognisably with existing code, use lists internally
    but transform into array on the boundary between Oboe.js and
    the outside world (at same time, strip off special 'root path'
    token)</li>
  </ul>

  <p>In parser, can't use 'y' flag to the regualr expression engine
  which would allow much more elegant matching. Only alternative is
  cumersome: to slice the string and match all tokens with regexes
  starting with '^' in order to track the current location.
  [<a href=
  "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular/_Expressions">https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular\_Expressions</a>]</p>

  <h2 id="incrementally-building-up-the-content"><a href=
  "#incrementally-building-up-the-content"><span class=
  "header-section-number">4.10</span> Incrementally building up the
  content</a></h2>

  <p>Like SAX, calls from clarinet are entirely 'context free'. Ie,
  am told that there is a new object but without the preceding
  calls the root object is indistinguishable from a deeply nested
  object. Luckily, it should be easy to see that building up this
  context is a simple matter of maintaining a stack describing the
  descent from the root node to the current node.</p>

  <p>jsonPath parser gets the output from the
  incrementalParsedContent, minimally routed there by the
  controller.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Show a call into a compiled jsonPath to explain coming from incrementalParsedContent with two lists, ie the paths and the objects and how they relate to each other. Can use links to show that object list contains objects that contain others on the list. Aubergine etc example might be a good one">

    <p class="caption">Show a call into a compiled jsonPath to
    explain coming from incrementalParsedContent with two lists, ie
    the paths and the objects and how they relate to each other.
    Can use links to show that object list contains objects that
    contain others on the list. Aubergine etc example might be a
    good one</p>
  </div>

  <p>Explain match starting from end of candidate path</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Some kind of diagram showing jsonPath expressions and functions partially completed to link back to the previous function. Include the statementExpr pointing to the last clause">

    <p class="caption">Some kind of diagram showing jsonPath
    expressions and functions partially completed to link back to
    the previous function. Include the statementExpr pointing to
    the last clause</p>
  </div>

  <p>On first attempt at ICB, had two stacks, both arrays, plus
  reference to current node, current key and root node. After
  refactorings, just one list was enough. Why single-argument
  functions are helpful (composition etc)</p>

  <p>Stateless makes using a debugger easier - can look back in
  stack trace and because of no reassignment, can see the whole,
  unchanged state of the parent call. What the params are now are
  what they always have been, no chance of reassignment (some code
  style guides recommend not to reassign parameters but imperative
  languages generally do not forbid it) No Side effects: can type
  expressions into debugger to see evaluation without risk of
  changing program execution.</p>

  <h2 id="identifying-interesting-objects-in-the-stream"><a href=
  "#identifying-interesting-objects-in-the-stream"><span class=
  "header-section-number">4.11</span> identifying interesting
  objects in the stream</a></h2>

  <p>NB: This consideration of type in json could be in the
  Background section.</p>

  <p>Xml comes with a strong concept of the <em>type</em> of an
  element, the tag name is taken as a more immediate fundamental
  property of the thing than the attributes. For example, in
  automatic json-Java object demarshallers, the tag name is always
  mapped to the Java class. In JSON, other than the base types
  common to most languages (array, object, string etc) there is no
  further concept of type. If we wish to build a further
  understanding of the type of the objects then the realtionship
  with the parent object, expressed by the attribute name, is more
  likely to indicate the type. A second approach is to use duck
  typing in which the relationship of the object to its ancestors
  is not examined but the properties of the object are used instead
  to communicate an enhanced concept of type. For example, we might
  say that any object with an isbn and a title is a book.</p>

  <p>Duck typing is of course a much looser concept than an XML
  document's tag names and collisions are possible where objects
  co-incidentally share property names. In practice however, I find
  the looseness a strength more often than a weakness. Under a
  tag-based marshalling from an OO language, sub-types are assigned
  a new tag name and as a consumer of the document, the 'isa'
  relationship between a 'class' tagname and it's 'sub-tabname' may
  be difficult to track. It is likely that if I'm unaware of this,
  I'm not interested in the extended capabilities of the subclass
  and would rather just continue to recieve the base superclass
  capabilities as before. Under duck typing this is easy - becase
  the data consumer lists the</p>

  <p>A third injection of type into json comes in the form of
  taking the first property of an object as being the tagname.
  Unsatisfactory, objects have an order while serialised as json
  but once deserialised typically have no further order.
  Clarinet.js seems to follow this pattern, notifying of new
  objects only once the first property's key is known so that it
  may be used to infer type. Can't be used with a general-purpose
  JSON writer tool, nor any JSON writer tool that reads from common
  objects.</p>

  <p>Design not just for now, design to be stable over future
  iterations of the software. Agile etc.</p>

  <p>Why an existing jsonPath implmentation couldn't be used: need
  to add new features and need to be able to check against a path
  expressed as a stack of nodes.</p>

  <p>More important to efficiently detect or efficiently compile
  the patterns?</p>

  <p>As discussed in section ???, Sax is difficult to program and
  not widely used.</p>

  <p>First way to identify an interesting thing is by its location
  in the document. In the absense of node typing beyond the
  categorisation as objects, arrays and various primative types,
  the key immediately mapping to the object is often taken as a
  lose concept of the type of the object. Quite fortunately, rather
  than because of a well considered object design, this tends to
  play well with automatically marshaling of domain objects
  expressed in a Java-style OO language because there is a stong
  tendency for field names -- and by extension, 'get' methods -- to
  be named after the <em>type</em> of the field, the name of the
  type also serving as a rough summary of the relationship between
  two objects. See figure below.</p>

  <div class="figure">
    <img src="images/marshall.png" alt=
    "UML class diagram showing a person class in relationship with an address class. In implementation as Java the 'hasAddress' relationship would typically be reified as a getAddress method. This co-incidence of object type and the name of the field referring to the type lends itself well to the tendency for the immediate key before an object to be taken as the type when Java models are marshaled into json ">

    <p class="caption">UML class diagram showing a person class in
    relationship with an address class. In implementation as Java
    the 'hasAddress' relationship would typically be reified as a
    getAddress method. This co-incidence of object type and the
    name of the field referring to the type lends itself well to
    the tendency for the immediate key before an object to be taken
    as the type when Java models are marshaled into json</p>
  </div>

  <p>By sensible convention, even in a serialisation format with
  only a loose definition of lists, lists contain only items of the
  same type. This gives way to a sister convention, that for lists
  of items, the key immediately linking to the</p>

  <p>Essentially two ways to identify an interesting node - by
  location (covered by existing jsonpath)</p>

  <p>Why duck typing is desirable in absense of genuine types in
  the json standard (ala tag names in XML). or by a loose concept
  of type which is not well supported by existing jsonpath
  spec.</p>

  <p>Compare duck typing to the tag name in</p>

  <p>To extend JsonPath to support a concise expression of duck
  typing, I chose a syntax which is similar to fields in
  jsonFormat:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
<span class=
"co">// the curly braces are my extension to jsonpath"</span>
<span class="kw">let</span> jsonPath = <span class=
"fu">jsonPathCompiler</span>(<span class=
"st">"{name, address, email}"</span>);

<span class=
"co">// the above jsonPath expression would match this object in json expression and </span>
<span class=
"co">// like all json path expressions the pattern is quite similar to the object that</span>
<span class=
"co">// it matches. The object below matches because it contains all the fields listed</span>
<span class=
"co">// in between the curly braces in the above json path expresson.</span>

<span class="kw">let</span> matchingObject = {
   <span class="st">"name"</span>: <span class="st">"..."</span>,
   <span class="st">"address"</span>: <span class=
"st">"..."</span>,
   <span class="st">"email"</span>: <span class="st">"...:</span>
}

<span class="fu">jsonPath</span>(matchingObject); <span class=
"co">// evaluates to true</span></code>
</pre>

  <p>When we aer searching</p>

  <h2 id="program-design"><a href="#program-design"><span class=
  "header-section-number">4.12</span> program design</a></h2>

  <div class="figure">
    <img src="images/overallDesign.png" alt=
    "Overall design of Oboe.js. Nodes in the diagram represent division of control so far that it has been split into different files.">

    <p class="caption">Overall design of Oboe.js. Nodes in the
    diagram represent division of control so far that it has been
    split into different files.</p>
  </div>

  <h2 id="incrementally-building-up-a-model"><a href=
  "#incrementally-building-up-a-model"><span class=
  "header-section-number">4.13</span> incrementally building up a
  model</a></h2>

  <p>A refactoring was used to separate logic and state:</p>

  <ul>
    <li>Take stateful code</li>

    <li>Refactor until there is just one stateful item</li>

    <li>This means that that item is reassigned rather than
    mutated</li>

    <li>Make stateless by making all functions take and return an
    instance of that item<br></li>

    <li>Replace all assignment of the single stateful var with a
    return statement</li>

    <li>Create a simple, separate stateful controller that just
    updates the state to that returned from the calls</li>
  </ul>

  <p>Very testable code because stateless - once correct for params
  under test, will always be correct. Nowhere for bad data to hide
  in the program.</p>

  <p>How do notifications fit into this?</p>

  <p>By going to List-style, enforced that functions fail when not
  able to give an answer. Js default is to return the special
  'undefined' value. Why this ensured more robustness but also
  sometimes took more code to write, ie couldn't just do if(
  tail(foo)) if foo could be empty but most of the time that would
  be correct</p>

  <p>Stateful controller very easy to test - only 1 function.</p>

  <h2 id="styles-of-programming"><a href=
  "#styles-of-programming"><span class=
  "header-section-number">4.14</span> styles of
  programming</a></h2>

  <p>The code presented is the result of the development many prior
  versions, it has never been rewritten in the sense of starting
  again. Nontheless, every part has been complely renewed several
  times. I am reviewing only the final version. Git promotes
  regular commits, there have been more than 500.</p>

  <p>some of it is pure functional (jsonPath, controller) ie, only
  semantically different from a Haskell programme others,
  syntactically functional but stateful to fit in with expected
  APIs etc</p>

  <p>JsonPath implementation allows the compilation of complex
  expressions into an executable form, but each part implementing
  the executable form is locally simple. By using recursion,
  assembling the simple functions into a more function expressing a
  more complex rule also follows as being locally simple but
  gaining a usefully sophisticated behaviour through composition of
  simple parts. Each recursive call of the parser identifies one
  token for non-empty input and then recursively digests the
  rest.</p>

  <p>The style of implementation of the generator of functions
  corresponding to json path expressions is reminiscent of a
  traditional parser generator, although rather than generating
  source, functions are dynamically composed. Reflecting on this,
  parser gens only went to source to break out of the ability to
  compose the expressive power of the language itself from inside
  the language itself. With a functional approach, assembly from
  very small pieces gives a similar level of expressivity as
  writing the logic out as source code.</p>

  <p>Why could implement Function#partial via prototype. Why not
  going to. Is a shame. However, are using prototype for minimal
  set of polyfills. Not general purpose.</p>

  <p>Different ways to do currying below:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
<span class="co">// function factory pattern (CITEME)</span>
<span class="kw">function</span> <span class=
"fu">foo</span>(a,b,c) {
   <span class="kw">return</span> <span class=
"kw">function</span> <span class=
"fu">partiallyCompleted</span>(d,e,f) {
   
      <span class=
"co">// may refer to partiallyCompleted in here</span>
   }
}

<span class="kw">function</span> <span class=
"fu">fooBar</span>(a,b,c,d,e,f) {
}

<span class="fu">partial</span>(fooBar, a,b);</code>
</pre>

  <p>Partial completion is implemented using the language itself,
  not provided by the language.</p>

  <p>Why would we choose 1 over the other? First simpler from
  caller side, second more flexible. Intuitive to call as a single
  call and can call self more easily.</p>

  <p>In same cases, first form makes it easier to communicate that
  the completion comes in two parts, for example:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"> <span class=
"fu">namedNodeExpr</span>(previousExpr, capturing, name, pathStack, nodeStack, stackIndex )</code>
</pre>

  <p>There is a construction part (first 3 args) and a usage part
  (last three). Comsume many can only be constructed to ues consume
  1 in second style because may refer to its own paritally
  completed version.</p>

  <p>In first case, can avoid this: <code>consume1(
  partialComplete(consumeMany, previousExpr, undefined, undefined),
  undefined, undefined, pathStack, nodeStack, stackIndex);</code>
  because function factory can have optional arguments so don't
  have to give all of them</p>

  <p>Function factory easier to debug. 'Step in' works. With
  partialCompletion have an awkward proxy function that breaks the
  programmer's train of thought as stepping through the code.</p>

  <p>Why it is important to consider the frame of mind of the coder
  (CITEME: Hackers and Painters) and not just the elegance of the
  possible language expressions.</p>

  <p>If implementing own functional caching, functional cache
  allows two levels of caching. Problematic though, for example no
  way to clear out the cache if memory becomes scarce.</p>

  <p>Functional programming tends to lend better to minification
  than OO-style because of untyped record objects (can have any
  keys).</p>

  <p>Lack of consistency in coding (don't write too much, leave to
  the conclusion)</p>

  <p>Final consideration of coding: packaging up each unit to
  export a minimal interface. * Why minimal interfaces are better
  for minification</p>

  <h2 id="the-mutability-problem"><a href=
  "#the-mutability-problem"><span class=
  "header-section-number">4.15</span> The mutability
  problem</a></h2>

  <p>Javascript provides no way to decalre an object with 'cohorts'
  who are allowed to change it whereas others cannot - vars may be
  hidden via use of scope and closures (CITE: crockford) but
  attributes are either mutable or immutable.</p>

  <p>Why this is a problem.</p>

  <ul>
    <li>bugs likely to be attributied to oboe because they'll be in
    a future <em>frame of execution</em>. But user error.</li>
  </ul>

  <p>Potential solutions:</p>

  <ul>
    <li>full functional-style immutability. Don't change the
    objects, just have a function that returns a new one with one
    extra property. Problem - language not optimised for this. A
    lot of copying. Still doesn't stop callback receiver from
    changing the state of hte object given. (CITE: optimisations
    other languages use)</li>

    <li>immutable wrappers.</li>

    <li>defensive cloning</li>

    <li>defining getter properties</li>
  </ul>

  <h2 id="performance-implications-of-functional-javascript">
  <a href=
  "#performance-implications-of-functional-javascript"><span class=
  "header-section-number">4.16</span> Performance implications of
  functional javascript</a></h2>

  <p>V8 and other modern JS engines are often said to be
  'near-native' speed, meaning it runs at close to the speed of a
  similarly coded C program. However, this relies on the programmer
  also coding in the style of a C programmer, for example with only
  mono-morphic callsites and without a functional style. Once
  either of those programming techniques is taken up performance
  drops rapidly [<a href=
  "http://rfrn.org/">http://rfrn.org/</a>~shu/2013/03/20/two-reasons-functional-style-is-slow-in-spidermonkey.html]
  9571 ms vs 504 ms. When used in a functional style, not
  'near-native' in the sense that not close to the performance
  gained by compiling a well designed functional language to
  natively executable code. Depends on style coded in, comparison
  to native somewhat takes C as the description of the operation of
  an idealised CPU rather than an abstract machine capable of
  executing on an actual CPU.</p>

  <p>(perhaps move to background, or hint at it, eg "although there
  are still some performance implications involved in a functional
  style, javascript may be used in a non-pure functional style") -
  with link to here</p>

  <p>The performance degradation, even with a self-hosted forEach,
  is due to the JIT&acirc;&euro;&trade;s inability to efficiently
  inline both the closures passed to forEach</p>

  <p>Lambda Lifting, currently not implemented in SpiderMonkey or
  V8: <a href=
  "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.4346">
  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.4346</a></p>

  <p>The transformations to enable the above criteria are tedious
  and are surely the purview of the compiler. All
  that&acirc;&euro;&trade;s needed are brave compiler hackers</p>

  <p>JS is much faster with "monomorphic call sites"</p>

  <p>However, js execution time is not much of a problem,</p>

  <h2 id="functions-over-constructors"><a href=
  "#functions-over-constructors"><span class=
  "header-section-number">4.17</span> functions over
  constructors</a></h2>

  <p>What constructors are in js. Any function, but usually an
  uppercase initial char indicates that it is intended to be used
  as a constructor.</p>

  <p>Inheritence is constructed using the language itself. While
  this is more flexible and allows each project to define a bespoke
  version of inherience to suit their particular needs or
  preferences, it also hampers portability more than an 'extends'
  keyword would.</p>

  <blockquote>
    <p>So far, the JavaScript community has not agreed on a common
    inheritance library (which would help tooling and code
    portability) and it is doubtful that that will ever happen.
    That means, we&acirc;&euro;&trade;re stuck with constructors
    under ECMAScript 5. <a href=
    "http://www.2ality.com/2013/07/defending-constructors.html">http://www.2ality.com/2013/07/defending-constructors.html</a></p>
  </blockquote>

  <p>Functions can be like Factories, gives me the flexability to
  chagne how something is created but by exposing a constructor are
  stuck with using 'new' to create an instance of exactly one
  type.</p>

  <p>Dart has 'factory' constructors which are called like
  constructors but act like factory functions: (<a href=
  "http://www.dartlang.org/docs/dart-up-and-running/contents/ch02.html">http://www.dartlang.org/docs/dart-up-and-running/contents/ch02.html</a>#ch02-constructor-factory)</p>

  <h2 id="targeting-node-and-the-browser"><a href=
  "#targeting-node-and-the-browser"><span class=
  "header-section-number">4.18</span> targeting node and the
  browser</a></h2>

  <p>Node+browser To use Node.js and</p>

  <p>Need to build an abstraction layer over xhr/xhr2/node. Can
  only work for packets in-order, for out-of-order packets
  something else happens.</p>

  <p>Use best of the capabilities of each.</p>

  <h2 id="packaging-the-library-as-a-single-distributable-file">
  <a href=
  "#packaging-the-library-as-a-single-distributable-file"><span class="header-section-number">
  4.19</span> Packaging the library as a single distributable
  file</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "packaging of many javascript files into multiple single-file packages. The packages are individually targeted at different execution contexts, either browsers or node get from notebook, split sketch diagram in half">

    <p class="caption">packaging of many javascript files into
    multiple single-file packages. The packages are individually
    targeted at different execution contexts, either browsers or
    node <em>get from notebook, split sketch diagram in
    half</em></p>
  </div>

  <ul>
    <li>One file for browser and node is common.</li>

    <li>say how this is done</li>

    <li>why not doing this (adds bloat, inhibits micro-lib)</li>

    <li>extra challenges</li>

    <li>http adaptor is different</li>

    <li>packaging is different</li>

    <li>two distributable files, for node minification is not
    important so don't do to help debugging.</li>
  </ul>

  <p>Composition of several source files into a distributable
  binary-like text file</p>

  <p>Why distributed javascript is more like a binary than a source
  file. Licencing implications? Would be (maybe) under GPL. Not so
  under BSD.</p>

  <p>Inherent hiding by wrapping in a scope.</p>

  <p>Names of functions and variable names which are provably not
  possible to reference are lost for the sake of reduction of size
  of the source.</p>

  <p>Packaging for node or browser. No need to minify for node but
  concatenation still done for ease of inclusion in projects</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">typical pattern <span class=
"kw">for</span> packaging to work <span class=
"kw">in</span> either a <span class="ot">node</span>.<span class=
"fu">js</span> <span class="fu">server</span> <span class=
"fu">or</span> <span class="fu">a</span> <span class=
"fu">web</span> <span class="fu">browser</span></code>
</pre>

  <p>Packaging for use in frameworks.</p>

  <ul>
    <li>Many frameworks already come with a wrapper arround the
    browser's inbuilt ajax capabilities</li>

    <li>
      <p>they don't add to the capabilities but present a nicer
      interface</p>
    </li>

    <li>
      <p>I'm not doing it but others are ** browser-packaged
      version should be use agnostic and therefore amenable to
      packaging in this way</p>
    </li>
  </ul>

  <p>Why uglify</p>

  <ul>
    <li>Covers whole language, not just a well-advised subset.</li>

    <li>Closure compiler works over a subset of javascript rather
    than the whole language.</li>
  </ul>

  <p>Why not require. Bits on what rq is can go into B&amp;R
  section. <em>Some of this can move into 3_Background.md</em></p>

  <ul>
    <li>What it is</li>

    <li>Why so popular</li>

    <li>Why a loader is necessary - js doesn't come with an import
    statement</li>

    <li>How it can be done in the language itself without an import
    statement</li>

    <li>Meant more for AMD than for single-load code</li>

    <li>Situations AMD is good for - large site, most visitors
    don't need all the code loaded</li>

    <li>Depends on run-time component to be loaded even after code
    has been optimised</li>

    <li>Small compatible versions exist that just do loading
    (almond)<br></li>

    <li>Why ultimately not suitable for a library like this - would
    require user to use Require before adopting it.</li>
  </ul>

  <p>Browserify is closer.</p>

  <ul>
    <li>Why it is better for some projects</li>

    <li>Very nearly meets my needs</li>

    <li>But http-compatability (<a href=
    "https://github.com/substack/http-browserify">https://github.com/substack/http-browserify</a>),
    while complete enough, isn't compact enough to not push project
    over micro-library size</li>
  </ul>

  <p>Testing post-packaging for small set of smoke tests. Can't
  test everything, only through public API.</p>

  <p>Uglify. Why not Google Closure Compiler.</p>

  <h2 id="polyfilling"><a href="#polyfilling"><span class=
  "header-section-number">4.20</span> polyfilling</a></h2>

  <p>The decline of bad browsers. Incompatability less of a concern
  than it was.</p>

  <p>Node doesn't require, built on v8.</p>

  <p><a href=
  "http://www.jimmycuadra.com/posts/ecmascript-5-array-methods">http://www.jimmycuadra.com/posts/ecmascript-5-array-methods</a>
  Unlike the new methods discussed in the first two parts, the
  methods here are all reproducible using JavaScript itself. Native
  implementations are simply faster and more convenient. Having a
  uniform API for these operations also promotes their usage,
  making code clearer when shared between developers.</p>

  <p>Even when only used once, preferable to polyfill as a generic
  solution rather than offer a one-time implementation because it
  better splits the intention of the logic being presented from the
  mechanisms that that logic sits on and, by providing abstraction,
  elucidates the code.</p>

  <h2 id="automated-testing"><a href=
  "#automated-testing"><span class=
  "header-section-number">4.21</span> automated testing</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Relationship between various files and test libraries other half of sketch from notebook">

    <p class="caption">Relationship between various files and test
    libraries <em>other half of sketch from notebook</em></p>
  </div>

  <p>How automated testing improves what can be written, not just
  making what is written more reliable.</p>

  <p>TDD drives development by influencing the design - good design
  is taken as that which is amenable to testing rather than which
  describes the problem domain accurately or solves a problem with
  minimum resources. Amenable to testing often means split into
  many co-operating parts so that each part may be tested via a
  simple test.</p>

  <p>Bt encourageing splitting into co-operating objects, TDD to a
  certain degree is anti-encapsulation. The public object that was
  extracted as a new concern from a larger object now needs public
  methods whereas before nothing was exposed.</p>

  <div class="figure">
    <img src="images/pyramid.png" alt=
    "The testing pyramid is a common concept, relying on the assumption that verification of small parts provides a solid base from which to compose system-level behaviours. A Lot of testing is done on the low-level components of the system, whereas for the high-level tests only smoke tests are provided. ">

    <p class="caption">The testing pyramid is a common concept,
    relying on the assumption that verification of small parts
    provides a solid base from which to compose system-level
    behaviours. A Lot of testing is done on the low-level
    components of the system, whereas for the high-level tests only
    smoke tests are provided.</p>
  </div>

  <p>Jstd can serve example files but need to write out slowly
  which it has no concept of. Customistation is via configuration
  rather than by plug-in, but even if it were, the threading model
  is not suitable to create this kind of timed output.</p>

  <p>Tests include an extremely large file twentyThousandRecords.js
  to test under stress</p>

  <p>Why jstd's built in proxy isn't sufficient. An example of a
  typical Java webserver, features thread-based mutlithreading in
  which threads wait for a while response to be received.</p>

  <p>Tests deal with the problem of "irreducible complexity" - when
  a program is made out of parts whose correct behaviour cannot be
  observed without all of the program. Allows smaller units to be
  verified before verifying the whole.</p>

  <p>Conversely, automated testing allows us to write
  incomprehensible code by making us into more powerful
  programmers, it is possible building up layers of complexity one
  very small part at a time that we couldn't write in a simple
  stage. Clarity &gt; cleverness but cleverness has its place as
  well (intriducing new concepts)</p>

  <p>Testing via node to give something to test against -
  slowserver. Proxy. JSTD not up to task. Shows how useful node is
  as a 'network glue'. The same as C was once described as a 'thin
  glue' [<a href=
  "http://www.catb.org/esr/writings/taoup/html/ch04s03.html">http://www.catb.org/esr/writings/taoup/html/ch04s03.html</a>].
  Transparent proxy is about 20 lines. Transparent enough to fool
  JSTD into thinking it is connecting directly to its server.</p>

  <p>Node comes with very little built in (not even http) but
  relies on libraries written in the language itself to do
  everything. Could implement own http on top of sockets if wanted
  rather than using the provided one.</p>

  <p>The test pyramid concept fits in well with the hiding that is
  provided. Under the testing pyramid only very high level
  behaviours are tested as ??? tests. While this is a lucky
  co-incidence, it is also an unavoidable restriction. Once
  compiled into a single source file, the individual components are
  hidden, callable only from withing their closure. Hence, it would
  not be possible to test the composed parts individually
  post-concatenation into a single javascript file, not even via a
  workarround for data hiding such as found in Java's reflection.
  Whereas in Java the protection is a means of protecting otherwise
  addressable resources, once a function is trapped inside a
  javascript closure without external exposure it is not just
  protected but, appearing in no namespaces, inherently
  unreferenceable.</p>

  <p>TDD fits well into an object pattern because the software is
  well composed into separate parts. The objects are almost
  tangible in their distinction as separate encapsulated entities.
  However, the multi-paradigm style of my implementation draws much
  fainter borders over the implementation's landscape.</p>

  <p>Approach has been to the test the intricate code, then for
  wiring don't have tests to check that things are plumbed together
  correctly, rather rely on this being obvious enough to be
  detected via a smoke test.</p>

  <p>A good test should be able to go unchanged as the source under
  test is refactored. Indeed, the test will be how we know that the
  code under test still works as intended. Experince tells me that
  testing that A listens to B (ie that the controller wires the
  jsonbuilder up to clarinet) produces the kind of test that
  'follows the code arround' meaning that because it is testing
  implementation details rather than behaviours, whenever the
  implementation is updated the tests have to be updated too.</p>

  <p>By testing individual tokens are correct and the use of those
  tokens as a wider expression, am testing the same thing twice.
  Arguably, redundant effort. But may simply be easier to write in
  that way - software is written by a human in a certain order and
  if we take a bottom-up approach to some of that design, each
  layer is easier to create if we first know the layers that it
  sits on are sound. Writing complex regular expressions is still
  programming and it is more difficult to test them completely when
  wrapped in rather a lot more logic than directly. For example, a
  regex which matches "{a,b}" or "{a}" but not "{a,}" is not
  trivial.</p>

  <p>Can test less exhaustively on higher levels if lower ones are
  well tested, testing where it is easier to do whilst giving good
  guarantees.</p>

  <p>Genuine data hiding gets in the way sometimes. Eg, token
  regexes are built from the combination of smaller regualar
  expressions for clarity (long regular expressions are concise but
  hard to read), and then wrapped in functions (why? - explain to
  generify interface) before being exposed. Because the components
  are hidden in a scope, they are not addressable by the tests and
  therefore cannot be directly tested. Reluctantly</p>

  <p>One dilemma in implementing the testing is how far to test the
  more generic sections of the codebase as generic components. A
  purist approach to TDD would say</p>

  <p>Could implement a resume function for if transmission stops
  halfway</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">   .<span class=
"fu">onError</span>( error ) {
      <span class="kw">this</span>.<span class=
"fu">resume</span>();
   }</code>
</pre>

  <h2 id="inversion-of-control"><a href=
  "#inversion-of-control"><span class=
  "header-section-number">4.22</span> Inversion of Control</a></h2>

  <p>Aim of creating a micro-library rules out building in a
  general-purpose IoC library.</p>

  <p>However, can still follow the general principles.</p>

  <p>Why the Observer pattern (cite: des patterns) lends itself
  well to MVC and inversion of control.</p>

  <p>What the central controller does; acts as a plumber connecting
  the various parts up. Since oboe is predominantly event/stream
  based, once wired up little intervention is needed from the
  controller. Ie, A knows how to listen for ??? events but is
  unintested who fired them.</p>

  <h2 id="stability-over-upgrades"><a href=
  "#stability-over-upgrades"><span class=
  "header-section-number">4.23</span> stability over
  upgrades</a></h2>

  <p>why jsonpath-like syntax allows upgrading message semantics
  without causing problems [SOA] how to guarantee non-breakages?
  could publish 'supported queries' that are guaranteed to work</p>

  <h2 id="support-for-older-browsers"><a href=
  "#support-for-older-browsers"><span class=
  "header-section-number">4.24</span> support for older
  browsers</a></h2>

  <p>Still works as well as non-progressive json Could be used for
  content that is inherently streaming (wouldn't make sense without
  streaming)</p>

  <h2 id="suitability-for-databases"><a href=
  "#suitability-for-databases"><span class=
  "header-section-number">4.25</span> suitability for
  databases</a></h2>

  <p>Databases offer data one row at a time, not as a big lump.</p>

  <h2 id="weaknesses"><a href="#weaknesses"><span class=
  "header-section-number">4.26</span> weaknesses</a></h2>

  <p>implementation keeps 'unreachable' listeners difficult
  decidability/proof type problem to get completely right but could
  cover most of the easy cases</p>

  <p>Parse time for large files spread out over a long time.
  Reaction to parsed content spread out over a long time, for
  example de-marshalling to domain objects. For UX may be
  preferable to have many small delays rather than one large
  one.</p>

  <p>Doesn't support all of jsonpath. Not a strict subset of the
  language.</p>

  <p>Rest client as a library is passing mutable objects to the
  caller. too inefficient to re-create a new map/array every time
  an item is not as efficient in immutability as list head-tail
  type storage</p>

  <p>An imutability wrapper might be possible with defineProperty.
  Can't casually overwrite via assignment but still possible to do
  defineProperty again.</p>

  <p>Would benefit from a stateless language where everything is
  stateless at all times to avoid having to program
  defensively.</p>

  <h1 id="conclusion"><a href="#conclusion"><span class=
  "header-section-number">5</span> Conclusion</a></h1><!---
**1 to 5 pages**
=-->

  <p>Doing things faster vs doing things earlier. "Hurry up and
  wait" approach to optimisation.</p>

  <h2 id="development-methodology"><a href=
  "#development-methodology"><span class=
  "header-section-number">5.1</span> Development
  methodology</a></h2>

  <p>Did it help?</p>

  <p>Switched several times. Could have started with winning side?
  Tension between choosing latest and greatest (promising much) or
  old established solution alraedy experienced with but with known
  problems. Judging if problems will become too much of a
  hinderence and underestimating the flaws. JSTD was yesterday's
  latest and greatest but Karma genuinely is great. In end, right
  solution was found despite not being found in most direct
  way.</p>

  <p>Packaging was a lot of work but has delivered the most concise
  possible library.</p>

  <h2 id="size"><a href="#size"><span class=
  "header-section-number">5.2</span> Size</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "A pie chart showing the sizes of the various parts of the codebase">

    <p class="caption">A pie chart showing the sizes of the various
    parts of the codebase</p>
  </div>

  <p>Comment on the size of the libraray</p>

  <h2 id="handling-invalid-input"><a href=
  "#handling-invalid-input"><span class=
  "header-section-number">5.3</span> Handling invalid
  input</a></h2>

  <p>Invalid jsonpaths made from otherwise valid clauses (for
  example two roots) perhaps could fail early, at compile time.
  Instead, get a jsonPath that couldn't match anything. Invalid
  syntax is picked up.</p>

  <p>Same pattern could be extended to XML. Or any tree-based
  format. Text is easier but no reason why not binary
  applications.</p>

  <p>Not particularly useful reading from local files.</p>

  <p>Does not save memory over DOM parsing since the same DOM tree
  is built. May slightly increase memory usage by utilising memory
  earlier that would otherwise be dept dormant until the whole
  transmission is received but worst case more often a concern than
  mean.</p>

  <p>Implementation in a purely functional language with lazy
  evaluation: could it mean that only the necessary parts are
  computed? Could I have implemented the same in javascript?</p>

  <p>Would be nice to: * discard patterns that can't match any
  further parts of the tree * discard branches of the tree that
  can't match any patterns * just over the parsing of branches of
  the tree that provably can't match any of the patterns</p>

  <h2 id="comparative-usages"><a href=
  "#comparative-usages"><span class=
  "header-section-number">5.4</span> Comparative usages</a></h2>

  <p>Interesting article from Clarinet: <a href=
  "http://writings.nunojob.com/2011/12/clarinet-sax-based-evented-streaming-json-parser-in-javascript-for-the-browser-and-nodejs.html">
  http://writings.nunojob.com/2011/12/clarinet-sax-based-evented-streaming-json-parser-in-javascript-for-the-browser-and-nodejs.html</a></p>

  <p>In terms of syntax: compare to SAX (clarinet) for getting the
  same job done. Draw examples from github project README. Or from
  reimplementing Clarinet's examples.</p>

  <p>Consider:</p>

  <ul>
    <li>Difficulty to program</li>

    <li>Ease of reading the program / clarity of code</li>

    <li>Resources consumed</li>

    <li>Performance (time) taken</li>

    <li>about the same. Can react equally quickly to io in
    progress, both largely io bound.</li>

    <li>Is earlier really faster?</li>
  </ul>

  <h2 id="community-reaction"><a href=
  "#community-reaction"><span class=
  "header-section-number">5.5</span> Community reaction</a></h2>

  <p>Built into Dojo Followers on Github Being posted in forums
  (hopefully also listed on blogs) No homepage as of yet other than
  the Github page</p>

  <h1 id="appendix"><a href="#appendix"><span class=
  "header-section-number">6</span> Appendix</a></h1>

  <h1 id="bibliography"><a href="#bibliography"><span class=
  "header-section-number">7</span> Bibliography</a></h1>

  <p>Ahuvia, Yogev. 2013. &acirc;&euro;&oelig;Design Patterns:
  Infinite Scrolling: Let&acirc;&euro;&trade;s Get To The Bottom Of
  This
  http://uxdesign.smashingmagazine.com/2013/05/03/infinite-scrolling-get-bottom/.&acirc;&euro;
  Smashing Magazine.</p>

  <p>Anon. 2011. &acirc;&euro;&oelig;3G mobile data network
  crowd-sourcing survey.&acirc;&euro; BBC News.</p>

  <p>Douglas, Crockford. 2009. &acirc;&euro;&oelig;JSON: The
  fat-free alternative to XML.&acirc;&euro; <a href=
  "http://json.org" title=
  "http://json.org">http://json.org</a>.</p>

  <p>Fielding, R. T. 2000. &acirc;&euro;&oelig;Principled design of
  the modern Web architecture.&acirc;&euro;</p>

  <p>Geelhoed, Erik, Peter Toft, Suzanne Roberts, and Patrick
  Hyland. 1995. &acirc;&euro;&oelig;To influence Time
  Perception.&acirc;&euro; Hewlett Packard Labs. <a href=
  "http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm"
  title=
  "http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm">http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm</a>.</p>

  <p>Graham, Paul. 2004. <em>The Other Road Ahead</em>.
  O&acirc;&euro;&trade;Reilly and Associates.</p>

  <p>Hopkins, Don. 1994. <em>The X-Windows Disaster</em>. Hungry
  Minds.</p>

  <p>Lea, Tom. 2012. &acirc;&euro;&oelig;Improving performance on
  twitter.com.&acirc;&euro; <a href=
  "[https://blog.twitter.com/2012/improving-performance-twittercom]"
  title=
  "[https://blog.twitter.com/2012/improving-performance-twittercom]">
  [https://blog.twitter.com/2012/improving-performance-twittercom]</a>.</p>

  <p>Mullany, Michael. 2013. &acirc;&euro;&oelig;5 Myths About
  Mobile Web Performance.&acirc;&euro; <a href=
  "http://www.sencha.com/blog/5-myths-about-mobile-web-performance"
  title=
  "http://www.sencha.com/blog/5-myths-about-mobile-web-performance">
  http://www.sencha.com/blog/5-myths-about-mobile-web-performance</a>.</p>

  <p>Ralston, Anthony. 2000. &acirc;&euro;&oelig;Encyclopedia of
  Computer Science.&acirc;&euro; Nature Pub. Group.</p>

  <p>Reis, Eric. 2011. <em>The Lean Startup: How
  Today&acirc;&euro;&trade;s Entrepreneurs Use Continuous
  Innovation to Create Radically Successful Businesses.</em> Crown
  Business Publishing.</p>

  <p>Sapir, E. 1958. &acirc;&euro;&oelig;Culture, Language and
  Personality (ed. D. G. Mandelbaum).&acirc;&euro; Berkeley, CA:
  University of California Press.</p>

  <p>Stefanov, Stoyan. 2009. &acirc;&euro;&oelig;Progressive
  rendering via multiple flushes.&acirc;&euro; <a href=
  "http://www.phpied.com/progressive-rendering-via-multiple-flushes/"
  title=
  "http://www.phpied.com/progressive-rendering-via-multiple-flushes/">
  http://www.phpied.com/progressive-rendering-via-multiple-flushes/</a>.</p>

  <p>Whorf, B. L. 1956. &acirc;&euro;&oelig;Language, Thought and
  Reality (ed. J. B. Carroll).&acirc;&euro; Cambridge, MA: MIT
  Press.</p>

  <p>van Kesteren, Anne. 2012. &acirc;&euro;&oelig;XMLHttpRequest
  Level 2 Working Draft.&acirc;&euro; <a href=
  "http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications"
  title=
  "http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications">
  http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications</a>.</p>

  <p>van Kesteren, Anne, and Dean Jackson. 2006.
  &acirc;&euro;&oelig;The XMLHttpRequest Object.&acirc;&euro;
  <a href="http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/"
  title=
  "http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/">http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/</a>.</p>

  <div class="footnotes">
    <hr>

    <ol>
      <li id="fn1">
        <p>See <a href=
        "http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html">
        http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html</a>.<a href="#fnref1">&acirc;&dagger;&copy;</a></p>
      </li>

      <li id="fn2">
        <p>Rather confusingly, X11 would call the <em>server</em>
        the <em>client</em> but I use terms here by their more
        cannonical meaning such that the client is the machine the
        user is actually interacting with.<a href=
        "#fnref2">&acirc;&dagger;&copy;</a></p>
      </li>

      <li id="fn3">
        <p>See <a href=
        "http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html">
        http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html</a>.<a href="#fnref3">&acirc;&dagger;&copy;</a></p>
      </li>
    </ol>
  </div>
</body>
</html>
