<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">

<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for Mac OS X (vers 31 October 2006 - Apple Inc. build 15.6), see www.w3.org">

  <title></title>
</head>

<body>
  <h1 id="abstract"><a href="#abstract"><span class=
  "header-section-number">1</span> Abstract</a></h1>

  <p>A Javascript REST client library targeting both Node.js and
  web browsers that incorporates http streaming, pattern matching,
  and progressive JSON parsing, with the aim of improving
  performance, fault tolerance, and encouraging a greater degree of
  loose coupling between programs. Loose coupling is particularly
  considered in light of the application of Agile methodologies to
  SOA, providing a framework in which it is acceptable to partially
  restructure the JSON format in which a resource is expressed
  whilst maintaining compatibility with dependent systems.</p>

  <p>A critique is made of current practice under which resources
  are entirely retrieved before items of interest are extracted
  programmatically. An alternative model is presented allowing the
  specification of items of interest using a declarative syntax
  similar to JSONPath. The identified items are then provided
  incrementally while the resource is still downloading.</p>

  <p>In addition to a consideration of performance in absolute
  terms, the usability implications of an incremental model are
  also evaluated with regards to differences in user perception of
  performance.</p>

  <h1 id="introduction"><a href="#introduction"><span class=
  "header-section-number">2</span> Introduction</a></h1>

  <p>This dissertation does not focus on implementing software for
  any particular problem domain. Rather, its purpose is to
  encourage the REST paradigm to be viewed through a novel lens. In
  application this may be used to deliver tangible benefits to many
  common REST use cases. Although I express my thesis through
  programming, the contribution I hope to deliver is felt more
  strongly as a shift in how we <em>think</em> about http than it
  is a change in the underlying technology.</p>

  <p>In the interest of developer ergonomics, REST clients have
  tended to style the calling of remote resources similar to the
  call style of the host programming language. Depending on the
  language, one of two schemas are followed: a synchronous style in
  which the http call is an expression which evaluates to the
  resource that was fetched; or asynchronous or monadic in which
  some logic is specified which may be applied to the response once
  it is complete. This tendency to cast REST calls using terms from
  the language feels quite natural; we may call a remote service
  without having to make any adjustment for the fact that it is
  remote. However, we should remember that this construct is not
  the only possible mapping. Importing some moderate Whorfianism
  <span class="citation">(Whorf 1956)</span><span class=
  "citation">(Sapir 1958)</span> from linguistics, we might venture
  to say that the programming languages we use encourage us to
  think in the terms that they easily support. Also UML! For any
  multi-packet message sent via a network some parts will arrive
  before others, at least approximately in-order, but whilst coding
  a C-inspired language whose return statements yield single,
  discrete values it comfortable to conceptualise the REST response
  as a discrete event. Perhaps better suited to representing a
  progressively returned value would have been the relatively
  unsupported Generator routine <span class="citation">(Ralston
  2000)</span>.</p>

  <p>In most practical cases where software is being used to
  perform a task there is no reasonable distinction between being
  earlier and being quicker. Therefore, if our interest is to
  create fast software we should be using data at the first
  possible opportunity. Examining data <em>while</em> it streams
  rather than hold unexamined until the message ends.</p>

  <p>The coining of the term REST represented a shift in how we
  think about http, away from the transfer of hypertext documents
  to that of arbitrary data <span class="citation">(Fielding 2000,
  407&acirc;&euro;&ldquo;416)</span>. It introduced no
  fundamentally new methods. Likewise, no genuinely new computer
  science techniques need be invented to realise my thesis. As a
  minimum, the implementation requires an http client which exposes
  the response whilst it is in progress and a parser which can
  start making sense of a response before it sees all of it. I also
  could not claim this thesis to be an entirely novel composition
  of such parts. Few ideas are genuinely new and it is often wiser
  to mine for solved problems then to solve again afresh. The
  intense competition of Web browsers to be as fast as possible has
  already found this solution. Load any graphics rich with images
  -- essentially an aggregation of hypertext and images -- the HTML
  is parsed incrementally while it is downloading and the images
  are requested as soon as individual &lt;img&gt; tags are
  encountered. The browser's implementation involves a highly
  optimised parser created for a single task, that of displaying
  web pages. The new contribution of this dissertation is to
  provide a generic analog applicable to any problem domain.</p>

  <p>Also progressive SVGs.<sup><a href="#fn1" class="footnoteRef"
  id="fnref1" name="fnref1">1</a></sup></p>

  <h2 id="rest-aggregation-could-be-faster"><a href=
  "#rest-aggregation-could-be-faster"><span class=
  "header-section-number">2.1</span> REST aggregation could be
  faster</a></h2>

  <div class="figure">
    <img src="images/rest_timeline_1.png" alt=
    "Aggregation of lower-level resources exposed via REST. The client fetches a listing of an author's publications and then the first three articles. The sequence represents the most commonly used technique in which the client does not react to the response until it is complete. In this example the second wave of requests cannot be made until the original response is complete, at which time they are issued in quick succession. ">

    <p class="caption"><strong>Aggregation of lower-level resources
    exposed via REST.</strong> The client fetches a listing of an
    author's publications and then the first three articles. The
    sequence represents the most commonly used technique in which
    the client does not react to the response until it is complete.
    In this example the second wave of requests cannot be made
    until the original response is complete, at which time they are
    issued in quick succession.</p>
  </div>

  <div class="figure">
    <img src="images/rest_timeline_2.png" alt=
    "Revised sequence of aggregation performed by a client capable of progressively interpreting the fetched resource. Because UML sequence diagrams arrows draw the concept of a returned value as a one-off event rather than a continuous process, I have introduced the notation of lighter arrows illustrating fragments of an ongoing response. Each individual publication request is made at the earliest possible time, as soon as the its URL can be extracted from the publications list. Once the required data has been read from the original resource it is aborted rather than continue to download unnecessary data. This results in a moderate reduction in wait time to see all three articles but a dramatic reduction in waiting before the first content is presented. Note also how the cadence of requests is more even with four connections opened at roughly equal intervals rather than a single request followed by a rapid burst of three. Clients frequently limit the number of simultaneous connections per domain so avoiding bursts of requests is further to our advantage. ">

    <p class="caption"><strong>Revised sequence of aggregation
    performed by a client capable of progressively interpreting the
    fetched resource.</strong> Because UML sequence diagrams arrows
    draw the concept of a returned value as a one-off event rather
    than a continuous process, I have introduced the notation of
    lighter arrows illustrating fragments of an ongoing response.
    Each individual publication request is made at the earliest
    possible time, as soon as the its URL can be extracted from the
    publications list. Once the required data has been read from
    the original resource it is aborted rather than continue to
    download unnecessary data. This results in a moderate reduction
    in wait time to see all three articles but a dramatic reduction
    in waiting before the first content is presented. Note also how
    the cadence of requests is more even with four connections
    opened at roughly equal intervals rather than a single request
    followed by a rapid burst of three. Clients frequently limit
    the number of simultaneous connections per domain so avoiding
    bursts of requests is further to our advantage.</p>
  </div><!--- 
connections per peer limited:
http://stackoverflow.com/questions/5751515/official-references-for-default-values-of-concurrent-http-1-1-connections-per-se 
=-->

  <p>Figures and illustrate how a progressive REST client may
  without adjustments to the server be used to aggregate REST
  resources faster. The greatest improvement is in how early the
  first piece of data is able to be used. This is advantageous:
  firstly, progressive display in itself raises the human
  perception of performance <span class="citation">(Geelhoed et al.
  1995)</span>; secondly, a user wanting to scan from top to bottom
  may start reading the first article while waiting for the later
  ones to arrive; thirdly, on seeing the first content the user may
  notice that they have requested the wrong aggregation, allowing
  them to backtrack earlier.</p>

  <p>Although the label "client software" in the figures above
  hints at software running directly on a user's own device this is
  not necessarily the case, for example the client may in fact be
  an server-side aggregation layer. Nodes in an n-tier architecture
  commonly defy categorisation as 'client' or 'server' in a way
  which is appropriate from all frames of reference. Rather, nodes
  may be thought of as a client from the layer below and as a
  server from the layer above. A further example would be a
  server-side webpage generator maintaining a perceptual
  performance improvement by progressively writing out html using
  http chunked encoding. <span class="citation">(Stefanov
  2009)</span>. The demonstrated advantages hold regardless of
  where in the stack the 'client' is located.</p>

  <h2 id="stepping-outside-the-big-small-tradeoff"><a href=
  "#stepping-outside-the-big-small-tradeoff"><span class=
  "header-section-number">2.2</span> Stepping outside the big-small
  tradeoff</a></h2>

  <p>Where a domain model contains a series of data, of which
  ranges are made available via REST, I have often seen a trade-off
  with regards to how much of the series each call should request.
  Answering this question is usually a compromise between competing
  concerns in which it is not simultaneously possible to addresses
  all concerns satisfactorily. A good example might be a Twitter's
  pages listing a series of tweets where the interface designers
  adopted a currently trending pattern <span class=
  "citation">(Ahuvia 2013)</span>, Infinite Scrolling. Starting
  from an initial page showing some finite number of tweets, upon
  scrolling to the bottom the next batch is automatically
  requested. The new batch is fetched in a json format and, once
  loaded, presented as html and added to the bottom of the page.
  Applied repeatedly this allows the user to scroll indefinitely,
  albeit punctuated by slightly jolting pauses while new content is
  loaded. To frame the big-small tradeoff we might consider the
  extreme choices. Firstly, requesting just one tweet per http
  request. By requesting the smallest possible content individual
  calls would complete very quickly and the pauses would be short.
  Taking the extreme small end the page stutters, pausing
  momentarily but frequently. Taking the opposite extreme, by
  requesting some huge number of tweets we see long periods of
  smooth scrolling partitioned by long waits.</p>

  <p>I propose that my thesis may be applied used to stand down
  from this compromise by delivering pauses which are both
  infrequent and short. In the Twitter example, once we have
  thinking about http progressively this may be achieved quite
  simply by issuing large requests but instead of deferring all
  rendering until the request completes, render individual tweets
  incrementally as they are progressively parsed out of the ongoing
  response.</p>

  <p>Integrate: twitter: page could update at bottom and top with
  same transport perhaps.</p>

  <h2 id="staying-fast-on-a-fallible-network"><a href=
  "#staying-fast-on-a-fallible-network"><span class=
  "header-section-number">2.3</span> Staying fast on a fallible
  network</a></h2>

  <p>The reliability of networks that REST operates over varies
  widely. Considering the worst case we see mobile networks in
  marginal signal over which it is common for ongoing downloads to
  be abruptly disconnected. Existing http clients handle this kind
  of unexpected termination poorly. Consider the everyday situation
  of somebody using a smartphone browser to check their email. The
  use of Webmail necessitates that the communication in made via
  REST rather than a mail specific protocol such as IMAP. Mobile
  data coverage is less than network operators claim <span class=
  "citation">(Anon. 2011)</span> so while travelling the signal can
  be expected to be lost and reestablished many times. Whilst not
  strictly forbidding their inspection, the web developer's
  standard AJAX toolkit are structured in such a way as to
  encourage the developer to consider partially successful messages
  as wholly unsuccessful. For example, the popular AJAX library
  jQuery automatically parses complete JSON or XML responses before
  handing back to the application. But on failure there is no
  attempt to parse or deliver the partial response. To programmers
  who know where to look the partial responses are retrievable as
  raw text but handling them is a special case,
  bringing-your-own-parser affair. Because of this difficulty I can
  only find examples of partial messages being dropped without
  inspection. In practice this means that for the user checking her
  email, even if 90% of her inbox had been retrieved she will be
  shown nothing. When the network is available again the
  application will have to download from scratch, including the 90%
  which it already fetched. I see much potential for improvement
  here.</p>

  <p>Not every message, incomplete, is useful. Whilst of course a
  generic REST client cannot understand the semantics of specific
  messages fully enough to decide if a partially downloaded message
  is useful, I propose it would be an improvement if the content
  from incomplete responses could be handled using much the same
  programming as for complete responses. This follows naturally
  from a conceptualisation of the http response as a progressive
  stream of many small parts; as each part arrives it should be
  possible to use it without knowing if the next will be delivered
  successfully. This style of programming encourages thinking in
  terms of optimistic locking. Upon each partial delivery there is
  an implicit assumption that it may be acted on straight away and
  the next will also be successful. In cases where this assumption
  fails the application should be notified so that some rollback
  may be performed.</p>

  <h2 id=
  "agile-methodologies-frequent-deployments-and-compatibility-today-with-versions-tomorrow">
  <a href=
  "#agile-methodologies-frequent-deployments-and-compatibility-today-with-versions-tomorrow">
  <span class="header-section-number">2.4</span> Agile
  methodologies, frequent deployments, and compatibility today with
  versions tomorrow</a></h2>

  <p>In most respects SOA architecture fits well with the fast
  release cycle that Agile methodologies encourage. Because in SOA
  we may consider that all data is local rather than global and
  that the components are loosely coupled and autonomous, frequent
  releases of any particular sub-system shouldn't pose a problem to
  the correct operation of the whole. Following emergent design it
  should be possible for the format of resources to be realised
  slowly and iteratively as a greater understanding of the problem
  is achieved. Unfortunately in practice the ability to change is
  hampered by tools which encourage programming against rigidly
  specified formats. Working in enterprise I have often seen the
  release of dozens of components cancelled because of a single
  unit that failed to meet acceptance criteria. By allowing a tight
  coupling that depends on exact versions of formats, the perfect
  environment is created for contagion whereby the updating of any
  single unit may only be done as part of the updating of the
  whole.</p>

  <p>An effective response to this problem would be to integrate
  into a REST client library the ability to use a response whilst
  being only loosely coupled to the <em>shape</em> of the overall
  message.</p>

  <h2 id="deliverables"><a href="#deliverables"><span class=
  "header-section-number">2.5</span> Deliverables</a></h2>

  <p>To avoid feature creep I am paring down the software
  deliverables to the smallest work which can we said to realise my
  thesis. Amongst commentators on start-up companies this is known
  as a <em>zoom-in pivot</em> and the work it produces should be
  the <em>Minimum Viable Product</em> or MVP <span class=
  "citation">(Reis 2011 p. ??)</span>, the guiding principle being
  that it is preferable to produce a little well than more badly.
  By focusing tightly I cannot not deliver a full stack so I am
  forced to implement only solutions which interoperate with
  existing deployments. This is advantageous; to somebody looking
  to improve their system small additions are easier to action than
  wholesale change.</p>

  <p>To reify the vision above, a streaming client is the MVP.
  Because all network transmissions may be viewed though a
  streaming lens an explicitly streaming server is not required.
  Additionally, whilst http servers capable of streaming are quite
  common even if they are not always programmed as such, I have
  been unable to find any example of a streaming-capable REST
  client.</p>

  <h2 id="criteria-for-success"><a href=
  "#criteria-for-success"><span class=
  "header-section-number">2.6</span> Criteria for success</a></h2>

  <p>In evaluating this project, we may say it has been a success
  if non-trivial improvements in speed can be made without a
  corresponding increase in the difficulty of programming the
  client. This improvement may be in terms of the absolute total
  time required to complete a representative task or in a user's
  perception of the speed in completing the task. Because
  applications in the target domain are much more io-bound than
  CPU-bound, optimisation in terms of the execution time of a
  algorithms will be de-emphasised unless especially egregious. The
  measuring of speed will include a consideration of performance
  degradation due to connections which are terminated early.</p>

  <p>Additionally, I shall be looking at common ways in which the
  semantics of a message are expanded as a system's design emerges
  and commenting on the value of loose coupling in avoiding
  disruption given unanticipated format changes.</p>

  <h1 id="background"><a href="#background"><span class=
  "header-section-number">3</span> Background</a></h1>

  <h2 id="the-web-as-an-application-platform"><a href=
  "#the-web-as-an-application-platform"><span class=
  "header-section-number">3.1</span> The web as an application
  platform</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "A webapp running with a front end generated partially on server and partially on client side. Ie, front-end client-side, front-end server-side, presentation layer a more meaningful distinction than">

    <p class="caption"><em>A webapp running with a front end
    generated partially on server and partially on client
    side.</em> Ie, front-end client-side, front-end server-side,
    presentation layer a more meaningful distinction than</p>
  </div>

  <p>Application design, particularly regarding the presentation
  layer, has charted an undulating path pulled by competing
  patterns of thick and thin clients. Having been taken up as the
  platform today for all but the most specialised applications, the
  web continues in this fashion by resisting easy categorisation as
  either mode. Although born on the network, at inception the web
  wasn't particularly graphical and didn't tread in the steps of
  networked graphical technologies such as X11 in which every
  presentation decision was made on a remote server<sup><a href=
  "#fn2" class="footnoteRef" id="fnref2" name="fnref2">2</a></sup>
  -- instead of sending fine-grained graphical instructions, a much
  more compact document mark-up format was used. At the same time,
  the markup-format was unlike like Gopher by being not totally
  semantic meaning that presentation layer concerns were kept
  partially resident on the server. At this time, whereas CGI was
  being used to serve documents with changeable content, it was not
  until 1996 with <em>ViaWeb</em> (later to become Yahoo Stores)
  that a user could be given pages comparable in function to the
  GUI interface of a desktop application. <span class=
  "citation">(Graham 2004 - get page number, in old dis)</span>.
  The interface of these early web applications comprised of pages
  dynamically generated on the server side, but handled statically
  on the client side so far as the browser was not able to be
  scripted to manipulate the page in any way.</p>

  <p>The modern, client-scripted web bears a striking resemblance
  to NeWS. Rather than send many individual drawings, the server
  could send parametrised instructions to show the client
  <em>how</em> some item of presentation is drawn. Having received
  the program, the only communications required are the parameters.
  This mixed-model provides no lesser degree of server-side control
  but by using client-side rendering a much faster experience was
  possible than would otherwise be possible over low-speed networks
  <span class="citation">(Hopkins 1994)</span>.</p>

  <p>Today it is agreed that program architecture should separate
  presentation from operational logic but there is no firm
  consensus on where each concern should be exercised. While it
  feels that Javascript is becoming requisite to even display a
  page, there are also actions in the opposite direction, for
  example in 2012 twitter moved much of their rendering back to the
  server-side reducing load times to one fifth of their previous
  design, commenting "The future is coming and it looks just like
  the past" <span class="citation">(Lea 2012)</span>. This model
  generated server-side short pages that load quick and are ready
  to be displayed but also sent the Javascript which would allow
  the display to be updated without another full server load. One
  weakness of this model is that the same presentational logic
  requires two expressions.</p>

  <p>Like most interactive programming, client-side scripts usually
  suffer greater delays waiting for io than because javascript
  execution times present a bottleneck. Because Javascript is used
  for user interfaces, frame-rates are important. Single threaded
  so js holds up rendering. Important to return control to the
  browser quickly. However, once execution of each js frame of
  execution is no more than the monitor refresh rate, further
  optimisation brings zero benefit. Hence, writing extremely
  optimised Javascript, especially focusing on micro-optimisations
  that hurt code readability is a bit silly.</p>

  <blockquote>
    <p>The user does something, then the app responds visually with
    immediacy at 30 frames per second or more, and completes a task
    in a few hundred milliseconds. As long as an app meets this
    user goal, it doesn&acirc;&euro;&trade;t matter how big an
    abstraction layer it has to go through to get to silicon.
    <span class="citation">(Mullany 2013)</span></p>
  </blockquote>

  <h2 id="node.js"><a href="#node.js"><span class=
  "header-section-number">3.2</span> Node.js</a></h2>

  <p>Include? Node not just for servers. CLI tools etc.</p>

  <p>Include? Compare to Erlang. Waiter model. Node restaurant much
  more efficient use of expensive resources.</p>

  <p>Include? No 'task' class or type, tasks are nothing more than
  functions, possibly having some values implicitly wrapped up in
  their closure.</p>

  <p>Include? Easy to distribute software (npm etc)</p>

  <p>It is difficult to say to what degree Node's use of Javascript
  is a distraction from the system's principled design aims and to
  what degree it defines the technology. Paradoxically, both may be
  so. Javascript has proven itself very effective as the language
  to meet Node's design goals but this suitability is not based on
  Javascript's association with web browsers, although it is
  certainly beneficial: for the first time it is possible to
  program presentation logic once which is capable of running on
  either client or server. Being already familiar with Javascript,
  web programmers were the first to take up Node.js first but the
  project mission statement makes no reference to the web; Node's
  architecture is well suited to any application domain where
  low-latency responses to i/o is more of a concern than
  heavyweight computation. Web applications fit well into this
  niche but they are far from the only domain that does so.</p>

  <p>In most imperative languages attempts at concurrency have
  focused on threaded execution, whereas Node is by design
  single-threaded. Threads are an effective means to speed up
  parallel computation but not well suited to concurrently running
  tasks which are mostly i/o dependent. Used for io, threads
  consume considerable resources while spending most of their lives
  waiting, occasionally punctuated with short bursts of activity.
  Programming Java safely with threads which share access to
  mutable objects requires great care and experience, otherwise the
  programmer is liable to create race conditions. If we consider
  for example a Java thread-based http aggregator; each 'requester'
  thread waits for seconds and then processes for milliseconds. The
  ratio of waiting to processing is so high that any gains achieved
  through actual concurrent execution of the active phase is
  pyrrhic. Following Node's lead, even traditionally thread-based
  environments such as Java are starting to embrace asynchronous,
  single-threaded servers with projects such as Netty.</p>

  <p>Node manages concurrency by managing an event loop of queued
  tasks and expects each task never to block. Non-blocking calls
  are used for all io and are callback based. Unlike Erlang, Node
  does not swap tasks out preemptively, it always waits for tasks
  to complete. This means that each task must complete quickly;
  while this might at first seem like an onerous requirement to put
  on the programmer, in practice the asynchronous nature of the
  toolkit makes following this requirement more natural than not.
  Indeed, other than accidental non-terminating loops or heavy
  number-crunching, the lack of any blocking io whatsoever makes it
  rather difficult to write a node program whose tasks do not exit
  quickly. This programming model of callback-based, asynchronous,
  non-blocking io with an event loop is already the model followed
  inside web browsers, which although multi-threaded in some
  regards, present a single-threaded virtual machine in terms of
  Javascript execution.</p>

  <p>A programmer working with Node's single-thread is able to
  switch contexts quickly to achieve a very efficient kind of
  concurrency because of Javascript's support for closures. Because
  of closures, under Node the responsibility to explicitly store
  state between making an asynchronous call and receiving the
  callback is removed from the programmer. Closures require no new
  syntax, the implicit storage of this data feels so natural and
  inevitable that looking at the typical program it is often not
  obvious that the responsibility exists at all.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
<span class=
"co">// Rather than blocking, this function relies on non-blocking io and</span>
<span class=
"co">// schedules three tasks, each of which exit quickly, allowing this</span>
<span class=
"co">// node instance to continue with other tasks in between. </span>
<span class=
"co">// However sophisticated and performant this style of programming, </span>
<span class=
"co">// to the developer it is barely more difficult than if a blocking io</span>
<span class="co">// model were followed. </span>

<span class="kw">function</span> <span class=
"fu">makeRequest</span>( host, path ) {

   <span class="kw">var</span> options = { <span class=
"dt">host</span>: host, <span class="dt">path</span>: path };
   
   <span class="ot">http</span>.<span class=
"fu">get</span>(options, <span class=
"kw">function</span>(response){
      
      <span class=
"co">// This function will be called when the response starts. The callback</span>
      <span class=
"co">// having started listening to the response object will quickly exit</span>
      <span class=
"co">// as Node tasks are want to do. Because of closures we are able to</span>
      <span class=
"co">// access the path variable declared in a containing scope, even after</span>
      <span class=
"co">// the containing scope has exited.      </span>
      <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">"The response has started for "</span> + path);
   
      <span class="ot">response</span>.<span class=
"fu">on</span>(<span class="st">'data'</span>, <span class=
"kw">function</span>(chunk) {
      
         <span class=
"co">// This function is called each time some data is received from the </span>
         <span class="co">// http request</span>
         
         <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">'Got some response '</span> + chunk);
       
      });
   }).<span class="fu">on</span>(<span class=
"st">"error"</span>, <span class="kw">function</span>(e){
   
      <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">"Got error: "</span> + <span class="ot">e</span>.<span class=
"fu">message</span>);
   });
   
   <span class="ot">console</span>.<span class=
"fu">log</span>(<span class="st">"Request has been made"</span>);
}</code>
</pre>

  <h2 id="streams-in-node"><a href="#streams-in-node"><span class=
  "header-section-number">3.3</span> Streams in Node</a></h2>

  <blockquote>
    <p>Streams in node are one of the rare occasions when doing
    something the fast way is actually easier. SO USE THEM. not
    since bash has streaming been introduced into a high level
    language as nicely as it is in node." <a href=
    "https://gist.github.com/2401787">high level node style
    guide</a></p>
  </blockquote>

  <p>Bash streams a powerful abstraction easily programmed for
  linear streaming. Node more powerful, allows a powerful streaming
  abstraction which is no more complex to program than a javascript
  webapp front end. Essentially a lower-level (and therefore more
  powerful) interface to streaming such as unix sockets or tcp
  connections.</p>

  <blockquote>
    <p>Node Stream API, which is the core I/O abstraction in
    Node.js (which is a tool for I/O) is essentially an abstract
    in/out interface that can handle any protocol/stream that also
    happens to be written in JavaScript. [<a href=
    "http://maxogden.com/a-proposal-for-streaming-xhr.html">http://maxogden.com/a-proposal-for-streaming-xhr.html</a>]</p>
  </blockquote>

  <p>Streams in node are a variant of the observer pattern and fit
  into a wider Node event model. Streams emit 'readable' events
  when they have some data to be read and 'end' events when they
  are finished. Apart from error handling, so far as reading is
  concerned, that is the extent of the API.</p>

  <h2 id="web-browsers-hosting-rest-clients"><a href=
  "#web-browsers-hosting-rest-clients"><span class=
  "header-section-number">3.4</span> Web browsers hosting REST
  clients</a></h2>

  <p>Http is essentially a thinly-wrapped text response around some
  usually text-based (but sometimes binary) data. It may give the
  length of the content as a header, but is not obliged to. It
  supports an explicitly chunked mode, but even the non-chunked
  mode may be considered as a stream. For example, a program
  generating web pages on the server side might choose to use
  chunking so that the browser is better able to choose when to
  re-render during the progressive display of a page <span class=
  "citation">(Stefanov 2009)</span> but this is optional and
  without these hints progressive rendering will still take
  place.</p>

  <p>The requesting of http from Javascript, commonly termed AJAX,
  was so significant a technique in establishing the modern web
  application architecture that it is often taken as being a
  synonym for Javascript-heavy web pages. Although an acronym for
  Asynchronous Javascript and XML, for data services designed with
  delivery to client-side web applications in mind JSON is almost
  exclusively preferred to XML and the term is used without regard
  for the data format of the response (the unpronounceable
  <em>AJAJ</em> never took off). During the 'browser war' years
  adding non-standard features was a common form of competition
  between authors; following this pattern Internet Explorer
  originally made AJAX possible by exposing Microsoft's Active X
  <em>Xml Http Request</em>, or XHR, object to Javascript
  programmers. This was widely copied as functional equivalents
  were added to all major browsers and the technique was eventually
  formalised by the W3C<span class="citation">(van Kesteren and
  Jackson 2006)</span>. What followed was a period of stagnation
  for web browsers. HTML4 reached W3C Recommendation status in 2001
  but having subsequently found several evolutionary dead ends such
  as XHTML, the developer community would see no major updates
  until HTML5 started to gather pace some ten years later. In this
  context the web continued to rapidly mature as an application
  platform and AJAX programming inevitably overtook the original
  XHR specification, browser vendors again adding their own
  proprietary extensions to compensate.</p>

  <p>Given this backdrop of non-standard extensions and lagging
  standardisation, abstraction layers predictably rose in
  popularity. Despite a reputation Javascript being poorly
  standardised, as a language it is very consistently implemented.
  More accurately we should say that the libraries provided by the
  environment lack compatibility. Given an abstraction layer to
  gloss over considerable differences cross-browser webapp
  developers found little difficulty in targeting multiple
  platforms. The various abstraction competed on developer
  ergonomics with the popular jQuery and Prototype.js promoting
  themselves respectively as <em>"do more, write less"</em> and
  <em>"elegant APIs around the clumsy interfaces of Ajax"</em>.
  JSON being a subset of Javascript, web developers barely noticed
  their privileged position whereby the serialisation of their data
  format mapped exactly onto the basic types of their programming
  language. As such there was never any confusion as to which exact
  object structure to de-serialise to. If this seems like a small
  advantage, contrast with the plethora of confusing and
  incompatible representations of JSON output presented by the
  various Java JSON parsers; JSON's Object better resembles Java's
  Map than Object and the confusion between JSON null, Java null,
  and Jackson's NullNode<sup><a href="#fn3" class="footnoteRef" id=
  "fnref3" name="fnref3">3</a></sup> is a common cause of errors.
  Endowed with certainty regarding deserialisation, JSON parsers
  could be safely integrated directly into AJAX libraries. This
  provided a call style while working with remote resources so
  streamlined as to require hardly any additional effort.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"ot">jQuery</span>.<span class="fu">ajax</span>(<span class=
"st">'http://example.com/people.json'</span>, <span class=
"kw">function</span>( people ) {

   <span class=
"co">// The parsing of the people json into a javascript object</span>
   <span class=
"co">// feels so natural that it is easy to forget while looking </span>
   <span class="co">// at the code that it happens at all. </span>
   
   <span class="fu">alert</span>(<span class=
"st">'the first person is called '</span> + people[<span class=
"dv">0</span>].<span class="fu">name</span>);
});</code>
</pre>

  <p>Whilst simple, the above call style is built on the assumption
  that a response is a one-time event and no accommodation is made
  for a continuously delivered response. Meanwhile, the XHR2
  standardisation process had started and was busy observing and
  specifying proprietary extensions to the original XHR1. Given an
  interest in streaming, the most interesting of these is the
  progress event:</p>

  <blockquote>
    <p>While the download is progressing, queue a task to fire a
    progress event named progress about every 50ms or for every
    byte received, whichever is least frequent. <span class=
    "citation">(van Kesteren 2012)</span></p>
  </blockquote>

  <p>Prior to this addition there had been no mechanism, at least
  so far as the published specs to an XHR instance in a streaming
  fashion. However, while all major browsers currently support
  progress events in their most recently versions, the installed
  userbase of supporting browsers is unlikely to grow fast enough
  that this technique may be relied upon without a fallback for
  several years.</p>

  <p>In fact, this is exactly how web browsers are implemented.
  However, this progressive use of http is hardwired into the
  browser engines rather than exposing an API suitable for general
  use and as such is treated as something of a special case
  specific to web browsers and has not so far seen a more general
  application. I wish to argue that a general application of this
  technique is viable and offers a worthwhile improvement over
  current common methods.</p>

  <p>While until recently browsers have provided no mechanism to
  stream into AJAX, almost every other instance of downloading has
  taken advantage of streaming and progressive interpretation. This
  includes image formats, as the progressive PNG and JPEG; markup
  as progressive display of html and svg; video; and Javascript
  itself -- script interpretation starts before the script is
  wholly fetched. Each of these progressive considerations is
  implemented as a specific-purpose mechanism internal to the
  browser which is not exported to Javascript and as such is not
  possible to repurpose.</p>

  <h2 id="browser-streaming-frameworks"><a href=
  "#browser-streaming-frameworks"><span class=
  "header-section-number">3.5</span> Browser streaming
  frameworks</a></h2>

  <p>As the web's remit spread to include more applications which
  would previously have been native apps, to be truly 'live' many
  applications found the need to be able to receive real-time push
  events. Dozens of streaming transports have been developed
  sidestepping the browser's apparent limitations.</p>

  <p>The earliest and most basic attempt was to poll by making many
  requests, I won't consider this approach other than to say it
  came with all the usually associated downsides. Despite the
  inadequacy of this approach, from here the improved technique of
  <em>long polling</em> was invented. A client makes a request to
  the server side. Once the connection is open the server waits,
  writing nothing until a push is required. To push the server
  writes the message and closes the http connection; since the http
  response is now complete the content may be handled by the
  Javascript client which then immediately makes a new request,
  reiterating the cycle of wait and response. This approach works
  well where messages are infrequently pushed but where the
  frequency is high the limitation of one http transmission per
  connections requires imposes a high overhead.</p>

  <p>Observing that while browsers lack progressive ajax,
  progressive html rendering is available, <em>push tables</em>
  achieve progressive data transfer by serialising streaming data
  to a HTML format. Most commonly messages are written to a table,
  one row per message. On the client side this table is hidden in
  an off-screen frame and the Javascript streaming client watches
  the table and reacts whenever a new row is found. In many ways an
  improvement over long-polling, this approach nevertheless suffers
  from an unnatural data format. Whilst html is a textual format so
  provides a degree of human-readability, html was not designed
  with the goal of an elegent or compact transfer of asynchronous
  data. Contrasted with a SOA ideal of <em>'plumbing on the
  outside'</em>, peeking inside the system is difficult whilst
  bloated and confusing formats are tasked with conveying
  meaning.</p>

  <p>Both long polling and push tables are better throught of as a
  means to circumvent restrictions than indigene technology. A
  purose-built stack, <em>Websockets</em> is poised to take over,
  building a standardised duplex transport and API on top of http's
  chunked mode. While the newest browsers support websockets, most
  of the wild use base does not. Nor do older browsers provide a
  fine-grained enough interface into http in order to allow a
  Javascript implementation. In practice, real-world streaming
  libraries such as socket.io [CITE] are capable of several
  streaming techniques and can select the best for a given context.
  To the programmer debugging an application the assortment of
  transports only enhances the black-box mentality with regards to
  the underlying transports.</p><!---
*some or all of the below could move to A&R, it is wondering into
analysis* =-->

  <p>Whilst there is some overlap, each of the approaches above
  addresses a problem only tangentially related to this project's
  aims. Firstly, requiring a server that can write to an esoteric
  format feels quite anti-REST, especially given that the server is
  sending in a format which requires a specific, known, specialised
  client rather than a generic tool. In REST I have always valued
  how prominently the plumbing of a system is visible, so that to
  sample a resource all that is required is to type a URL and be
  presented with it in a human-comprehensible format.</p>

  <p>Secondly, as adaptations to the context in which they were
  created, these frameworks realise a view of network usage in
  which downloading and streaming are dichotomously split, whereas
  I aim to realise a schema without dichotomy in which
  <em>streaming is adapted as the most effective means of
  downloading</em>. In existing common practice a wholly distinct
  mechanism is provided vs for data which is ongoing vs data which
  is finite. For example, the display of real-time stock data might
  start by AJAXing in historical and then separately use a
  websocket to maintain up-to-the-second updates. This requires the
  server to support two distinct modes. However, I see no reason
  why a single transport could not be used for both. Such a server
  might start answering a request by write historic events from a
  database, then switch to writing out live data in the same format
  in response to messages from a MOM. By closing the dichotomy we
  would have the advantage that a single implementation is able to
  handle all cases.</p>

  <p>It shouldn't be a surprise that a dichotomous implementation
  of streaming, where a streaming transport is used only for live
  events is incompatible with http caching. If an event is streamed
  when it is new, but then when it is old made available for
  download, http caching between the two requests is impossible.
  However, where a single mode is used for both live and historic
  events the transport is wholly compatible with http caching.</p>

  <p>If we take streaming as a technique to achieve efficient
  downloading, not only for the transfer of forever-ongoing data,
  none of these approaches are particularly satisfactory.</p>

  <h2 id="json-and-xml"><a href="#json-and-xml"><span class=
  "header-section-number">3.6</span> Json and XML</a></h2>

  <p>Although AJAX started as a means to transfer XML, today JSON
  "The fat-free alternative to XML<span class="citation">(Douglas
  2009)</span>" is the more popular serialisation format. The goals
  of XML were to simplify SGML to the point that a graduate student
  would be able to implement a parser in a week [@javaxml p ???].
  For the student tackling JSON a few hours with a parser generator
  should surfice, being expressable in 15 CFGs. Indeed, because
  JSON is a strict subset of Javascript, in many cases the
  Javascript programmer requires no parser at all. Unimpeeded by
  SGML's roots as a document format, JSON provides a much more
  direct analogue to the metamodel of a canonical modern
  programming language with entities such as <em>string</em>,
  <em>number</em>, <em>object</em> and <em>array</em>. By closely
  mirroring a programmer's metamodel, visualising a mapping between
  a domain model and it's serialised objects becomes trivial.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{
   <span class="dt">people</span>: [
      {<span class="dt">name</span>: <span class=
"st">'John'</span>, <span class="dt">town</span>:<span class=
"st">'Oxford'</span>},
      {<span class="dt">name</span>: <span class=
"st">'Jack'</span>, <span class="dt">town</span>:<span class=
"st">'Bristol'</span>}
   ]
}</code>
</pre>

  <p>This close resemblance to the model of the programming in some
  cases causes fast-changing formats.</p>

  <p>Like XML attributes, as a serialised text format, JSON objects
  have an order but are almost always parsed to and from orderless
  maps meaning that the order of the keys/value pairings as seen in
  the stream usually follows no defined order. No rule in the
  format would forbid representing of an ordered map in an ordered
  way but most tools on receiving such a message would ignore the
  ordering.</p>

  <p>(MINE SOA assignment). Also the diagram.</p>

  <h2 id="parsing-sax-and-dom"><a href=
  "#parsing-sax-and-dom"><span class=
  "header-section-number">3.7</span> Parsing: SAX and Dom</a></h2>

  <p>In the XML world two standard parser models exist, SAX and
  DOM, with DOM far the more popular. DOM performs a parse as a
  single evaluation, on the request of the programmer, returning an
  object model representing the whole of the document. At this
  level of abstraction the details of the markup are only distant
  concern. Conversely, SAX parsers are probably better considered
  as tokenisers, providing a very low-level event driven interface
  in line with the Observer pattern to notify the programmer of
  syntax as it is seen. Each element's opening and closing tag is
  noted</p>

  <p>This presents poor developer ergonomics by requiring that the
  programmer implement the recording of state with regard to the
  nodes that they have seen. For programmers using SAX, a
  conversion to their domain objects is usually implemented
  imperatively. This programming tends to be difficult to read and
  programmed once per usage rather than assembled as the
  combination of reusable parts. For this reason SAX is usually
  reserved for fringe cases where messages are very large or memory
  unusually scarce.</p>

  <p>DOM isn't just a parser, it is also a cross-language defined
  interface for manipulating the XML in real time, for example to
  change the contents of a web page in order to provide some
  interactivity. In JSON world, DOM-style parser not referring to
  the DOM spec, or what browser makers would mean. Rather,
  borrowing from the XML world to mean a parser which requires the
  whole file to be loaded.</p>

  <p>Suppose we want to extract the name of the first person. Given
  a DOM parser this is very easy:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">function</span> <span class=
"fu">nameOfFirstPerson</span>( myJsonString ) {

   <span class=
"co">// Extracting an interesting part from JSON-serialised data is</span>
   <span class=
"co">// relatively easy given a DOM-style parser. Unfortunately this</span>
   <span class=
"co">// forbids any kind of progressive consideration of the data.</span>
   <span class=
"co">// All recent browsers provide a JSON parser as standard. </span>

   <span class="kw">var</span> document = <span class=
"ot">JSON</span>.<span class="fu">parse</span>( myJsonString );
   <span class="kw">return</span> <span class=
"ot">document</span>.<span class="fu">people</span>[<span class=
"dv">0</span>].<span class="fu">name</span>; <span class=
"co">// that was easy!</span>
}</code>
</pre>

  <p>Contrast with the programming below which uses the clarinet
  JSON SAX parser. To prove that I'm not exaggerating the case, see
  published usages at [Clarinet demos].</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">function</span> <span class=
"fu">nameOfFirstPerson</span>( myJsonString, callbackFunction ){

   <span class=
"co">// The equivalent logic, expressed in the most natural way</span>
   <span class=
"co">// fora s JSON SAX parser is longer and much more </span>
   <span class=
"co">// difficult to read. The developer pays a high price for </span>
   <span class="co">// progressive parsing. </span>

   <span class="kw">var</span> clarinet = <span class=
"ot">clarinet</span>.<span class="fu">parser</span>(),
   
       <span class=
"co">// with a SAX parser it is the developer's responsibility </span>
       <span class=
"co">// to track where in the document the cursor currently is,</span>
       <span class=
"co">// requiring several variables to maintain.        </span>
       inPeopleArray = <span class="kw">false</span>,   
       inPersonObject = <span class="kw">false</span>,
       inNameAttribute = <span class="kw">false</span>,
       found = <span class="kw">false</span>;
   
   <span class="ot">clarinet</span>.<span class=
"fu">onopenarray</span> = <span class="kw">function</span>(){
      <span class=
"co">// for brevity we'll cheat by assuming there is only one</span>
      <span class=
"co">// array in the document. In practice this would be overly</span>
      <span class="co">// brittle.</span>
      
      inPeopleArray = <span class="kw">true</span>; 
   };
   
   <span class="ot">clarinet</span>.<span class=
"fu">onclosearray</span> = <span class="kw">function</span>(){
      inPeopleArray = <span class="kw">false</span>;
   };   
   
   <span class="ot">clarinet</span>.<span class=
"fu">onopenobject</span> = <span class="kw">function</span>(){
      inPersonObject = inPeopleArray; 
   };
   
   <span class="ot">clarinet</span>.<span class=
"fu">oncloseobject</span> = <span class="kw">function</span>(){
      inPersonObject = <span class="kw">false</span>;
   };   
      
   <span class="ot">clarinet</span>.<span class=
"fu">onkey</span> = <span class="kw">function</span>(key){
      inNameAttribute = ( inPeopleObject &amp;&amp; key == <span class="st">'name'</span>);
   };

   <span class="ot">clarinet</span>.<span class=
"fu">onvalue</span> = <span class="kw">function</span>(value){
      <span class=
"kw">if</span>( !found &amp;&amp; inNameAttribute ) {
         <span class="co">// finally!</span>
         <span class="fu">callbackFunction</span>( value );
         found = <span class="kw">true</span>;
      }
   };      
   
   <span class="ot">clarinet</span>.<span class=
"fu">write</span>(myJsonString);   
}</code>
</pre>

  <p>As we can see above, SAX's low-level semantics require a
  lengthy expression and for the programmer to maintain state
  regarding the position in the document -- usually recording the
  ancestors seen on the descent from the root to the current node
  -- in order to identify the interesting parts. This order of the
  code is also quite unintuitive; generally event handlers will
  cover multiple unrelated concerns and each concern will span
  multiple event handlers. This lends to programming in which
  separate concerns are not separately expressed in the code.</p>

  <h2 id="common-patterns-when-connecting-to-rest-services">
  <a href="#common-patterns-when-connecting-to-rest-services"><span class="header-section-number">
  3.8</span> Common patterns when connecting to REST
  services</a></h2>

  <p>Marshaling provides two-way mapping between a domain model and
  a serialisation as JSON or XML, either completely automatically
  or based on a declarative specification. To handle a fetched rest
  response it is common to automatically demarshal it so that the
  application may make use of the response from inside its own
  model, no differently from objects assembled in any other way.
  From the perspective of the programmer it is as if the domain
  objects themselves had been fetched. Another common design
  pattern, intended to give a degree of isolation between concerns,
  is to demarshal automatically only so far as Data Transfer
  Objects (DTOs), instances of classes which implement no logic
  other than storage, and from there programmatically instantiate
  the domain model objects. Going one step further, for JSON
  resources sent to loosely-typed languages with a native
  representation of objects as generic key-value pairs such as
  Javascript or Clojure, the marshaling step is often skipped: the
  output from the parser so closely resembles the language's
  built-in types that it is simplest to use it directly. Depending
  on the programming style adopted we might say that the JSON
  parser's output <em>is</em> the DTO and create domain model
  objects based on it, or that no further instantiation is
  necessary.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Degrees of automatic marshaling. From marshaling directly to domain objects, DTOs, using parser output as a DTO, or using objects directly. Distinguish work done by library vs application programmer's domain">

    <p class="caption"><em>Degrees of automatic marshaling</em>.
    From marshaling directly to domain objects, DTOs, using parser
    output as a DTO, or using objects directly. Distinguish work
    done by library vs application programmer's domain</p>
  </div>

  <p>Ultimately the degree of marshaling that is used changes only
  the level of abstraction of the resource that the REST client
  library hands over to the application developer. Regardless of
  the exact form of the response model, the developer will usually
  programmatically extract one or more parts from it via calls in
  the programming language itself. For example, on receiving a
  resource de-marshaled to domain objects, a Java developer will
  inspect it by calling a series of getters in order to narrow down
  to the interesting parts. This is not to say that the whole of
  the message might not in some way be interesting, only that by
  using it certain parts will need to be identified as distinct
  areas of concern.</p>
  <pre class="sourceCode java">
<code class="sourceCode java"><span class=
"co">// An example programmatic approach to a domain model interrogation </span>
<span class=
"co">// under Java; upon receiving a list of people, each person's name</span>
<span class=
"co">// is added to a database. The methods used to drill down to the</span>
<span class=
"co">// pertinent components of the response are all getters: getPeople, </span>
<span class="co">// getGivenName, and getSurname. </span>
<span class="dt">void</span> <span class=
"fu">handleResponse</span>( RestResponse response ) {

   <span class="kw">for</span>( Person p : response.<span class=
"fu">getPeople</span>() ) {
      <span class="fu">addNameToDb</span>( p.<span class=
"fu">getGivenName</span>(), p.<span class=
"fu">getSurname</span>() );
   }   
}</code>
</pre>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"co">// Although in this Javascript example the objects passed to the handler </span>
<span class=
"co">// remain in the form given by the JSON parser, containing no domain-specific</span>
<span class=
"co">// getters, the programming represents a different expression of the same </span>
<span class="co">// basic process.</span>
<span class="kw">function</span> <span class=
"fu">handleResponse</span>( response ){

   <span class="ot">response</span>.<span class=
"ot">people</span>.<span class="fu">forEach</span>( <span class=
"kw">function</span>( person ){
      <span class="fu">addNameToDb</span>( <span class=
"ot">p</span>.<span class="fu">givenName</span>, <span class=
"ot">p</span>.<span class="fu">surname</span> );
   });
}</code>
</pre>

  <p>Because it is applied directly to the metamodel of the
  language[^ It could be argued that getters aren't a part of the
  metamodel of Java itself, but they form such a common pattern
  that it is a part ], this extraction has become such a natural
  component of a workflow that it maye be used while thinking of it
  as wholly unremarkable. In the examples above we are interacting
  with the model in the way that the language makes the most easy
  to conceptualise. However se should consider that, however subtly
  embedded, the technique is an invented construct and only one of
  the possible formulations which might have been drawn.</p>

  <p>One weakness of this inspection model is that, once much code
  is written to interrogate models in this way, the interface of
  the model becomes increasingly expensive to change as the code
  making the inspections becomes more tightly coupled with the
  thing that it is inspecting. Taking the above example, if the
  model were later refactored such that the concepts of firstName
  and surName were pulled from the Person class into an extracted
  Name class, because the inspection relies on a sequence of calls
  made directly into domain objects, the code making the query
  would also have to change. Whilst following the object oriented
  principle of encapsulation of data, such that the caller does not
  have to concern themselves with the data structures hidden behind
  the getter, there is no such abstraction for when the structure
  itself changes. Given an Agile environment where the shape of
  data is refactored regularly, this would be a problem when
  programming against any kind of resource; for example, if change
  of objects formats propagates knock-on changes where ever the
  object is used it is very difficult to commit small diffs to the
  VCS which make incremental changes to a tightly focused area of
  the system. A method of programming which truly embraced extreme
  programming would allow constant change without disparate, barely
  related parts having to be modified in parallel when structural
  refactoring occurs. The coupling is all the more acute where the
  format of the item being inspected is defined by an independently
  maintained service.</p>

  <p><em>contagion problem</em></p>

  <p>Extraneous changes dilute the changelog, making it less easily
  defined by code changes which are intrinsically linked to the
  actual change in the logic being expressed by the program, and
  therefore to the thinking behind the change and the reason for
  the change.</p>

  <h2 id="jsonpath-and-xpath"><a href=
  "#jsonpath-and-xpath"><span class=
  "header-section-number">3.9</span> JsonPath and XPath</a></h2>

  <p>Both the above difficulty in identifying the interesting parts
  of a message whilst using a streaming parser and the problem with
  tight coupling of programmatic drilling down to REST formats
  leads me to search for areas where this problem has already been
  solved.</p>

  <p>In the domain of markup languages there are associated query
  languages such as XPATH whose coupling is loose enough that their
  expressions may continue to function after the exact shape of a
  message is refactored. While observing this is nothing more
  radical than using the query languages in more-or-less they were
  intended, their employment is not the most natural coming from a
  programming context in which the application developer's
  responsibilities usually start where the demarshaler's end.
  Consider the following XML:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;givenName&gt;</span>...<span class=
"kw">&lt;/givenName&gt;</span>   
      <span class="kw">&lt;familyName&gt;</span>Bond<span class=
"kw">&lt;/familyName&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>The XPath //person[0]//surname//text() would continue to
  identify the correct part of the resource without being updated
  after the xml analogue of the above Java Name refactor:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class="kw">&lt;givenName&gt;</span>...<span class=
"kw">&lt;/givenName&gt;</span>
         <span class="kw">&lt;familyName&gt;</span>Bond<span class=
"kw">&lt;/familyName&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>Luckily in JSON there exists already an attempt at an
  equivalent named Jsonpath. JsonPath closely resembles the
  javascript code which would select the same nodes. Not a real
  spec.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
<span class=
"co">// an in-memory person with a multi-line address:</span>
<span class="kw">let</span> person = {
   <span class="dt">name</span>: {<span class=
"dt">givenName</span>:<span class="st">''</span>, <span class=
"dt">familyName</span>:<span class="st">''</span>},
   <span class="dt">address</span>: [
      <span class="st">"line1"</span>,
      <span class="st">"line2"</span>,
      <span class="st">"line3"</span>
   ]
}


<span class=
"co">// in javascript we can get line two of the address as such:</span>
<span class="kw">let</span> address = <span class=
"ot">person</span>.<span class="fu">address</span>[<span class=
"dv">2</span>]

<span class=
"co">// the equivalent jsonpath expression is identical:</span>
<span class="kw">let</span> jsonPath = <span class=
"st">"person.address[2]"</span>

<span class=
"co">// although jsonpath also allows ancestor relationships which are not</span>
<span class=
"co">// expressible quite so neatly as basic Javascript:</span>
<span class="kw">let</span> jsonPath2 = <span class=
"st">"person..given"</span></code>
</pre>

  <p>Xpath is able to express identifiers which often survive
  refactoring because XML represents a tree, hence we can consider
  relationships between entities to be that of contains/contained
  in (also siblings?). In application of XML, in the languages that
  we build on top of XML, it is very natural to consider all
  elements to belong to their ancestors. Examples are myriad, for
  example consider a word count in a book written in DOCBook format
  - it should be calculable without knowing if the book is split
  into chapters or not since this is a concept internal to the
  organisation of the book itself nd not something that a querier
  is likely to find interesting - if this must be considered the
  structure acts as barrier to information rather than enabling the
  information's delivery. Therefore, in many cases the exact
  location of a piece of information is not as important as a more
  general location of x being in some way under y.</p>

  <p>This may not always hold. A slightly contrived example might
  be if we were representing a model of partial knowledge:</p>
  <pre class="sourceCode xml">
<code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class=
"kw">&lt;isNot&gt;&lt;surname&gt;</span>Bond<span class=
"kw">&lt;/surname&gt;&lt;/isNot&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code>
</pre>

  <p>The typical use pattern of XPath or JSONPath is to search for
  nodes once the whole serialisation has been parsed into a
  DOM-style model. JSONPath implementation only allows for
  search-type usage: <a href=
  "https://code.google.com/p/jsonpath/To">https://code.google.com/p/jsonpath/To</a>
  examine a whole document for the list of nodes that match a
  jsonpath expression the whole of the tree is required. But to
  evaluate if a single node matches an expression, only the
  <em>path of the descent from the root to that node</em> is
  required -- the same state as a programmer usually maintains
  whilst employing a SAX parser. This is possible because JSONPath
  does not have a way to express the relationship with sibling
  nodes, only ancestors and decedents.</p>

  <p>One limitation of the JSONPath language is that it is not
  possible to construct an 'containing' expression. CSS4 allows
  this in a way that is likely to become familiar to web developers
  over the next five years or so.</p>

  <h2 id="testing"><a href="#testing"><span class=
  "header-section-number">3.10</span> Testing</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Relationship between the main players in the JS testing landscape. JSTD, Karma, Jasmine, NodeUnit, jasmine-node, Browsers">

    <p class="caption">Relationship between the main players in the
    JS testing landscape. JSTD, Karma, Jasmine, NodeUnit,
    jasmine-node, Browsers</p>
  </div>

  <p>By the commonjs spec, test directory should be called 'test'
  (<a href=
  "http://wiki.commonjs.org/wiki/Packages/1.0">http://wiki.commonjs.org/wiki/Packages/1.0</a>#Package_Directory_Layout)
  doesn't matter for my project since not using commonjs, but might
  as well stick to the convention.</p>

  <p>How TDD helps How can fit into methodology</p>

  <ul>
    <li>JSTD</li>

    <li>NodeUnit</li>

    <li>Karma</li>

    <li>Jasmine</li>
  </ul>

  <p>Initially started with jstestdriver but found it difficult.
  Karma started because engineers working on the Angular project in
  Google were "struggling a lot with jstd": <a href=
  "http://www.youtube.com/watch?v=MVw8N3hTfCI">http://www.youtube.com/watch?v=MVw8N3hTfCI</a>
  - jstd is a google project Even Jstd's authors seems to be
  disowning it slightly. Describe what was once its main mode of
  operation as now being for stress testing of jstd itself only.
  Problems: browsers become unresponsive. Generally unreliable, has
  to be restarted frequently.</p>

  <p>JSTD, as a Java program, is difficult to start via Grunt. Also
  an issue that Grunt post-dates Karma by enough that JSTD doesn't
  have the attention of the Grunt community.</p><!---
**40 to 60 pages**

@333w, 13,000 to 19,000
=-->

  <h1 id="application-and-reflection"><a href=
  "#application-and-reflection"><span class=
  "header-section-number">4</span> Application and
  Reflection:</a></h1>

  <p>Using a combination of the techniques investigated in the
  previous chapter, I propose that a simple design is possible
  which makes REST clients more efficient whilst being no more
  difficult to program. Although simple, this model fits poorly
  with established vocabulary, requiring a transport that sits
  <em>somewhere between 'stream' and 'download'</em> and a parsing
  strategy which <em>takes elements from SAX and DOM</em> but
  follows neither model*.</p>

  <p>Implementation in Javascript gives me the widest deployment
  options, covering client-side browser programming, server
  programming, use in command line tools, or any other usage. This
  context dictates a design which is non-blocking, asynchronous and
  callback based. While influenced by the language, the model of
  REST client proposed here is not limited to Javascript or web
  usage and I intent to comment briefly also on the applicability
  to other platforms. Likewise, I have also chosen to focus on JSON
  although I will also be commenting on the parallel applicability
  of these ideas to XML.</p>

  <p>From DOM we may observe that as a programmer, using a resource
  is simpler when a parsed entity is passed whole to a single
  callback, rather than the SAX model which requires the programmer
  to infer the entity from a lengthy series of callbacks. From
  observing SAX parsers or progressive HTML rendering, we can say
  that http is more efficient if we no not wait until we have
  everything before we start using the parts that we do have. DOM
  parsers pass a fully parsed node to registered callbacks, whole
  and ready to use, invariably at the root of the parsed document.
  From the vantage of the library's user, my thesis duplicates this
  convenience but removes one restriction; that the node which is
  passed must be the root. Because the mark-up formats we are
  dealing with are hierarchical and serialised depth-first it is
  possible to fully parse any sub-tree without fully knowing the
  parent node. From these observations we may program a new kind of
  REST client which is as performant as SAX but as easy to program
  as DOM.</p>

  <p>To follow this progressive-but-complete model, identifying the
  interesting parts of a document involves turning the traditional
  model for drilling down inside-out. Traditionally the
  programmer's callback receives the document then inside that
  callback drills down to locate the parts that they are interested
  in. Instead I propose taking the drilling-down logic out from
  inside the callback and instead wrap the callback in it. This
  means that the callback receives selected parts of the response
  which the library has already drilled down to on behalf of the
  programmer.</p>

  <p>Whilst JSONPath's existing implementation is only implemented
  for searching over already gathered objects, this kind of
  searching is just one application for the query language. I find
  that this is a very suitable declarative language to use to
  specify the parts of a response that a developer would like to
  drill-down to given the context of a document whose parse is in
  progress. JSONPath is especially applicable because it specifies
  only 'contained-in/contains' type relationships. On encountering
  any node in a serialised JSON stream, because of the depth-first
  serialisation order I will always have previously seen its
  ancestors. Hence, having written a suitably flexible JSONPath
  expression compiler such that it does not require a complete
  document, I will have enough information to evaluate any
  expression against any node at the time when it is first
  identified in the document. Because XML is also written
  depth-first, the same logic would apply to an XPath/XML variant
  of this project.</p>

  <p>The definition of 'interesting' will be generic and
  accommodating enough so as to apply to any data domain and allow
  any granularity of interest, from large object to individual
  datums. With just a few lines of programming</p>

  <h2 id="jsonpath-expressions"><a href=
  "#jsonpath-expressions"><span class=
  "header-section-number">4.1</span> JSONPath expressions</a></h2>

  <p>In searching through a model stocked with data is common to
  ask for database-style queries such as 'books costing more than
  X'. However, not searching, selecting from a resource that the
  programmer has requested, probably assembled on their behalf by
  their parameters where we can expect the developer to be
  interested in most of the content. Modify JSONPath for this
  actual situation. Avoid implementing, at least at first, the
  language features which are less likely to be used and are easily
  tested for inside the callback. At same time, add features which
  are more likely to be useful for this context.</p>

  <p>We impose types on top of JSON/XML markup. Only few basic
  types defined in the markup languages themselves. Essence of
  marshaling.</p>

  <p>Xml comes with a strong concept of the <em>type</em> of an
  element, the tag name is taken as a more immediate fundamental
  property of the thing than the attributes. For example, in
  automatic json-Java object demarshallers, the tag name is always
  mapped to the Java class. In JSON, other than the base types
  common to most languages (array, object, string etc) there is no
  further concept of type. If we wish to build a further
  understanding of the type of the objects then the realtionship
  with the parent object, expressed by the attribute name, is more
  likely to indicate the type. A second approach is to use duck
  typing in which the relationship of the object to its ancestors
  is not examined but the properties of the object are used instead
  to communicate an enhanced concept of type. For example, we might
  say that any object with an isbn and a title is a book.</p>

  <p>Whereas XML has a pretty good concept of the type of an
  element (beyond simply being an element node) in the tagName,
  JSON's objects are all simply objects. In JSON the type of a node
  is usually inferred in one of two ways: either, the fieldName in
  the parent object which references a node; or, from the fields
  that the object has.</p>

  <p>In the absence of node typing beyond the categorisation as
  objects, arrays and various primitive types, the key immediately
  mapping to the object is often taken as a lose concept of the
  type of the object. Quite fortunately, rather than because of a
  well considered object design, this tends to play well with
  automatically marshaling of domain objects expressed in a
  Java-style OO language because there is a strong tendency for
  field names -- and by extension, 'get' methods -- to be named
  after the <em>type</em> of the field, the name of the type also
  serving as a rough summary of the relationship between two
  objects. See figure below.</p>

  <p>In the below example, we assign the node the type 'address'
  because of the parent node's field name. Other than this, these
  are standard arrays of strings:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{
   <span class="dt">name</span>: <span class="st">'...'</span>
,  <span class="dt">residence</span>: {
      <span class="dt">address</span>: [
         <span class="st">'...'</span>, <span class=
"st">'...'</span>, <span class="st">'...'</span>
      ]
   }
,  <span class="dt">employer</span>: {
      <span class="dt">name</span>: <span class="st">'...'</span>
   ,  <span class="dt">address </span>:[
         <span class="st">'...'</span>, <span class=
"st">'...'</span>, <span class="st">'...'</span>      
      ]
   }   
}</code>
</pre>

  <p>By sensible convention, even in a serialisation format which
  allows lists of disparate types, lists contain only items of
  equivalent type. This gives way to a sister convention seen in
  the below example, when serialising with multiple addresses in an
  array, it is the grandparent's node field name which indicates
  the type, the parent being the array containing the multiple
  addressees:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{
   <span class="dt">residences</span>: {
      <span class="dt">addresses</span>: [
         [<span class="st">'Townhouse'</span>, <span class=
"st">'Street'</span>, <span class="st">'Some town'</span>]      
      ,  [<span class="st">'Beach Hut'</span>, <span class=
"st">'Secret Island'</span>, <span class="st">'Bahamas'</span>]
      ]
   }
}</code>
</pre>

  <p>In this third example, the field names linking to addresses
  refer to the relationship between the parent and child nodes
  rather than the type of the child. The address type is more
  easily recognised by its list of fields rather than its position
  in the document:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{
   <span class="dt">name</span>: <span class="st">'...'</span>
,  <span class="dt">residence</span>: {
      <span class="dt">number</span>:<span class=
"st">'...'</span>, <span class="dt">street</span>:<span class=
"st">'...'</span>, <span class="dt">town</span>:<span class=
"st">'...'</span> 
   }
,  <span class="dt">employer</span>:{
      <span class="dt">name</span>: <span class="st">'...'</span>
   ,  <span class="dt">premises</span>:[
         { <span class="dt">number</span>:<span class=
"st">'...'</span>, <span class="dt">street</span>:<span class=
"st">'...'</span>, <span class="dt">town</span>:<span class=
"st">'...'</span> }
      ,  { <span class="dt">number</span>:<span class=
"st">'...'</span>, <span class="dt">street</span>:<span class=
"st">'...'</span>, <span class="dt">town</span>:<span class=
"st">'...'</span> }
      ,  { <span class="dt">number</span>:<span class=
"st">'...'</span>, <span class="dt">street</span>:<span class=
"st">'...'</span>, <span class="dt">town</span>:<span class=
"st">'...'</span> }
      ]
   ,  <span class="dt">registeredOffice</span>:{
         <span class="dt">number</span>:<span class=
"st">'...'</span>, <span class="dt">street</span>:<span class=
"st">'...'</span>, <span class="dt">town</span>:<span class=
"st">'...'</span>
      }
   }
}  </code>
</pre>

  <p>Duck typing is of course a much looser concept than an XML
  document's tag names and collisions are possible where objects
  co-incidentally share property names. In practice however, I find
  the looseness a strength more often than a weakness. Under a
  tag-based marshaling from an OO language, sub-types are assigned
  a new tag name and as a consumer of the document, the 'isa'
  relationship between a 'class' tagname and it's 'sub-tagname' may
  be difficult to track. It is likely that if I'm unaware of this,
  I'm not interested in the extended capabilities of the subclass
  and would rather just continue to receive the base superclass
  capabilities as before. Under duck typing this is easy - because
  the data consumer lists the</p>

  <p>A final concept of type in json comes in the form of taking
  the first property of an object as being the tagname.
  Unsatisfactory, objects have an order while serialised as json
  but once deserialised typically have no further order.
  Clarinet.js seems to follow this pattern, notifying of new
  objects only once the first property's key is known so that it
  may be used to infer type. Can't be used with a general-purpose
  JSON writer tool, nor any JSON writer tool that reads from common
  objects.</p>

  <p>Relationship between type of a node and its purpose in the
  document. Purpose is often obvious from a combination of URL and
  type so can disregard the place in the document. This structure
  may be carefully designed but ultimately a looser interpretation
  of the structure can be safer.</p>

  <p>To extend JSONPath to support a concise expression of duck
  typing, I chose a syntax which is similar to fields in
  jsonFormat:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">
{
   <span class="st">"name"</span>: <span class="st">"..."</span>
,  <span class="st">"address"</span>: <span class="st">"..."</span>
,  <span class="st">"email"</span>: <span class="st">"..."</span>
}</code>
</pre>

  <p><code>{name address email}</code> The above JSONPath
  expression would match this object in json expression and like
  all json path expressions the pattern is quite similar to the
  object that it matches. The object below matches because it
  contains all the fields listed in between the curly braces in the
  above json path expresson.</p>

  <p>CSS4-style capturing. Reshuffle 'root' syntax to accommodate !
  and $.</p>

  <div class="figure">
    <img src="images/marshall.png" alt=
    "UML class diagram showing a person class in relationship with an address class. In implementation as Java the 'hasAddress' relationship would typically be reified as a getAddress method. This co-incidence of object type and the name of the field referring to the type lends itself well to the tendency for the immediate key before an object to be taken as the type when Java models are marshaled into json ">

    <p class="caption">UML class diagram showing a person class in
    relationship with an address class. In implementation as Java
    the 'hasAddress' relationship would typically be reified as a
    getAddress method. This co-incidence of object type and the
    name of the field referring to the type lends itself well to
    the tendency for the immediate key before an object to be taken
    as the type when Java models are marshaled into json</p>
  </div>

  <h2 id="stability-over-upgrades"><a href=
  "#stability-over-upgrades"><span class=
  "header-section-number">4.2</span> Stability over
  upgrades</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "extended json rest service that still works - maybe do a table instead ">

    <p class="caption">extended json rest service that still works
    - maybe do a table instead</p>
  </div>

  <p>Programming to identify a certain interesting part of a
  resource today should with a high probability still work when
  applied to future releases.</p>

  <p>Requires some discipline on behalf of the service provider:
  Upgrade by adding of semantics only most of the time rather than
  changing existing semantics.</p>

  <p>Adding of semantics should could include adding new fields to
  objects (which could themselves contain large sub-trees) or a
  "push-down" refactor in which what was a root node is pushed down
  a level by being suspended from a new parent.</p>

  <p>why JSONPath-like syntax allows upgrading message semantics
  without causing problems [SOA] how to guarantee non-breakages?
  could publish 'supported queries' that are guaranteed to work</p>

  <h2 id="parsing-the-json"><a href=
  "#parsing-the-json"><span class="header-section-number">4.3</span>
  Parsing the JSON</a></h2>

  <p>While SAX parsers provide an unfriendly interface to
  application developers, as a starting point for higher-level
  parsers they work very well (in fact, most XML DOM parsers are
  made in this way). The pre-existing project Clarinet is well
  tested, liberally licenced and compact, meeting the goals of this
  project perfectly. In fact, the name of this project, Oboe.js,
  was chosen in tribute to the value delivered by Clarinet.</p>

  <h2 id="api-design"><a href="#api-design"><span class=
  "header-section-number">4.4</span> API design</a></h2>

  <p>In designing the API developer ergonomics are the top
  priority. This is especially pertinent given that the library
  does nothing that can't be done with existing tools such as JSON
  SAX parsers but that those tools are not used because they
  require too much effort to form a part of most developers'
  everyday toolkit.</p>

  <p>Expose single global.</p>

  <p>To pursue good ergonomics, I will study successful libraries
  and, where appropriate, copy their APIs. We may assume that the
  existing libraries have already over time come to refined
  solutions to similar problems. Working in a style similar to
  existing libraries also makes the library easier to learn.
  Lastly, if we create a library which functions similarly enough
  to existing tools it should be easy to modify an existing project
  to adopt it. In the most common use cases, it should be possible
  to create a library with a close functional equivalence that can
  be used as a direct drop-in replacement. Used in this way, no
  progressive loading would be done but it opens the door for the
  project taking up the library to be refactored towards a
  progressive model over time. By imitating existing APIs we allow
  adoption as a series of small, easily manageable steps rather
  than a single leap. This is especially helpful for teams wishing
  to adopt this project working under Scrum because all tasks must
  be self-contained and fit within a fairly short timeframe.</p>

  <p>jQuery's basic call style for making an AJAX GET request
  follows:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"ot">jQuery</span>.<span class="fu">ajax</span>(<span class=
"st">"resources/shortMessage.txt"</span>)
   .<span class="fu">done</span>(<span class=
"kw">function</span>( text ) {
      <span class="ot">console</span>.<span class=
"fu">log</span>( <span class=
"st">'Got the text: '</span> + text ); 
   }).
   .<span class="fu">fail</span>(<span class=
"kw">function</span>(data) {
      <span class="ot">console</span>.<span class=
"fu">log</span>( <span class=
"st">'the request failed'</span> );      
   });</code>
</pre>

  <p>While for simple web applications usage is much as above,<br>
  In real world usage on more complex apps jQuery.ajax is often
  injected into the scope of the code which wants to use it. Easier
  stubbing so that tests don't have to make actual AJAX calls.</p>

  <p>While certainly callback-based, the jQuery is somewhat
  implicit in being event-based. There are no event names separate
  from the methods which add the listeners and there are no event
  objects, preferring to pass the content directly. The names used
  to add the events (done, fail) are also generic, used for all
  asynchronous requests. The methods are chainable which allows
  several listeners to be added in one statement.</p>

  <p>By method overloading, if the request requires more
  information than the parameter to <code>jQuery.ajax</code> may be
  an object. This pattern of accepting function parameters as an
  object is a common in Javascript for functions that take a large
  number of optional arguments because it makes understanding the
  purpose of each argument easier to understand from the callsite
  than if the meaning depended on the position in a linear
  arguments list and the gaps filled in with nulls.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"ot">jQuery</span>.<span class="fu">ajax</span>({ <span class=
"dt">url</span>:<span class=
"st">"resources/shortMessage.txt"</span>,
              <span class="dt">accepts</span>: <span class=
"st">"text/plain"</span>,
              <span class="dt">headers</span>: { <span class=
"st">'X-MY-COOKIE'</span>: <span class="st">'123ABC'</span> }
           });</code>
</pre>

  <p>Taking on this style,</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"fu">oboe</span>(<span class="st">'resources/someJson.json'</span>)
   .<span class="fu">node</span>( <span class=
"st">'person.name'</span>, <span class=
"kw">function</span>(name, path, ancestors) {
      <span class="ot">console</span>.<span class=
"fu">log</span>(<span class="st">"got a name "</span> + name);   
   })
   .<span class="fu">done</span>( <span class=
"kw">function</span>( wholeJson ) {
      <span class="ot">console</span>.<span class=
"fu">log</span>(<span class="st">'got everything'</span>);
   })
   .<span class="fu">error</span>( <span class=
"kw">function</span>() {
      <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">'actually, the download failed. Forget the'</span> + 
                  <span class=
"st">' people I just told you about'</span>);
   });</code>
</pre>

  <p>Because I forsee several patterns being added for most types
  of JSON documents, a shortcut format is also available for adding
  multiple patterns in a single call by using the patterns as the
  keys and the callbacks as the values in a key/value mapping:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"fu">oboe</span>(<span class="st">'resources/someJson.json'</span>)
   .<span class="fu">node</span>({  
      <span class="st">'person.name'</span>: <span class=
"kw">function</span>(personName, path, ancestors) {
         <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">"let me tell you about "</span> + name);
      },
      <span class="st">'person.address.town'</span>: <span class=
"kw">function</span>(townName, path, ancestors) {
         <span class="ot">console</span>.<span class=
"fu">log</span>(<span class=
"st">"they live in "</span> + townName);
      }
   });</code>
</pre>

  <p>Note the path and ancestors parameters in the examples above.
  Most of the time giving the callback the matching content is
  enough to be able to act but it is easy to imagine cases where a
  wider context matters. Consider this JSON:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">{ 
   <span class="st">"event"</span>: <span class=
"st">"mens 100m"</span>,
   <span class="st">"date"</span>: <span class=
"st">"5 Aug 2012"</span>,
   <span class="st">"medalWinners"</span>: {
      <span class="st">"gold"</span>:     {<span class=
"st">"name"</span>: <span class="st">'Bolt'</span>,    <span class=
"st">"time"</span>: <span class="st">"9.63s"</span>},
      <span class="st">"silver"</span>:   {<span class=
"st">"name"</span>: <span class="st">'Blake'</span>,   <span class=
"st">"time"</span>: <span class="st">"9.75s"</span>},
      <span class="st">"bronze"</span>:   {<span class=
"st">"name"</span>: <span class="st">'Gatlin'</span>,  <span class=
"st">"time"</span>: <span class="st">"9.79s"</span>}
   }
}  </code>
</pre>

  <p>Here we can extract the runners by the patterns such as
  <code>{name time}</code> or <code>medalWinners.*</code> but
  clearly the location of the node in the document is interesting
  as well as the context. The <code>path</code> parameter provides
  this information by way of an array of strings plotting the
  descent from the JSON root to the match, for example
  <code>['medalWinners', 'gold']</code>. Similarly, the
  <code>ancestors</code> array is a list of the ancestors starting
  at the immediate parent of the found node and ending with the
  JSON root node. For all but the root node (which has no ancestors
  anyway) the nodes in this list will be only partially parsed.
  Being untyped, Javascript does not enforce the arity of the
  callback. Because much of the time only the content itself is
  needed, the API design orders the callback parameters to take
  advantage of the loose typing so that a unary function taking
  only the content may be given.</p>

  <p>For the widest context currently available, the whole document
  as it has been parsed so far may be accessed using the
  <code>.root</code> method. Since <code>.root</code> relates to
  the oboe instance itself rather than the callback per-say, it can
  be accessed from any code with a reference to the oboe
  object.</p>

  <p>
  <code>http://nodejs.org/docs/latest/api/events.html#events_emitter_on_event_listener</code></p>

  <p>In node.js the code style is more obviously event-based.
  Listeners are added via a <code>.on</code> method with a string
  event name given as the first argument. Adopting this style, my
  API design for oboe.js also allows events to be added as:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"fu">oboe</span>(<span class="st">'resources/someJson.json'</span>)
   .<span class="fu">on</span>( <span class=
"st">'node'</span>, <span class=
"st">'medalWinners.*'</span>, <span class=
"kw">function</span>(person, path, ancestors) {
      <span class="ot">console</span>.<span class=
"fu">log</span>( <span class="ot">person</span>.<span class=
"fu">name</span> + <span class=
"st">' won the '</span> + <span class=
"fu">lastOf</span>(path) + <span class="st">' medal'</span> );
   });</code>
</pre>

  <p>While allowing both styles uncountably creates an API which is
  larger than it needs to be, creating a library which is targeted
  at both the client and server side, I hope this will help
  adoption by either camp. The Two styles are similar enough that a
  person familiar with one should be able to pick up the other
  without difficulty. In implementation a duplicative API should
  require only a minimal degree of extra coding because these parts
  may be expressed in common and their scope reduced using partial
  completion. Because <code>'!'</code> is the JSONPath for the root
  of the document, for some callback c, <code>.done(c)</code> is a
  synonym for <code>.node('!', c)</code> and therefore below a thin
  interface layer may share an implementation. Likewise,
  <code>.node</code> is easily expressible as a partial completion
  of <code>.on</code> with <code>'node'</code>.</p>

  <h2 id="earlier-callbacks-when-paths-are-matched"><a href=
  "#earlier-callbacks-when-paths-are-matched"><span class=
  "header-section-number">4.5</span> Earlier callbacks when paths
  are matched</a></h2>

  <p>Following with the project's aim of giving callbacks as early
  as possible, sometimes useful work can be done when a node is
  known to exist but before we have the contents of the node. This
  means that each node found in a JSON document has the potential
  to trigger notifications at two points: when it is first
  discovered and when it is complete. The API facilitates this by
  providing a <code>path</code> callback following much the same
  pattern as the <code>node</code> callback.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"fu">oboe</span>(<span class="st">'events.json'</span>)
   .<span class="fu">path</span>( <span class=
"st">'medalWinners'</span>, <span class="kw">function</span>() {
      <span class=
"co">// We don't know the winners yet but we know we have some so let's</span>
      <span class=
"co">// start drawing the table already:    </span>
      <span class="kw">interface</span>.<span class=
"fu">showMedalTable</span>();
   })
   .<span class="fu">node</span>( <span class=
"st">'medalWinners.*'</span>, <span class=
"kw">function</span>(person, path) {    
      <span class="kw">interface</span>.<span class=
"fu">addPersonToMedalTable</span>(person, <span class=
"fu">lastOf</span>(path));
   })
   .<span class="fu">fail</span>( <span class=
"kw">function</span>(){
      <span class="co">// That didn't work. Revert!</span>
      <span class="kw">interface</span>.<span class=
"fu">hideMedalTable</span>();
   });</code>
</pre>

  <p>In implementation providing path notifications is a simple
  matter of allowing the evaluation of the json path expressions
  when items are pushed to the stack of current nodes in addition
  to when they are popped.</p>

  <h2 id="micro-library"><a href="#micro-library"><span class=
  "header-section-number">4.6</span> Micro-library</a></h2>

  <p>Http traffic, especially sending entropy-sparse text formats
  is often gzipped at point of sending in order to deliver it more
  quickly, so in measuring a download footprint it usually makes
  more sense to compare post-gzipping. A Javascript library
  qualifies as being <em>micro</em> if it is delivered in 5k or
  less, 5120 bytes. Micro-libraries also tend to follow the ethos
  that it is better for a developer to gather together several tiny
  libraries than one that uses a one-size-fits-all approach. This
  perhaps echos the unix command line tradition of small programs
  which each do do exactly one thing.<br>
  This project feels on the edge of what is possible to elegantly
  do as a micro-library so while the limit is somewhat arbitrary,
  for the sake of adoption smaller is better and keeping below this
  limit whilst writing readable code is an interesting
  challenge.</p>

  <p>Link to micro-js in there somewhere</p>

  <h2 id="responding-to-failures"><a href=
  "#responding-to-failures"><span class=
  "header-section-number">4.7</span> Responding to
  failures</a></h2>

  <p><em>is this section really needed?</em></p>

  <p>As discussed above in API design, errors due to the transport
  will cause a callback to be returned to the calling
  application.</p>

  <p>Not automatic, just inform user. Most of the time will want to
  perform some kind of rollback from optimistic locking.</p>

  <p>I did consider the option of automatially resuming failed
  requests. Http 1.1 provides a mechanism for Byte Serving via the
  Accepts-Ranges header [<a href=
  "http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html">http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html</a>#sec14.5]
  which can be used to request any contiguous part of a response
  rather than the whole. However, having examined this option I
  came to the conclusion that this approach would be brittle
  because it assumes two requests to the same URL will give exactly
  the same response.</p>

  <p>A better option</p>

  <h2 id="fallback-support-on-less-capable-platforms"><a href=
  "#fallback-support-on-less-capable-platforms"><span class=
  "header-section-number">4.8</span> Fallback support on
  less-capable platforms</a></h2>

  <p><em>something about market share and link to figures in an
  appendix?</em></p>

  <p>Because of differences in the capabilities in browsers,
  providing a streaming REST client is not possible on all
  browsers. If this were possible, it would not have been necessary
  to invent push pages or long polling. Specifically, none but the
  most recent versions of Internet Explorer provide any way to
  access an AJAX response before it is complete. I have taken the
  design decision that it is ok to degrade on these platforms so
  long as the programmer developing with Oboe.js does not have to
  make special cases for these platforms. Likewise, nor should the
  REST service need be aware of the client, disallowing detecting
  client capabilities and switching transport strategy. Requiring
  branching on either side places extra responsibilities on the
  programmer which they would not otherwise be required to consider
  whilst viewing REST through a non-streaming lens.</p>

  <p>Given that streaming is not possible on older platforms, I
  must considering the best experience that is possible. We may
  imagine a situation in which the whole download completes
  followed by all listeners being notified from a single Javascript
  frame of execution. While not progressive in any way, this
  situation is essentially standard REST plus JSONPath routing and
  no less performant than if more traditional libraries were used.
  I find this satisfactory: for the majority of users the
  experience is improved and for the others it is made no worse,
  resulting in a net overall benefit.</p>

  <p>In the Javascript language itself interoperability is very
  rarely an issue. Javascript's model of prototypical inheritance
  allows changes to be made to the browser's libraries on the fly;
  as soon as a prototype is changed all instances of the type
  reflect the change even if they has already been created
  (source). Because the base types that come with the browser are
  essentially global, changing them for the use of a single
  codebase is generally deprecated because of the possibility of
  collisions. However, this technique is often used to retrofit new
  standards onto older platforms. For example, the Functional-style
  Array iteration methods remove the need to write C-style for
  loops and are defined in the ECMAScript 5 specification <a href=
  "http://www.jimmycuadra.com/posts/ecmascript-5-array-methods">http://www.jimmycuadra.com/posts/ecmascript-5-array-methods</a>
  - all of these methods are implementable in pure Javascript.
  There exist several mature pure Javascript projects for browsers
  which lack native support, licenced to allow inclusion in this
  project (CITE ONE). While I am constrained in the ability to
  accept streaming AJAX in older browsers, there is no such
  restriction on my ability to express my thesis in a more modern,
  functional style of Javascript.</p>

  <p>Node is highly capable, with no shortcomings that will make
  Oboe.js difficult to implement. It does, however use its own
  stream API rather than emulate the browser API so will require
  platform-specific programming inside the library. This
  abstraction will be hidden from the library user so will not
  require any special programming on their part.</p>

  <h1 id="implementation"><a href="#implementation"><span class=
  "header-section-number">5</span> Implementation</a></h1>

  <h2 id="decomposition-into-components"><a href=
  "#decomposition-into-components"><span class=
  "header-section-number">5.1</span> Decomposition into
  components</a></h2>

  <div class="figure">
    <img src="images/overallDesign.png" alt=
    "Inter-related components that make up Oboe.js showing flow from http transport to registered callbacks. UML facet/receptacle notation is used to show the flow of events but a centralised event bus which transmits the events is omitted for clarity">

    <p class="caption"><strong>Inter-related components that make
    up Oboe.js</strong> showing flow from http transport to
    registered callbacks. UML facet/receptacle notation is used to
    show the flow of events but a centralised event bus which
    transmits the events is omitted for clarity</p>
  </div>

  <p>Split up concerns etc</p>

  <p>Controller</p>

  <p>The problem decomposes nicely into loosely-coupled components,
  each quite detailed but unconcerned with the others. Once these
  parts are made, bringing them together under a simple controller
  is just a matter of joining the dots.</p>

  <p>Code is good when each line feels like a statement of fact
  rather than a way of going about making an intention true.</p>

  <p>Content builder: like a decorator/wrapper but event based, not
  based on object wrapping.</p>

  <h2 id="inversion-of-control-and-communication-between-parts">
  <a href=
  "#inversion-of-control-and-communication-between-parts"><span class="header-section-number">
  5.2</span> Inversion of Control and communication between
  parts</a></h2>

  <p>Aim of creating a micro-library rules out building in a
  general-purpose IoC library.</p>

  <p>However, can still follow the general principles.</p>

  <p>Why the Observer pattern (cite: des patterns) lends itself
  well to MVC and inversion of control.</p>

  <p>What the central controller does; acts as a plumber connecting
  the various parts up. Since oboe is predominantly event/stream
  based, once wired up little intervention is needed from the
  controller. Ie, A knows how to listen for ??? events but is
  unintested who fired them.</p>

  <p>Local event bus Why? Makes testing easy (just put appropriate
  event on the bus rather than trying to fake calls from linked
  stubs). Decouples, avoids parts having to locate or be passed
  other parts. Wouldn't scale indefinately, does provide something
  of a mingled-purpose space. Why not more direct event passing
  without a separate event bus (ie, everything as an emitter and
  reciever of events?)</p>

  <h2 id="grunt"><a href="#grunt"><span class=
  "header-section-number">5.3</span> Grunt</a></h2>

  <h2 id="automated-testing"><a href=
  "#automated-testing"><span class=
  "header-section-number">5.4</span> Automated testing</a></h2>

  <p>Do it on every save!</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Relationship between various files and test libraries other half of sketch from notebook">

    <p class="caption">Relationship between various files and test
    libraries <em>other half of sketch from notebook</em></p>
  </div>

  <p>How automated testing improves what can be written, not just
  making what is written more reliable.</p>

  <p>TDD drives development by influencing the design - good design
  is taken as that which is amenable to testing rather than which
  describes the problem domain accurately or solves a problem with
  minimum resources. Amenable to testing often means split into
  many co-operating parts so that each part may be tested via a
  simple test.</p>

  <p>Bt encourageing splitting into co-operating objects, TDD to a
  certain degree is anti-encapsulation. The public object that was
  extracted as a new concern from a larger object now needs public
  methods whereas before nothing was exposed.</p>

  <div class="figure">
    <img src="images/pyramid.png" alt=
    "The testing pyramid is a common concept, relying on the assumption that verification of small parts provides a solid base from which to compose system-level behaviours. A Lot of testing is done on the low-level components of the system, whereas for the high-level tests only smoke tests are provided. ">

    <p class="caption">The testing pyramid is a common concept,
    relying on the assumption that verification of small parts
    provides a solid base from which to compose system-level
    behaviours. A Lot of testing is done on the low-level
    components of the system, whereas for the high-level tests only
    smoke tests are provided.</p>
  </div>

  <p>Jstd can serve example files but need to write out slowly
  which it has no concept of. Customistation is via configuration
  rather than by plug-in, but even if it were, the threading model
  is not suitable to create this kind of timed output.</p>

  <p>Tests include an extremely large file twentyThousandRecords.js
  to test under stress</p>

  <p>Why jstd's built in proxy isn't sufficient. An example of a
  typical Java webserver, features thread-based mutlithreading in
  which threads wait for a while response to be received.</p>

  <p>Tests deal with the problem of "irreducible complexity" - when
  a program is made out of parts whose correct behaviour cannot be
  observed without all of the program. Allows smaller units to be
  verified before verifying the whole.</p>

  <p>Conversely, automated testing allows us to write
  incomprehensible code by making us into more powerful
  programmers, it is possible building up layers of complexity one
  very small part at a time that we couldn't write in a simple
  stage. Clarity &gt; cleverness but cleverness has its place as
  well (intriducing new concepts)</p>

  <p>Testing via node to give something to test against -
  slowserver. Proxy. JSTD not up to task. Shows how useful node is
  as a 'network glue'. The same as C was once described as a 'thin
  glue' [<a href=
  "http://www.catb.org/esr/writings/taoup/html/ch04s03.html">http://www.catb.org/esr/writings/taoup/html/ch04s03.html</a>].
  Transparent proxy is about 20 lines. Transparent enough to fool
  JSTD into thinking it is connecting directly to its server.</p>

  <p>Node comes with very little built in (not even http) but
  relies on libraries written in the language itself to do
  everything. Could implement own http on top of sockets if wanted
  rather than using the provided one.</p>

  <p>The test pyramid concept fits in well with the hiding that is
  provided. Under the testing pyramid only very high level
  behaviours are tested as ??? tests. While this is a lucky
  co-incidence, it is also an unavoidable restriction. Once
  compiled into a single source file, the individual components are
  hidden, callable only from withing their closure. Hence, it would
  not be possible to test the composed parts individually
  post-concatenation into a single javascript file, not even via a
  workarround for data hiding such as found in Java's reflection.
  Whereas in Java the protection is a means of protecting otherwise
  addressable resources, once a function is trapped inside a
  javascript closure without external exposure it is not just
  protected but, appearing in no namespaces, inherently
  unreferenceable.</p>

  <p>TDD fits well into an object pattern because the software is
  well composed into separate parts. The objects are almost
  tangible in their distinction as separate encapsulated entities.
  However, the multi-paradigm style of my implementation draws much
  fainter borders over the implementation's landscape.</p>

  <p>Approach has been to the test the intricate code, then for
  wiring don't have tests to check that things are plumbed together
  correctly, rather rely on this being obvious enough to be
  detected via a smoke test.</p>

  <p>A good test should be able to go unchanged as the source under
  test is refactored. Indeed, the test will be how we know that the
  code under test still works as intended. Experince tells me that
  testing that A listens to B (ie that the controller wires the
  jsonbuilder up to clarinet) produces the kind of test that
  'follows the code arround' meaning that because it is testing
  implementation details rather than behaviours, whenever the
  implementation is updated the tests have to be updated too.</p>

  <p>By testing individual tokens are correct and the use of those
  tokens as a wider expression, am testing the same thing twice.
  Arguably, redundant effort. But may simply be easier to write in
  that way - software is written by a human in a certain order and
  if we take a bottom-up approach to some of that design, each
  layer is easier to create if we first know the layers that it
  sits on are sound. Writing complex regular expressions is still
  programming and it is more difficult to test them completely when
  wrapped in rather a lot more logic than directly. For example, a
  regex which matches "{a,b}" or "{a}" but not "{a,}" is not
  trivial.</p>

  <p>Can test less exhaustively on higher levels if lower ones are
  well tested, testing where it is easier to do whilst giving good
  guarantees.</p>

  <p>Genuine data hiding gets in the way sometimes. Eg, token
  regexes are built from the combination of smaller regualar
  expressions for clarity (long regular expressions are concise but
  hard to read), and then wrapped in functions (why? - explain to
  generify interface) before being exposed. Because the components
  are hidden in a scope, they are not addressable by the tests and
  therefore cannot be directly tested. Reluctantly</p>

  <p>One dilemma in implementing the testing is how far to test the
  more generic sections of the codebase as generic components. A
  purist approach to TDD would say</p>

  <h2 id="styles-of-programming"><a href=
  "#styles-of-programming"><span class=
  "header-section-number">5.5</span> Styles of Programming</a></h2>

  <p>"Mixed paradigm" design. But not classical: don't need
  inheritance.</p>

  <p>Interestingly, the mixed paradigm design hasn't changed the
  top-level design very much from how it'd be as a pure OO project
  (IoC, decorators, event filters, pub/sub etc).</p>

  <p>The code presented is the result of the development many prior
  versions, it has never been rewritten in the sense of starting
  again. Nonetheless, every part has been complely renewed several
  times. I am reviewing only the final version. Git promotes
  regular commits, there have been more than 500.</p>

  <p>some of it is pure functional (jsonPath, controller) ie, only
  semantically different from a Haskell programme others,
  syntactically functional but stateful to fit in with expected
  APIs etc</p>

  <p>JsonPath implementation allows the compilation of complex
  expressions into an executable form, but each part implementing
  the executable form is locally simple. By using recursion,
  assembling the simple functions into a more function expressing a
  more complex rule also follows as being locally simple but
  gaining a usefully sophisticated behaviour through composition of
  simple parts. Each recursive call of the parser identifies one
  token for non-empty input and then recursively digests the
  rest.</p>

  <p>The style of implementation of the generator of functions
  corresponding to json path expressions is reminiscent of a
  traditional parser generator, although rather than generating
  source, functions are dynamically composed. Reflecting on this,
  parser gens only went to source to break out of the ability to
  compose the expressive power of the language itself from inside
  the language itself. With a functional approach, assembly from
  very small pieces gives a similar level of expressivity as
  writing the logic out as source code.</p>

  <p>Why could implement Function#partial via prototype. Why not
  going to. Is a shame. However, are using prototype for minimal
  set of polyfills. Not general purpose.</p>

  <p>Different ways to do currying below:</p>

  <p>Partial completion is implemented using the language itself,
  not provided by the language.</p>

  <p>Why would we choose 1 over the other? First simpler from
  caller side, second more flexible. Intuitive to call as a single
  call and can call self more easily.</p>

  <p>In same cases, first form makes it easier to communicate that
  the completion comes in two parts, for example:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"> <span class=
"fu">namedNodeExpr</span>(previousExpr, capturing, name, pathStack, nodeStack, stackIndex )</code>
</pre>

  <p>There is a construction part (first 3 args) and a usage part
  (last three). Comsume many can only be constructed to ues consume
  1 in second style because may refer to its own paritally
  completed version.</p>

  <p>In first case, can avoid this: <code>consume1(
  partialComplete(consumeMany, previousExpr, undefined, undefined),
  undefined, undefined, pathStack, nodeStack, stackIndex);</code>
  because function factory can have optional arguments so don't
  have to give all of them</p>

  <p>Function factory easier to debug. 'Step in' works. With
  partialCompletion have an awkward proxy function that breaks the
  programmer's train of thought as stepping through the code.</p>

  <p>Why it is important to consider the frame of mind of the coder
  (CITEME: Hackers and Painters) and not just the elegance of the
  possible language expressions.</p>

  <p>If implementing own functional caching, functional cache
  allows two levels of caching. Problematic though, for example no
  way to clear out the cache if memory becomes scarce.</p>

  <p>Functional programming tends to lend better to minification
  than OO-style because of untyped record objects (can have any
  keys).</p>

  <p>Lack of consistency in coding (don't write too much, leave to
  the conclusion)</p>

  <p>Final consideration of coding: packaging up each unit to
  export a minimal interface. * Why minimal interfaces are better
  for minification</p>

  <p>Need to build an abstraction layer over xhr/xhr2/node. Can
  only work for packets in-order, for out-of-order packets
  something else happens.</p>

  <h2 id="js-code-style"><a href="#js-code-style"><span class=
  "header-section-number">5.6</span> JS code style</a></h2>

  <p>Javascript: not the greatest for 'final' elegant presentation
  of programming. Does allow 'messy' first drafts which can be
  refactored into beautiful code. Ie, can write stateful and
  refactor in small steps towards being stateless. An awareness of
  beautiful languages lets us know the right direction to go in. An
  ugly language lets us find something easy to write that works to
  get us started. Allows a very sketchy program to be written,
  little more than a programming scratchpad.</p>

  <p>Without strict typing, hard to know if program is correct
  without running it. In theory (decidability) and in practice
  (often find errors through running and finding errors thrown).
  Echo FPR: once compiling, good typing tends to give a reasonable
  sureness that the code is correct.</p>

  <p>Criticisms of Node. Esp from Erlang etc devs. Pyramid code and
  promises. Node programs often so asynchronous and callback based
  they become unclear in structure. Promises approach to avoid
  pyramid-shaped code and callback spaghetti.</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"co">// example of pyramid code</span></code>
</pre>

  <p>functional, pure functional possible [FPR] but not as nicely
  as in a pure functional language, ie function caches although can
  be implemented, not universal on all functions.</p>

  <p>Although the streams themselves are stateful, because they are
  based on callbacks it is entirely possible to use them from a
  component of a javascript program which is wholly stateless.</p>

  <h2 id=
  "performance-implications-of-functional-javascript-subsume-into-above">
  <a href=
  "#performance-implications-of-functional-javascript-subsume-into-above">
  <span class="header-section-number">5.7</span> Performance
  implications of functional javascript (subsume into
  above?)</a></h2>

  <p>V8 and other modern JS engines are often said to be
  'near-native' speed, meaning it runs at close to the speed of a
  similarly coded C program. However, this relies on the programmer
  also coding in the style of a C programmer, for example with only
  mono-morphic callsites and without a functional style. Once
  either of those programming techniques is taken up performance
  drops rapidly [<a href=
  "http://rfrn.org/">http://rfrn.org/</a>~shu/2013/03/20/two-reasons-functional-style-is-slow-in-spidermonkey.html]
  9571 ms vs 504 ms. When used in a functional style, not
  'near-native' in the sense that not close to the performance
  gained by compiling a well designed functional language to
  natively executable code. Depends on style coded in, comparison
  to native somewhat takes C as the description of the operation of
  an idealised CPU rather than an abstract machine capable of
  executing on an actual CPU.</p>

  <p>(perhaps move to background, or hint at it, eg "although there
  are still some performance implications involved in a functional
  style, javascript may be used in a non-pure functional style") -
  with link to here</p>

  <p>The performance degradation, even with a self-hosted forEach,
  is due to the JIT&acirc;&euro;&trade;s inability to efficiently
  inline both the closures passed to forEach</p>

  <p>Lambda Lifting, currently not implemented in SpiderMonkey or
  V8: <a href=
  "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.4346">
  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.4346</a></p>

  <p>The transformations to enable the above criteria are tedious
  and are surely the purview of the compiler. All
  that&acirc;&euro;&trade;s needed are brave compiler hackers</p>

  <p>JS is much faster with "monomorphic call sites"</p>

  <p>However, js execution time is not much of a problem,</p>

  <h2 id=
  "preferring-functions-over-constructors-subsume-into-above-section">
  <a href=
  "#preferring-functions-over-constructors-subsume-into-above-section">
  <span class="header-section-number">5.8</span> Preferring
  functions over constructors (subsume into above
  section?)</a></h2>

  <p>What constructors are in js. Any function, but usually an
  uppercase initial char indicates that it is intended to be used
  as a constructor.</p>

  <p>Inheritence is constructed using the language itself. While
  this is more flexible and allows each project to define a bespoke
  version of inherience to suit their particular needs or
  preferences, it also hampers portability more than an 'extends'
  keyword would.</p>

  <blockquote>
    <p>So far, the JavaScript community has not agreed on a common
    inheritance library (which would help tooling and code
    portability) and it is doubtful that that will ever happen.
    That means, we&acirc;&euro;&trade;re stuck with constructors
    under ECMAScript 5. <a href=
    "http://www.2ality.com/2013/07/defending-constructors.html">http://www.2ality.com/2013/07/defending-constructors.html</a></p>
  </blockquote>

  <p>Functions can be like Factories, gives me the flexability to
  chagne how something is created but by exposing a constructor are
  stuck with using 'new' to create an instance of exactly one type.
  'new' is inconsistent invocation with rest of language.</p>

  <p>Dart has 'factory' constructors which are called like
  constructors but act like factory functions: (<a href=
  "http://www.dartlang.org/docs/dart-up-and-running/contents/ch02.html">http://www.dartlang.org/docs/dart-up-and-running/contents/ch02.html</a>#ch02-constructor-factory)</p>

  <h2 id="design-and-implementation-of-the-jsonpath-parser">
  <a href="#design-and-implementation-of-the-jsonpath-parser"><span class="header-section-number">
  5.9</span> Design and implementation of the JSONPath
  parser</a></h2>

  <p>Show evolution of it. Like most compilers, first try was just
  a bunch of regexes that generated a regex to match the pattern.
  While compact, was unmaintainable. Moved onto functional,
  stateless Javascript. Lots of refactoring possible because very
  comprehensively tested.</p>

  <p>Split into tokens and statement builder.</p>

  <p>NB: This consideration of type in json could be in the
  Background section.</p>

  <p>Clause functions, each passes onto the next function if it
  passes. Functions to consume. Can apply more than one test to a
  single node. Tests generated by clause functions may be against
  either the immediate path to that node (name clauses) or the node
  itself (duck-type clauses). For example, the jsonPath
  <code>!.$person..{height tShirtSize}</code> may be expressed
  functionally in Javascript as such:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">var</span> jsonPathEvaluator =
   <span class="fu">statementExpr</span>( 
      <span class="fu">duckTypeClause</span>(
         <span class=
"fu">skipMany</span>(                                <span class=
"co">// for '..'  </span>
            <span class=
"fu">capture</span>(                              <span class=
"co">// for css4-style '$' notation</span>
               <span class="fu">nameClause</span>(
                  <span class=
"fu">skip1</span>(                          <span class=
"co">// '.' after '!'  </span>
                     rootExpr                     <span class=
"co">// '!' at start of JSONPath expression</span>
                  ) 
               <span class="st">'person'</span> )
            )
      ), [<span class="st">'height'</span>, <span class=
"st">'tShirtSize'</span>])
   );      </code>
</pre>

  <p>The above is actually a slight simplification because calls to
  partialComplete are implied. Once this evaluator function has
  been created, testing against a candidate ascent is simply
  function invocation:</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript"><span class=
"kw">var</span> result = <span class=
"fu">jsonPathEvaluator</span>(ascent);</code>
</pre>

  <p>Why done as a function returning a function (many calls per
  pattern - one for each node found to check for matches).</p>

  <p>Match from right-to-left, or, deepest-to-root. Why this way?
  That's how the patterns work (mostly)</p>

  <p>Why an existing jsonPath implmentation couldn't be used: need
  to add new features and need to be able to check against a path
  expressed as a stack of nodes.</p>

  <p>More important to efficiently detect or efficiently compile
  the patterns?</p>

  <p>As discussed in section ???, Sax is difficult to program and
  not widely used.</p>

  <p>Essentially two ways to identify an interesting node - by
  location (covered by existing jsonpath)</p>

  <p>Why duck typing is desirable in absense of genuine types in
  the json standard (ala tag names in XML). or by a loose concept
  of type which is not well supported by existing jsonpath
  spec.</p>

  <p>Compare duck typing to the tag name in</p>

  <p>Explain why Haskel/lisp style lists are used rather than
  arrays</p>

  <ul>
    <li>In parser clauses, lots of 'do this then go to the next
    function with the rest'.</li>

    <li>Normal arrays extremely inefficient to make a copy with one
    item popped off the start</li>

    <li>Link to FastList on github</li>

    <li>For sake of micro-library, implemented tiny list code with
    very bare needed</li>

    <li>Alternative (first impl) was to pass an index around</li>

    <li>But clause fns don't really care about indexes, they care
    about top of the list.</li>

    <li>Slight advantage to index: allows going past the start for
    the root path (which doesn't have any index) instead, have to
    use a special value to keep node and path list of the same
    length</li>

    <li>Special token for root, takes advantage of object identity
    to make certain that cannot clash with something from the json.
    Better than '<strong>root</strong>' or similar which could
    clash. String in js not considered distinct, any two strings
    with identical character sequences are indistinguishable.</li>
  </ul>

  <p>Anti-list: nothing is quite so small when making a
  mircro-library as using the types built into the language, coming
  as they are for zero bytes.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Diagram showing why list is more memory efficient - multiple handles into same structure with different starts, contrast with same as an array">

    <p class="caption">Diagram showing why list is more memory
    efficient - multiple handles into same structure with different
    starts, contrast with same as an array</p>
  </div>

  <ul>
    <li>For recognisably with existing code, use lists internally
    but transform into array on the boundary between Oboe.js and
    the outside world (at same time, strip off special 'root path'
    token)</li>
  </ul>

  <p>In parser, can't use 'y' flag to the regualr expression engine
  which would allow much more elegant matching. Only alternative is
  cumersome: to slice the string and match all tokens with regexes
  starting with '^' in order to track the current location.
  [<a href=
  "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular/_Expressions">https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular\_Expressions</a>]</p>

  <h2 id="incrementally-building-up-the-content"><a href=
  "#incrementally-building-up-the-content"><span class=
  "header-section-number">5.10</span> Incrementally building up the
  content</a></h2>

  <p>Like SAX, calls from clarinet are entirely 'context free'. Ie,
  am told that there is a new object but without the preceding
  calls the root object is indistinguishable from a deeply nested
  object. Luckily, it should be easy to see that building up this
  context is a simple matter of maintaining a stack describing the
  descent from the root node to the current node.</p>

  <p>jsonPath parser gets the output from the
  incrementalParsedContent, minimally routed there by the
  controller.</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Show a call into a compiled jsonPath to explain coming from incrementalParsedContent with two lists, ie the paths and the objects and how they relate to each other. Can use links to show that object list contains objects that contain others on the list. Aubergine etc example might be a good one">

    <p class="caption">Show a call into a compiled jsonPath to
    explain coming from incrementalParsedContent with two lists, ie
    the paths and the objects and how they relate to each other.
    Can use links to show that object list contains objects that
    contain others on the list. Aubergine etc example might be a
    good one</p>
  </div>

  <p>Explain match starting from end of candidate path</p>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "Some kind of diagram showing jsonPath expressions and functions partially completed to link back to the previous function. Include the statementExpr pointing to the last clause">

    <p class="caption">Some kind of diagram showing jsonPath
    expressions and functions partially completed to link back to
    the previous function. Include the statementExpr pointing to
    the last clause</p>
  </div>

  <p>On first attempt at ICB, had two stacks, both arrays, plus
  reference to current node, current key and root node. After
  refactorings, just one list was enough. Why single-argument
  functions are helpful (composition etc)</p>

  <p>Stateless makes using a debugger easier - can look back in
  stack trace and because of no reassignment, can see the whole,
  unchanged state of the parent call. What the params are now are
  what they always have been, no chance of reassignment (some code
  style guides recommend not to reassign parameters but imperative
  languages generally do not forbid it) No Side effects: can type
  expressions into debugger to see evaluation without risk of
  changing program execution.</p>

  <p>A refactoring was used to separate logic and state:</p>

  <ul>
    <li>Take stateful code</li>

    <li>Refactor until there is just one stateful item</li>

    <li>This means that that item is reassigned rather than
    mutated</li>

    <li>Make stateless by making all functions take and return an
    instance of that item<br></li>

    <li>Replace all assignment of the single stateful var with a
    return statement</li>

    <li>Create a simple, separate stateful controller that just
    updates the state to that returned from the calls</li>
  </ul>

  <p>Very testable code because stateless - once correct for params
  under test, will always be correct. Nowhere for bad data to hide
  in the program.</p>

  <p>How do notifications fit into this?</p>

  <p>By going to List-style, enforced that functions fail when not
  able to give an answer. Js default is to return the special
  'undefined' value. Why this ensured more robustness but also
  sometimes took more code to write, ie couldn't just do if(
  tail(foo)) if foo could be empty but most of the time that would
  be correct</p>

  <h2 id="callback-and-mutability-problem"><a href=
  "#callback-and-mutability-problem"><span class=
  "header-section-number">5.11</span> Callback and mutability
  Problem</a></h2>

  <p>Stateful controller very easy to test - only 1 function.</p>

  <p>Javascript provides no way to decalre an object with 'cohorts'
  who are allowed to change it whereas others cannot - vars may be
  hidden via use of scope and closures (CITE: crockford) but
  attributes are either mutable or immutable.</p>

  <p>Why this is a problem.</p>

  <ul>
    <li>bugs likely to be attributied to oboe because they'll be in
    a future <em>frame of execution</em>. But user error.</li>
  </ul>

  <p>Potential solutions:</p>

  <ul>
    <li>full functional-style immutability. Don't change the
    objects, just have a function that returns a new one with one
    extra property. Problem - language not optimised for this. A
    lot of copying. Still doesn't stop callback receiver from
    changing the state of hte object given. (CITE: optimisations
    other languages use)</li>

    <li>immutable wrappers.</li>

    <li>defensive cloning</li>

    <li>defining getter properties</li>
  </ul>

  <h2 id="packaging-the-library-as-a-single-distributable-file">
  <a href=
  "#packaging-the-library-as-a-single-distributable-file"><span class="header-section-number">
  5.12</span> Packaging the library as a single distributable
  file</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "packaging of many javascript files into multiple single-file packages. The packages are individually targeted at different execution contexts, either browsers or node get from notebook, split sketch diagram in half">

    <p class="caption">packaging of many javascript files into
    multiple single-file packages. The packages are individually
    targeted at different execution contexts, either browsers or
    node <em>get from notebook, split sketch diagram in
    half</em></p>
  </div>

  <ul>
    <li>One file for browser and node is common.</li>

    <li>say how this is done</li>

    <li>why not doing this (adds bloat, inhibits micro-lib)</li>

    <li>extra challenges</li>

    <li>http adaptor is different</li>

    <li>packaging is different</li>

    <li>two distributable files, for node minification is not
    important so don't do to help debugging.</li>
  </ul>

  <p>Composition of several source files into a distributable
  binary-like text file</p>

  <p>Why distributed javascript is more like a binary than a source
  file. Licencing implications? Would be (maybe) under GPL. Not so
  under BSD.</p>

  <p>Inherent hiding by wrapping in a scope.</p>

  <p>Names of functions and variable names which are provably not
  possible to reference are lost for the sake of reduction of size
  of the source.</p>

  <p>Packaging for node or browser. No need to minify for node but
  concatenation still done for ease of inclusion in projects</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">typical pattern <span class=
"kw">for</span> packaging to work <span class=
"kw">in</span> either a <span class="ot">node</span>.<span class=
"fu">js</span> <span class="fu">server</span> <span class=
"fu">or</span> <span class="fu">a</span> <span class=
"fu">web</span> <span class="fu">browser</span></code>
</pre>

  <p>Packaging for use in frameworks.</p>

  <ul>
    <li>Many frameworks already come with a wrapper arround the
    browser's inbuilt ajax capabilities</li>

    <li>
      <p>they don't add to the capabilities but present a nicer
      interface</p>
    </li>

    <li>
      <p>I'm not doing it but others are ** browser-packaged
      version should be use agnostic and therefore amenable to
      packaging in this way</p>
    </li>
  </ul>

  <p>Why uglify</p>

  <ul>
    <li>Covers whole language, not just a well-advised subset.</li>

    <li>Closure compiler works over a subset of javascript rather
    than the whole language.</li>
  </ul>

  <p>Why not require. Bits on what rq is can go into B&amp;R
  section. <em>Some of this can move into 3_Background.md</em></p>

  <ul>
    <li>What it is</li>

    <li>Why so popular</li>

    <li>Why a loader is necessary - js doesn't come with an import
    statement</li>

    <li>How it can be done in the language itself without an import
    statement</li>

    <li>Meant more for AMD than for single-load code</li>

    <li>Situations AMD is good for - large site, most visitors
    don't need all the code loaded</li>

    <li>Depends on run-time component to be loaded even after code
    has been optimised</li>

    <li>Small compatible versions exist that just do loading
    (almond)<br></li>

    <li>Why ultimately not suitable for a library like this - would
    require user to use Require before adopting it.</li>
  </ul>

  <p>Browserify is closer.</p>

  <ul>
    <li>Why it is better for some projects</li>

    <li>Very nearly meets my needs</li>

    <li>But http-compatability (<a href=
    "https://github.com/substack/http-browserify">https://github.com/substack/http-browserify</a>),
    while complete enough, isn't compact enough to not push project
    over micro-library size</li>
  </ul>

  <p>Testing post-packaging for small set of smoke tests. Can't
  test everything, only through public API.</p>

  <p>Uglify. Why not Google Closure Compiler.</p>

  <h2 id="resume-not"><a href="#resume-not"><span class=
  "header-section-number">5.13</span> Resume (not)</a></h2>

  <p>Could implement a resume function for if transmission stops
  halfway</p>
  <pre class="sourceCode javascript">
<code class="sourceCode javascript">   .<span class=
"fu">onError</span>( error ) {
      <span class="kw">this</span>.<span class=
"fu">resume</span>();
   }</code>
</pre>

  <h1 id="conclusion"><a href="#conclusion"><span class=
  "header-section-number">6</span> Conclusion</a></h1><!---
**1 to 5 pages**
=-->

  <p>Doing things faster vs doing things earlier. "Hurry up and
  wait" approach to optimisation.</p>

  <h2 id="weaknesses"><a href="#weaknesses"><span class=
  "header-section-number">6.1</span> weaknesses</a></h2>

  <p>implementation keeps 'unreachable' listeners difficult
  decidability/proof type problem to get completely right but could
  cover most of the easy cases</p>

  <p>Parse time for large files spread out over a long time.
  Reaction to parsed content spread out over a long time, for
  example de-marshalling to domain objects. For UX may be
  preferable to have many small delays rather than one large
  one.</p>

  <p>Doesn't support all of jsonpath. Not a strict subset of the
  language.</p>

  <p>Rest client as a library is passing mutable objects to the
  caller. too inefficient to re-create a new map/array every time
  an item is not as efficient in immutability as list head-tail
  type storage</p>

  <p>An immutability wrapper might be possible with defineProperty.
  Can't casually overwrite via assignment but still possible to do
  defineProperty again.</p>

  <p>Would benefit from a stateless language where everything is
  stateless at all times to avoid having to program
  defensively.</p>

  <p>Aborting http request may not stop processing on the server.
  Why this is perhaps desirable - transactions, leaving resources
  in a half-complete state.</p>

  <h2 id="suitability-for-databases-really-just-an-inline-asside">
  <a href=
  "#suitability-for-databases-really-just-an-inline-asside"><span class="header-section-number">
  6.2</span> Suitability for databases (really just an inline
  asside)</a></h2>

  <p>Databases offer data one row at a time, not as a big lump.</p>

  <h2 id="development-methodology"><a href=
  "#development-methodology"><span class=
  "header-section-number">6.3</span> Development
  methodology</a></h2>

  <p>Did it help?</p>

  <p>Switched several times. Could have started with winning side?
  Tension between choosing latest and greatest (promising much) or
  old established solution alraedy experienced with but with known
  problems. Judging if problems will become too much of a
  hinderence and underestimating the flaws. JSTD was yesterday's
  latest and greatest but Karma genuinely is great. In end, right
  solution was found despite not being found in most direct
  way.</p>

  <p>Packaging was a lot of work but has delivered the most concise
  possible library.</p>

  <h2 id="size"><a href="#size"><span class=
  "header-section-number">6.4</span> Size</a></h2>

  <div class="figure">
    <img src="images/placeholder.png" alt=
    "A pie chart showing the sizes of the various parts of the codebase">

    <p class="caption">A pie chart showing the sizes of the various
    parts of the codebase</p>
  </div>

  <p>Comment on the size of the libraray</p>

  <h2 id="handling-invalid-input"><a href=
  "#handling-invalid-input"><span class=
  "header-section-number">6.5</span> Handling invalid
  input</a></h2>

  <p>Invalid jsonpaths made from otherwise valid clauses (for
  example two roots) perhaps could fail early, at compile time.
  Instead, get a jsonPath that couldn't match anything. Invalid
  syntax is picked up.</p>

  <p>Same pattern could be extended to XML. Or any tree-based
  format. Text is easier but no reason why not binary
  applications.</p>

  <p>Not particularly useful reading from local files.</p>

  <p>Does not save memory over DOM parsing since the same DOM tree
  is built. May slightly increase memory usage by utilising memory
  earlier that would otherwise be dept dormant until the whole
  transmission is received but worst case more often a concern than
  mean.</p>

  <p>Implementation in a purely functional language with lazy
  evaluation: could it mean that only the necessary parts are
  computed? Could I have implemented the same in javascript?</p>

  <p>Would be nice to: * discard patterns that can't match any
  further parts of the tree * discard branches of the tree that
  can't match any patterns * just over the parsing of branches of
  the tree that provably can't match any of the patterns</p>

  <h2 id="comparative-usages"><a href=
  "#comparative-usages"><span class=
  "header-section-number">6.6</span> Comparative usages</a></h2>

  <p>Interesting article from Clarinet: <a href=
  "http://writings.nunojob.com/2011/12/clarinet-sax-based-evented-streaming-json-parser-in-javascript-for-the-browser-and-nodejs.html">
  http://writings.nunojob.com/2011/12/clarinet-sax-based-evented-streaming-json-parser-in-javascript-for-the-browser-and-nodejs.html</a></p>

  <p>In terms of syntax: compare to SAX (clarinet) for getting the
  same job done. Draw examples from github project README. Or from
  reimplementing Clarinet's examples.</p>

  <p>Consider:</p>

  <ul>
    <li>Difficulty to program</li>

    <li>Ease of reading the program / clarity of code</li>

    <li>Resources consumed</li>

    <li>Performance (time) taken</li>

    <li>about the same. Can react equally quickly to io in
    progress, both largely io bound.</li>

    <li>Is earlier really faster?</li>
  </ul>

  <h2 id="community-reaction"><a href=
  "#community-reaction"><span class=
  "header-section-number">6.7</span> Community reaction</a></h2>

  <p>Built into Dojo Followers on Github Being posted in forums
  (hopefully also listed on blogs) No homepage as of yet other than
  the Github page</p>

  <h2 id="possible-future-work"><a href=
  "#possible-future-work"><span class=
  "header-section-number">6.8</span> Possible future work</a></h2>

  <p>Do da XMLs</p>

  <h1 id="appendix"><a href="#appendix"><span class=
  "header-section-number">7</span> Appendix</a></h1>

  <h1 id="bibliography"><a href="#bibliography"><span class=
  "header-section-number">8</span> Bibliography</a></h1>

  <p>Ahuvia, Yogev. 2013. &acirc;&euro;&oelig;Design Patterns:
  Infinite Scrolling: Let&acirc;&euro;&trade;s Get To The Bottom Of
  This
  http://uxdesign.smashingmagazine.com/2013/05/03/infinite-scrolling-get-bottom/.&acirc;&euro;
  Smashing Magazine.</p>

  <p>Anon. 2011. &acirc;&euro;&oelig;3G mobile data network
  crowd-sourcing survey.&acirc;&euro; BBC News.</p>

  <p>Douglas, Crockford. 2009. &acirc;&euro;&oelig;JSON: The
  fat-free alternative to XML.&acirc;&euro; <a href=
  "http://json.org" title=
  "http://json.org">http://json.org</a>.</p>

  <p>Fielding, R. T. 2000. &acirc;&euro;&oelig;Principled design of
  the modern Web architecture.&acirc;&euro;</p>

  <p>Geelhoed, Erik, Peter Toft, Suzanne Roberts, and Patrick
  Hyland. 1995. &acirc;&euro;&oelig;To influence Time
  Perception.&acirc;&euro; Hewlett Packard Labs. <a href=
  "http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm"
  title=
  "http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm">http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm</a>.</p>

  <p>Graham, Paul. 2004. <em>The Other Road Ahead</em>.
  O&acirc;&euro;&trade;Reilly and Associates.</p>

  <p>Hopkins, Don. 1994. <em>The X-Windows Disaster</em>. Hungry
  Minds.</p>

  <p>Lea, Tom. 2012. &acirc;&euro;&oelig;Improving performance on
  twitter.com.&acirc;&euro; <a href=
  "[https://blog.twitter.com/2012/improving-performance-twittercom]"
  title=
  "[https://blog.twitter.com/2012/improving-performance-twittercom]">
  [https://blog.twitter.com/2012/improving-performance-twittercom]</a>.</p>

  <p>Mullany, Michael. 2013. &acirc;&euro;&oelig;5 Myths About
  Mobile Web Performance.&acirc;&euro; <a href=
  "http://www.sencha.com/blog/5-myths-about-mobile-web-performance"
  title=
  "http://www.sencha.com/blog/5-myths-about-mobile-web-performance">
  http://www.sencha.com/blog/5-myths-about-mobile-web-performance</a>.</p>

  <p>Ralston, Anthony. 2000. &acirc;&euro;&oelig;Encyclopedia of
  Computer Science.&acirc;&euro; Nature Pub. Group.</p>

  <p>Reis, Eric. 2011. <em>The Lean Startup: How
  Today&acirc;&euro;&trade;s Entrepreneurs Use Continuous
  Innovation to Create Radically Successful Businesses.</em> Crown
  Business Publishing.</p>

  <p>Sapir, E. 1958. &acirc;&euro;&oelig;Culture, Language and
  Personality (ed. D. G. Mandelbaum).&acirc;&euro; Berkeley, CA:
  University of California Press.</p>

  <p>Stefanov, Stoyan. 2009. &acirc;&euro;&oelig;Progressive
  rendering via multiple flushes.&acirc;&euro; <a href=
  "http://www.phpied.com/progressive-rendering-via-multiple-flushes/"
  title=
  "http://www.phpied.com/progressive-rendering-via-multiple-flushes/">
  http://www.phpied.com/progressive-rendering-via-multiple-flushes/</a>.</p>

  <p>Whorf, B. L. 1956. &acirc;&euro;&oelig;Language, Thought and
  Reality (ed. J. B. Carroll).&acirc;&euro; Cambridge, MA: MIT
  Press.</p>

  <p>van Kesteren, Anne. 2012. &acirc;&euro;&oelig;XMLHttpRequest
  Level 2 Working Draft.&acirc;&euro; <a href=
  "http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications"
  title=
  "http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications">
  http://www.w3.org/TR/XMLHttpRequest2/#make-progress-notifications</a>.</p>

  <p>van Kesteren, Anne, and Dean Jackson. 2006.
  &acirc;&euro;&oelig;The XMLHttpRequest Object.&acirc;&euro;
  <a href="http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/"
  title=
  "http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/">http://www.w3.org/TR/2006/WD-XMLHttpRequest-20060405/</a>.</p>

  <div class="footnotes">
    <hr>

    <ol>
      <li id="fn1">
        <p>See <a href=
        "http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html">
        http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html</a>.<a href="#fnref1">&acirc;&dagger;&copy;</a></p>
      </li>

      <li id="fn2">
        <p>Rather confusingly, X11 would call the <em>server</em>
        the <em>client</em> but I use terms here by their more
        cannonical meaning such that the client is the machine the
        user is actually interacting with.<a href=
        "#fnref2">&acirc;&dagger;&copy;</a></p>
      </li>

      <li id="fn3">
        <p>See <a href=
        "http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html">
        http://jackson.codehaus.org/1.0.1/javadoc/org/codehaus/jackson/node/NullNode.html</a>.<a href="#fnref3">&acirc;&dagger;&copy;</a></p>
      </li>
    </ol>
  </div>
</body>
</html>
